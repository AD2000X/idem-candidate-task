{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f384fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "iDEM Task 2: Complex vs Simple Classifier using Small Transformer\n",
    "==================================================================\n",
    "\n",
    "This script fine-tunes DistilBERT to classify sentence complexity\n",
    "using anchor-based training strategy.\n",
    "\n",
    "Official Label Definition (from README):\n",
    "    - Label = 0: Simple (sentence annotated as simple)\n",
    "    - Label = 1: Complex (sentence annotated as complex)\n",
    "\n",
    "Source (derived from ID prefix):\n",
    "    - wiki-* : Wikipedia\n",
    "    - viki-* : Vikidia\n",
    "\n",
    "Key Features:\n",
    "    - Anchor-based training: Clean samples (short Simple + long Complex)\n",
    "    - Downsampling: Balance class imbalance in anchors\n",
    "    - ACC Calibration: Adjust for classifier bias\n",
    "    - Error Analysis: On full corpus to find real misclassifications\n",
    "    - Full prediction: P(Simple) and P(Complex) for all sentences\n",
    "\n",
    "Environment: Kaggle Notebook with GPU\n",
    "Dataset Path: /kaggle/input/dataset-cleaned/\n",
    "\n",
    "Note: Requires transformers >= 4.46.0 (uses eval_strategy parameter)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fcf88",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 1: Setup and Imports\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face imports\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3282f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9497d5",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 2: Configuration (English Only)\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle dataset path\n",
    "INPUT_DIR = Path(\"/kaggle/input/dataset-cleaned\")\n",
    "OUTPUT_DIR = Path(\"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file (English only)\n",
    "DATA_FILE = \"En-Dataset_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    \"name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    \"downsample_ratio\": 2.0,  # Majority = N * Minority\n",
    "    \"min_anchor_samples\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis settings\n",
    "HIGH_CONF_THRESHOLD = 0.9\n",
    "LOW_CONF_THRESHOLD = 0.1\n",
    "BOUNDARY_RANGE = (0.45, 0.55)\n",
    "ERROR_SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3591145",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 3: Reproducibility\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = TRAIN_CONFIG[\"seed\"]) -> None:\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ed751",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba82d2",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 4: Data Loading\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load cleaned dataset.\n",
    "    \n",
    "    Official Label Definition:\n",
    "        - Label = 0: Simple\n",
    "        - Label = 1: Complex\n",
    "    \n",
    "    Source (from ID prefix):\n",
    "        - wiki-* : Wikipedia\n",
    "        - viki-* : Vikidia\n",
    "    \"\"\"\n",
    "    filepath = INPUT_DIR / DATA_FILE\n",
    "    print(f\"\\nLoading: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Add source column based on ID prefix\n",
    "    df['source'] = df['ID'].apply(\n",
    "        lambda x: 'wiki' if str(x).startswith('wiki-') else 'viki'\n",
    "    )\n",
    "    \n",
    "    # Statistics by Label (complexity)\n",
    "    n_simple = (df['Label'] == 0).sum()\n",
    "    n_complex = (df['Label'] == 1).sum()\n",
    "    \n",
    "    # Statistics by Source\n",
    "    n_wiki = (df['source'] == 'wiki').sum()\n",
    "    n_viki = (df['source'] == 'viki').sum()\n",
    "    \n",
    "    print(f\"\\n  Total sentences: {len(df):,}\")\n",
    "    print(f\"\\n  By Complexity (Label):\")\n",
    "    print(f\"    Simple (Label=0):  {n_simple:,} ({n_simple/len(df)*100:.1f}%)\")\n",
    "    print(f\"    Complex (Label=1): {n_complex:,} ({n_complex/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\n  By Source:\")\n",
    "    print(f\"    Wikipedia: {n_wiki:,} ({n_wiki/len(df)*100:.1f}%)\")\n",
    "    print(f\"    Vikidia:   {n_viki:,} ({n_viki/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    print(f\"\\n  Cross-tabulation (Source x Label):\")\n",
    "    cross_tab = pd.crosstab(df['source'], df['Label'], margins=True)\n",
    "    cross_tab.columns = ['Simple(0)', 'Complex(1)', 'Total']\n",
    "    cross_tab.index = ['Vikidia', 'Wikipedia', 'Total']\n",
    "    print(cross_tab.to_string())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa48164",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 5: Anchor Selection and Downsampling\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_anchors(\n",
    "    df: pd.DataFrame,\n",
    "    downsample_ratio: float = TRAIN_CONFIG[\"downsample_ratio\"],\n",
    "    min_samples: int = TRAIN_CONFIG[\"min_anchor_samples\"]\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Select clean anchor samples for training.\n",
    "    \n",
    "    Simple Anchor: Label=0 (Simple) with LengthWords <= Q1\n",
    "    Complex Anchor: Label=1 (Complex) with LengthWords >= Q3\n",
    "    \n",
    "    Downsample majority class to balance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANCHOR SELECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate quartiles\n",
    "    q1 = df['LengthWords'].quantile(0.25)\n",
    "    q3 = df['LengthWords'].quantile(0.75)\n",
    "    \n",
    "    print(f\"LengthWords quartiles: Q1={q1:.1f}, Q3={q3:.1f}\")\n",
    "    \n",
    "    # Select anchors based on CORRECT label semantics\n",
    "    # Label=0 is Simple, Label=1 is Complex\n",
    "    simple_mask = (df['Label'] == 0) & (df['LengthWords'] <= q1)\n",
    "    complex_mask = (df['Label'] == 1) & (df['LengthWords'] >= q3)\n",
    "    \n",
    "    simple_anchors = df[simple_mask].copy()\n",
    "    complex_anchors = df[complex_mask].copy()\n",
    "    \n",
    "    n_simple = len(simple_anchors)\n",
    "    n_complex = len(complex_anchors)\n",
    "    \n",
    "    # Validation\n",
    "    if n_simple < min_samples:\n",
    "        raise ValueError(\n",
    "            f\"Insufficient simple anchors: {n_simple} < {min_samples}\"\n",
    "        )\n",
    "    if n_complex < min_samples:\n",
    "        raise ValueError(\n",
    "            f\"Insufficient complex anchors: {n_complex} < {min_samples}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nBefore downsampling:\")\n",
    "    print(f\"  Simple anchors (Label=0, LengthWords <= {q1:.1f}): {n_simple:,}\")\n",
    "    print(f\"  Complex anchors (Label=1, LengthWords >= {q3:.1f}): {n_complex:,}\")\n",
    "    \n",
    "    # Determine majority class and downsample\n",
    "    if n_simple > n_complex:\n",
    "        minority_count = n_complex\n",
    "        majority_class = \"simple\"\n",
    "        max_majority = int(minority_count * downsample_ratio)\n",
    "        if n_simple > max_majority:\n",
    "            simple_anchors = simple_anchors.sample(\n",
    "                n=max_majority,\n",
    "                random_state=TRAIN_CONFIG[\"seed\"]\n",
    "            )\n",
    "            print(f\"\\nDownsampling Simple: {n_simple:,} -> {len(simple_anchors):,}\")\n",
    "    else:\n",
    "        minority_count = n_simple\n",
    "        majority_class = \"complex\"\n",
    "        max_majority = int(minority_count * downsample_ratio)\n",
    "        if n_complex > max_majority:\n",
    "            complex_anchors = complex_anchors.sample(\n",
    "                n=max_majority,\n",
    "                random_state=TRAIN_CONFIG[\"seed\"]\n",
    "            )\n",
    "            print(f\"\\nDownsampling Complex: {n_complex:,} -> {len(complex_anchors):,}\")\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    anchor_df = pd.concat([simple_anchors, complex_anchors], ignore_index=True)\n",
    "    anchor_df = anchor_df.sample(\n",
    "        frac=1, random_state=TRAIN_CONFIG[\"seed\"]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    n_simple_final = (anchor_df['Label'] == 0).sum()\n",
    "    n_complex_final = (anchor_df['Label'] == 1).sum()\n",
    "    \n",
    "    print(f\"\\nFinal anchor set:\")\n",
    "    print(f\"  Total: {len(anchor_df):,}\")\n",
    "    print(f\"  Simple (Label=0): {n_simple_final:,}\")\n",
    "    print(f\"  Complex (Label=1): {n_complex_final:,}\")\n",
    "    print(f\"  Ratio: {max(n_simple_final, n_complex_final) / min(n_simple_final, n_complex_final):.2f}x\")\n",
    "    \n",
    "    anchor_info = {\n",
    "        'q1': float(q1),\n",
    "        'q3': float(q3),\n",
    "        'n_simple_original': int(n_simple),\n",
    "        'n_complex_original': int(n_complex),\n",
    "        'n_simple_final': int(n_simple_final),\n",
    "        'n_complex_final': int(n_complex_final),\n",
    "        'majority_class': majority_class,\n",
    "        'downsample_ratio': downsample_ratio\n",
    "    }\n",
    "    \n",
    "    return anchor_df, anchor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_anchor_data(\n",
    "    anchor_df: pd.DataFrame,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split anchor data into train/val/test with stratification.\"\"\"\n",
    "    print(f\"\\nSplitting anchors ({train_ratio:.0%}/{val_ratio:.0%}/{test_ratio:.0%})...\")\n",
    "    \n",
    "    train_df, temp_df = train_test_split(\n",
    "        anchor_df,\n",
    "        test_size=(val_ratio + test_ratio),\n",
    "        stratify=anchor_df['Label'],\n",
    "        random_state=TRAIN_CONFIG[\"seed\"]\n",
    "    )\n",
    "    \n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=test_ratio / (val_ratio + test_ratio),\n",
    "        stratify=temp_df['Label'],\n",
    "        random_state=TRAIN_CONFIG[\"seed\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"  Train: {len(train_df):,} (Simple: {(train_df['Label']==0).sum():,}, Complex: {(train_df['Label']==1).sum():,})\")\n",
    "    print(f\"  Val:   {len(val_df):,} (Simple: {(val_df['Label']==0).sum():,}, Complex: {(val_df['Label']==1).sum():,})\")\n",
    "    print(f\"  Test:  {len(test_df):,} (Simple: {(test_df['Label']==0).sum():,}, Complex: {(test_df['Label']==1).sum():,})\")\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df888fcb",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 6: Tokenization and Dataset Preparation\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    max_length: int\n",
    ") -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Prepare HuggingFace datasets with tokenization.\"\"\"\n",
    "    print(\"\\nPreparing datasets...\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"Sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    train_ds = Dataset.from_pandas(train_df[['Sentence', 'Label']].reset_index(drop=True))\n",
    "    val_ds = Dataset.from_pandas(val_df[['Sentence', 'Label']].reset_index(drop=True))\n",
    "    test_ds = Dataset.from_pandas(test_df[['Sentence', 'Label']].reset_index(drop=True))\n",
    "    \n",
    "    # Tokenize\n",
    "    train_ds = train_ds.map(tokenize_function, batched=True, desc=\"Tokenizing train\")\n",
    "    val_ds = val_ds.map(tokenize_function, batched=True, desc=\"Tokenizing val\")\n",
    "    test_ds = test_ds.map(tokenize_function, batched=True, desc=\"Tokenizing test\")\n",
    "    \n",
    "    # Rename label column for Trainer\n",
    "    train_ds = train_ds.rename_column(\"Label\", \"labels\")\n",
    "    val_ds = val_ds.rename_column(\"Label\", \"labels\")\n",
    "    test_ds = test_ds.rename_column(\"Label\", \"labels\")\n",
    "    \n",
    "    # Set format\n",
    "    columns = ['input_ids', 'attention_mask', 'labels']\n",
    "    train_ds.set_format(type=\"torch\", columns=columns)\n",
    "    val_ds.set_format(type=\"torch\", columns=columns)\n",
    "    test_ds.set_format(type=\"torch\", columns=columns)\n",
    "    \n",
    "    print(f\"  Train: {len(train_ds):,}, Val: {len(val_ds):,}, Test: {len(test_ds):,}\")\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee23b3",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 7: Custom Trainer with Class Weights\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer with class-weighted loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        weights = self.class_weights.to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(train_df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"Compute balanced class weights.\"\"\"\n",
    "    label_counts = train_df['Label'].value_counts().sort_index()\n",
    "    n_simple = label_counts[0]  # Label=0 is Simple\n",
    "    n_complex = label_counts[1]  # Label=1 is Complex\n",
    "    total = n_simple + n_complex\n",
    "    \n",
    "    weight_simple = total / (2 * n_simple)\n",
    "    weight_complex = total / (2 * n_complex)\n",
    "    \n",
    "    weights = torch.tensor([weight_simple, weight_complex], dtype=torch.float32)\n",
    "    print(f\"\\nClass weights: [Simple(0): {weight_simple:.4f}, Complex(1): {weight_complex:.4f}]\")\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627ef1f",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 8: Metrics Computation\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a57f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    \n",
    "    Label semantics:\n",
    "        - Label=0: Simple\n",
    "        - Label=1: Complex (positive class for ROC-AUC)\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    prob_simple = probs[:, 0]\n",
    "    prob_complex = probs[:, 1]\n",
    "    \n",
    "    # Predictions: Complex (Label=1) is positive class\n",
    "    preds = (prob_complex >= 0.5).astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"f1_simple\": f1_score(labels, preds, pos_label=0),\n",
    "        \"f1_complex\": f1_score(labels, preds, pos_label=1),\n",
    "        \"precision_weighted\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall_weighted\": recall_score(labels, preds, average=\"weighted\"),\n",
    "        \"roc_auc\": roc_auc_score(labels, prob_complex),  # prob of positive class\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0461b",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 9: Training Function\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd51bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    tokenizer,\n",
    "    class_weights: torch.Tensor\n",
    ") -> Tuple[Trainer, object]:\n",
    "    \"\"\"Train transformer model.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"TRAINING: {MODEL_CONFIG['name']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load model with CORRECT label mapping\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_CONFIG['name'],\n",
    "        num_labels=2,\n",
    "        id2label={0: \"Simple\", 1: \"Complex\"},\n",
    "        label2id={\"Simple\": 0, \"Complex\": 1}\n",
    "    )\n",
    "    \n",
    "    output_dir = MODELS_DIR / \"transformer_en\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=TRAIN_CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=TRAIN_CONFIG[\"batch_size\"] * 2,\n",
    "        num_train_epochs=TRAIN_CONFIG[\"num_epochs\"],\n",
    "        weight_decay=TRAIN_CONFIG[\"weight_decay\"],\n",
    "        warmup_ratio=TRAIN_CONFIG[\"warmup_ratio\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_weighted\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        seed=TRAIN_CONFIG[\"seed\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e88ba3",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 10: Evaluation on Test Set\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(\n",
    "    trainer: Trainer,\n",
    "    test_ds: Dataset,\n",
    "    test_df: pd.DataFrame\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        prob_simple, prob_complex, preds, labels, metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION ON ANCHOR TEST SET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pred_output = trainer.predict(test_ds)\n",
    "    logits = pred_output.predictions\n",
    "    labels = pred_output.label_ids\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    prob_simple = probs[:, 0]\n",
    "    prob_complex = probs[:, 1]\n",
    "    \n",
    "    # Predictions: pred_label matches official Label semantics\n",
    "    preds = (prob_complex >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\")),\n",
    "        \"f1_simple\": float(f1_score(labels, preds, pos_label=0)),\n",
    "        \"f1_complex\": float(f1_score(labels, preds, pos_label=1)),\n",
    "        \"precision_simple\": float(precision_score(labels, preds, pos_label=0)),\n",
    "        \"precision_complex\": float(precision_score(labels, preds, pos_label=1)),\n",
    "        \"recall_simple\": float(recall_score(labels, preds, pos_label=0)),\n",
    "        \"recall_complex\": float(recall_score(labels, preds, pos_label=1)),\n",
    "        \"roc_auc\": float(roc_auc_score(labels, prob_complex)),\n",
    "        \"average_precision\": float(average_precision_score(labels, prob_complex)),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"  Accuracy:            {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1 (weighted):       {metrics['f1_weighted']:.4f}\")\n",
    "    print(f\"  F1 (Simple, Label=0):  {metrics['f1_simple']:.4f}\")\n",
    "    print(f\"  F1 (Complex, Label=1): {metrics['f1_complex']:.4f}\")\n",
    "    print(f\"  ROC-AUC:             {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Pred Simple(0)  Pred Complex(1)\")\n",
    "    print(f\"  Actual Simple(0)  {cm[0,0]:>12,}  {cm[0,1]:>14,}\")\n",
    "    print(f\"  Actual Complex(1) {cm[1,0]:>12,}  {cm[1,1]:>14,}\")\n",
    "    \n",
    "    return prob_simple, prob_complex, preds, labels, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf644d4",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 11: Full Dataset Prediction\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_dataset(\n",
    "    trainer: Trainer,\n",
    "    full_df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    max_length: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Predict probabilities for all sentences.\n",
    "    \n",
    "    Returns:\n",
    "        prob_simple, prob_complex\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL DATASET PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Predicting for {len(full_df):,} sentences...\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"Sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    \n",
    "    full_ds = Dataset.from_pandas(full_df[['Sentence']].reset_index(drop=True))\n",
    "    full_ds = full_ds.map(tokenize_function, batched=True, desc=\"Tokenizing full\")\n",
    "    full_ds.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    pred_output = trainer.predict(full_ds)\n",
    "    logits = pred_output.predictions\n",
    "    \n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    prob_simple = probs[:, 0]\n",
    "    prob_complex = probs[:, 1]\n",
    "    \n",
    "    print(f\"\\n  P(Simple) - Mean: {prob_simple.mean():.4f}, Std: {prob_simple.std():.4f}\")\n",
    "    print(f\"  P(Complex) - Mean: {prob_complex.mean():.4f}, Std: {prob_complex.std():.4f}\")\n",
    "    print(f\"  Predicted Simple (P>=0.5): {(prob_simple >= 0.5).mean():.4f}\")\n",
    "    print(f\"  Predicted Complex (P>=0.5): {(prob_complex >= 0.5).mean():.4f}\")\n",
    "    \n",
    "    return prob_simple, prob_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae9d27",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 12: ACC Calibration\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_from_test(\n",
    "    test_labels: np.ndarray,\n",
    "    test_preds: np.ndarray\n",
    ") -> Tuple[float, float, Dict]:\n",
    "    \"\"\"\n",
    "    Compute TPR and FPR from anchor test set for ACC calibration.\n",
    "    \n",
    "    Positive class = Complex (Label=1)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ACC CALIBRATION (from anchor test set)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # For Complex as positive class (Label=1):\n",
    "    # TPR = TP / (TP + FN) = correctly identified Complex\n",
    "    # FPR = FP / (FP + TN) = Simple misclassified as Complex\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TN (Simple correct)={tn:,}, FP (Simple->Complex)={fp:,}\")\n",
    "    print(f\"  FN (Complex->Simple)={fn:,}, TP (Complex correct)={tp:,}\")\n",
    "    print(f\"\\nRates (Complex as positive):\")\n",
    "    print(f\"  TPR (Recall Complex): {tpr:.4f}\")\n",
    "    print(f\"  FPR (Fall-out):       {fpr:.4f}\")\n",
    "    print(f\"  TNR (Recall Simple):  {tnr:.4f}\")\n",
    "    print(f\"  FNR (Miss rate):      {fnr:.4f}\")\n",
    "    \n",
    "    calibration_info = {\n",
    "        'method': 'anchor_test_set',\n",
    "        'n_samples': int(len(test_labels)),\n",
    "        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n",
    "        'tpr': float(tpr),\n",
    "        'fpr': float(fpr),\n",
    "        'tnr': float(tnr),\n",
    "        'fnr': float(fnr)\n",
    "    }\n",
    "    \n",
    "    return tpr, fpr, calibration_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c546f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_acc_correction(p_pred: float, tpr: float, fpr: float) -> float:\n",
    "    \"\"\"Apply ACC formula: p_true = (p_pred - FPR) / (TPR - FPR)\"\"\"\n",
    "    denominator = tpr - fpr\n",
    "    if abs(denominator) < 1e-10:\n",
    "        print(\"  [WARNING] TPR ~= FPR, using p_pred as fallback\")\n",
    "        return p_pred\n",
    "    p_true = (p_pred - fpr) / denominator\n",
    "    return max(0.0, min(1.0, p_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463b6cb",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 13: Error Analysis on Full Corpus\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors_on_full_corpus(\n",
    "    full_df: pd.DataFrame,\n",
    "    prob_simple: np.ndarray,\n",
    "    prob_complex: np.ndarray,\n",
    "    sample_size: int = ERROR_SAMPLE_SIZE\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze prediction errors on FULL corpus.\n",
    "    \n",
    "    This finds real misclassifications, unlike anchor test set\n",
    "    which may have 100% accuracy due to clean separation.\n",
    "    \n",
    "    Label semantics:\n",
    "        - Label=0: Simple\n",
    "        - Label=1: Complex\n",
    "        - pred_label = (prob_complex >= 0.5)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ERROR ANALYSIS ON FULL CORPUS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Build analysis dataframe\n",
    "    analysis_df = full_df.copy()\n",
    "    analysis_df['prob_simple'] = prob_simple\n",
    "    analysis_df['prob_complex'] = prob_complex\n",
    "    analysis_df['pred_label'] = (prob_complex >= 0.5).astype(int)\n",
    "    analysis_df['correct'] = analysis_df['Label'] == analysis_df['pred_label']\n",
    "    \n",
    "    errors_df = analysis_df[~analysis_df['correct']]\n",
    "    \n",
    "    n_total = len(analysis_df)\n",
    "    n_errors = len(errors_df)\n",
    "    error_rate = n_errors / n_total if n_total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTotal errors: {n_errors:,} / {n_total:,} ({error_rate:.2%})\")\n",
    "    \n",
    "    results = {\n",
    "        'total_samples': n_total,\n",
    "        'total_errors': n_errors,\n",
    "        'error_rate': float(error_rate),\n",
    "        'false_positives': [],  # Pred=Complex but True=Simple\n",
    "        'false_negatives': [],  # Pred=Simple but True=Complex\n",
    "        'boundary_cases': []\n",
    "    }\n",
    "    \n",
    "    # False Positives: pred_label=1 (Complex) but Label=0 (Simple)\n",
    "    # Simple sentences misclassified as Complex\n",
    "    fp_df = errors_df[(errors_df['pred_label'] == 1) & (errors_df['Label'] == 0)]\n",
    "    fp_high_conf = fp_df[fp_df['prob_complex'] >= HIGH_CONF_THRESHOLD]\n",
    "    fp_sample = fp_df.sort_values('prob_complex', ascending=False).head(sample_size)\n",
    "    \n",
    "    print(f\"\\n[1] FALSE POSITIVES (pred=Complex, true=Simple)\")\n",
    "    print(f\"    Total FP: {len(fp_df):,}\")\n",
    "    print(f\"    High confidence FP (P(Complex) >= {HIGH_CONF_THRESHOLD}): {len(fp_high_conf):,}\")\n",
    "    print(f\"    These Simple sentences were predicted as Complex\")\n",
    "    \n",
    "    # By source breakdown\n",
    "    fp_wiki = len(fp_df[fp_df['source'] == 'wiki'])\n",
    "    fp_viki = len(fp_df[fp_df['source'] == 'viki'])\n",
    "    print(f\"    By source: Wikipedia={fp_wiki:,}, Vikidia={fp_viki:,}\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(fp_sample.iterrows(), 1):\n",
    "        sample = {\n",
    "            'sentence': row['Sentence'][:200],\n",
    "            'source': row['source'],\n",
    "            'true_label': int(row['Label']),\n",
    "            'pred_label': int(row['pred_label']),\n",
    "            'prob_simple': float(row['prob_simple']),\n",
    "            'prob_complex': float(row['prob_complex']),\n",
    "            'length_words': int(row['LengthWords']) if 'LengthWords' in row else None\n",
    "        }\n",
    "        results['false_positives'].append(sample)\n",
    "        print(f\"\\n    FP-{i} [P(Complex)={row['prob_complex']:.3f}] [Source={row['source']}] [Len={row.get('LengthWords', 'N/A')}]:\")\n",
    "        print(f\"    {row['Sentence'][:150]}...\")\n",
    "    \n",
    "    # False Negatives: pred_label=0 (Simple) but Label=1 (Complex)\n",
    "    # Complex sentences misclassified as Simple\n",
    "    fn_df = errors_df[(errors_df['pred_label'] == 0) & (errors_df['Label'] == 1)]\n",
    "    fn_high_conf = fn_df[fn_df['prob_simple'] >= HIGH_CONF_THRESHOLD]\n",
    "    fn_sample = fn_df.sort_values('prob_simple', ascending=False).head(sample_size)\n",
    "    \n",
    "    print(f\"\\n[2] FALSE NEGATIVES (pred=Simple, true=Complex)\")\n",
    "    print(f\"    Total FN: {len(fn_df):,}\")\n",
    "    print(f\"    High confidence FN (P(Simple) >= {HIGH_CONF_THRESHOLD}): {len(fn_high_conf):,}\")\n",
    "    print(f\"    These Complex sentences were predicted as Simple\")\n",
    "    \n",
    "    fn_wiki = len(fn_df[fn_df['source'] == 'wiki'])\n",
    "    fn_viki = len(fn_df[fn_df['source'] == 'viki'])\n",
    "    print(f\"    By source: Wikipedia={fn_wiki:,}, Vikidia={fn_viki:,}\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(fn_sample.iterrows(), 1):\n",
    "        sample = {\n",
    "            'sentence': row['Sentence'][:200],\n",
    "            'source': row['source'],\n",
    "            'true_label': int(row['Label']),\n",
    "            'pred_label': int(row['pred_label']),\n",
    "            'prob_simple': float(row['prob_simple']),\n",
    "            'prob_complex': float(row['prob_complex']),\n",
    "            'length_words': int(row['LengthWords']) if 'LengthWords' in row else None\n",
    "        }\n",
    "        results['false_negatives'].append(sample)\n",
    "        print(f\"\\n    FN-{i} [P(Simple)={row['prob_simple']:.3f}] [Source={row['source']}] [Len={row.get('LengthWords', 'N/A')}]:\")\n",
    "        print(f\"    {row['Sentence'][:150]}...\")\n",
    "    \n",
    "    # Boundary cases: P close to 0.5\n",
    "    boundary_df = analysis_df[\n",
    "        (analysis_df['prob_simple'] >= BOUNDARY_RANGE[0]) &\n",
    "        (analysis_df['prob_simple'] <= BOUNDARY_RANGE[1])\n",
    "    ].copy()\n",
    "    \n",
    "    if len(boundary_df) > 0:\n",
    "        boundary_df['dist_from_05'] = (boundary_df['prob_simple'] - 0.5).abs()\n",
    "        boundary_sample = boundary_df.nsmallest(sample_size, 'dist_from_05')\n",
    "    else:\n",
    "        boundary_sample = boundary_df\n",
    "    \n",
    "    print(f\"\\n[3] BOUNDARY CASES (P(Simple) in {BOUNDARY_RANGE})\")\n",
    "    print(f\"    Total boundary: {len(boundary_df):,}\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(boundary_sample.iterrows(), 1):\n",
    "        sample = {\n",
    "            'sentence': row['Sentence'][:200],\n",
    "            'source': row['source'],\n",
    "            'true_label': int(row['Label']),\n",
    "            'pred_label': int(row['pred_label']),\n",
    "            'prob_simple': float(row['prob_simple']),\n",
    "            'correct': bool(row['correct'])\n",
    "        }\n",
    "        results['boundary_cases'].append(sample)\n",
    "        status = \"CORRECT\" if row['correct'] else \"WRONG\"\n",
    "        label_name = \"Simple\" if row['Label'] == 0 else \"Complex\"\n",
    "        print(f\"\\n    B-{i} [P(Simple)={row['prob_simple']:.3f}] [{status}] [True={label_name}]:\")\n",
    "        print(f\"    {row['Sentence'][:150]}...\")\n",
    "    \n",
    "    # Summary\n",
    "    results['summary'] = {\n",
    "        'total_fp': len(fp_df),\n",
    "        'total_fn': len(fn_df),\n",
    "        'high_conf_fp': len(fp_high_conf),\n",
    "        'high_conf_fn': len(fn_high_conf),\n",
    "        'total_boundary': len(boundary_df),\n",
    "        'fp_by_source': {'wiki': fp_wiki, 'viki': fp_viki},\n",
    "        'fn_by_source': {'wiki': fn_wiki, 'viki': fn_viki}\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da0fe6",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 14: Final Estimates\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_estimates(\n",
    "    full_df: pd.DataFrame,\n",
    "    prob_simple: np.ndarray,\n",
    "    prob_complex: np.ndarray,\n",
    "    tpr: float,\n",
    "    fpr: float\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compute final prevalence estimates with ACC correction.\n",
    "    \n",
    "    Label semantics:\n",
    "        - Label=0: Simple\n",
    "        - Label=1: Complex (positive class for ACC)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL ESTIMATES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Naive estimate from labels\n",
    "    naive_simple = (full_df['Label'] == 0).mean()\n",
    "    naive_complex = (full_df['Label'] == 1).mean()\n",
    "    \n",
    "    # Predicted proportions\n",
    "    pred_simple = (prob_simple >= 0.5).mean()\n",
    "    pred_complex = (prob_complex >= 0.5).mean()\n",
    "    \n",
    "    # ACC correction for Complex proportion\n",
    "    # p_true_complex = (p_pred_complex - FPR) / (TPR - FPR)\n",
    "    acc_complex = apply_acc_correction(pred_complex, tpr, fpr)\n",
    "    acc_simple = 1.0 - acc_complex\n",
    "    \n",
    "    print(f\"\\n1. GLOBAL SENTENCE COMPLEXITY:\")\n",
    "    print(f\"   Naive (from Label):\")\n",
    "    print(f\"     Simple (Label=0): {naive_simple:.4f} ({naive_simple*100:.2f}%)\")\n",
    "    print(f\"     Complex (Label=1): {naive_complex:.4f} ({naive_complex*100:.2f}%)\")\n",
    "    print(f\"   Predicted (P >= 0.5):\")\n",
    "    print(f\"     Simple: {pred_simple:.4f} ({pred_simple*100:.2f}%)\")\n",
    "    print(f\"     Complex: {pred_complex:.4f} ({pred_complex*100:.2f}%)\")\n",
    "    print(f\"   ACC-corrected:\")\n",
    "    print(f\"     Simple: {acc_simple:.4f} ({acc_simple*100:.2f}%)\")\n",
    "    print(f\"     Complex: {acc_complex:.4f} ({acc_complex*100:.2f}%)\")\n",
    "    \n",
    "    # Wikipedia internal analysis\n",
    "    wiki_mask = full_df['source'] == 'wiki'\n",
    "    wiki_prob_simple = prob_simple[wiki_mask]\n",
    "    wiki_prob_complex = prob_complex[wiki_mask]\n",
    "    n_wiki = wiki_mask.sum()\n",
    "    \n",
    "    wiki_pred_simple = (wiki_prob_simple >= 0.5).mean()\n",
    "    wiki_high_simple = (wiki_prob_simple >= HIGH_CONF_THRESHOLD).mean()\n",
    "    \n",
    "    print(f\"\\n2. WIKIPEDIA INTERNAL (Source=wiki):\")\n",
    "    print(f\"   Total sentences: {n_wiki:,}\")\n",
    "    print(f\"   Soft estimate (mean P(Simple)): {wiki_prob_simple.mean():.4f}\")\n",
    "    print(f\"   Predicted Simple (P >= 0.5): {wiki_pred_simple:.4f} ({wiki_pred_simple*100:.2f}%)\")\n",
    "    print(f\"   High conf Simple (P >= {HIGH_CONF_THRESHOLD}): {wiki_high_simple:.4f} ({wiki_high_simple*100:.2f}%)\")\n",
    "    \n",
    "    # Vikidia internal analysis\n",
    "    viki_mask = full_df['source'] == 'viki'\n",
    "    viki_prob_simple = prob_simple[viki_mask]\n",
    "    viki_prob_complex = prob_complex[viki_mask]\n",
    "    n_viki = viki_mask.sum()\n",
    "    \n",
    "    viki_pred_complex = (viki_prob_complex >= 0.5).mean()\n",
    "    viki_high_complex = (viki_prob_complex >= HIGH_CONF_THRESHOLD).mean()\n",
    "    \n",
    "    print(f\"\\n3. VIKIDIA INTERNAL (Source=viki):\")\n",
    "    print(f\"   Total sentences: {n_viki:,}\")\n",
    "    print(f\"   Soft estimate (mean P(Complex)): {viki_prob_complex.mean():.4f}\")\n",
    "    print(f\"   Predicted Complex (P >= 0.5): {viki_pred_complex:.4f} ({viki_pred_complex*100:.2f}%)\")\n",
    "    print(f\"   High conf Complex (P >= {HIGH_CONF_THRESHOLD}): {viki_high_complex:.4f} ({viki_high_complex*100:.2f}%)\")\n",
    "    \n",
    "    estimates = {\n",
    "        'naive': {\n",
    "            'simple': float(naive_simple),\n",
    "            'complex': float(naive_complex)\n",
    "        },\n",
    "        'predicted': {\n",
    "            'simple': float(pred_simple),\n",
    "            'complex': float(pred_complex)\n",
    "        },\n",
    "        'acc_corrected': {\n",
    "            'simple': float(acc_simple),\n",
    "            'complex': float(acc_complex)\n",
    "        },\n",
    "        'wikipedia_analysis': {\n",
    "            'n_sentences': int(n_wiki),\n",
    "            'mean_prob_simple': float(wiki_prob_simple.mean()),\n",
    "            'pred_simple_rate': float(wiki_pred_simple),\n",
    "            'high_conf_simple_rate': float(wiki_high_simple)\n",
    "        },\n",
    "        'vikidia_analysis': {\n",
    "            'n_sentences': int(n_viki),\n",
    "            'mean_prob_complex': float(viki_prob_complex.mean()),\n",
    "            'pred_complex_rate': float(viki_pred_complex),\n",
    "            'high_conf_complex_rate': float(viki_high_complex)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e45d8b",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 15: Main Pipeline\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> Dict:\n",
    "    \"\"\"Main execution pipeline for English only.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"iDEM TASK 2: TRANSFORMER FINE-TUNING (English)\")\n",
    "    print(\"Anchor-based Training + ACC Calibration\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Input:  {INPUT_DIR}\")\n",
    "    print(f\"Output: {OUTPUT_DIR}\")\n",
    "    print(f\"Model:  {MODEL_CONFIG['name']}\")\n",
    "    print(f\"Seed:   {TRAIN_CONFIG['seed']}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    set_seed()\n",
    "    \n",
    "    # Load data\n",
    "    full_df = load_data()\n",
    "    \n",
    "    # Select and split anchors\n",
    "    anchor_df, anchor_info = select_anchors(full_df)\n",
    "    train_df, val_df, test_df = split_anchor_data(anchor_df)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['name'])\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_ds, val_ds, test_ds = prepare_datasets(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer, MODEL_CONFIG['max_length']\n",
    "    )\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weights(train_df)\n",
    "    \n",
    "    # Train model\n",
    "    trainer, model = train_model(train_ds, val_ds, tokenizer, class_weights)\n",
    "    \n",
    "    # Evaluate on anchor test set\n",
    "    prob_simple_test, prob_complex_test, preds_test, labels_test, test_metrics = \\\n",
    "        evaluate_on_test(trainer, test_ds, test_df)\n",
    "    \n",
    "    # ACC calibration\n",
    "    tpr, fpr, calibration_info = compute_acc_from_test(labels_test, preds_test)\n",
    "    \n",
    "    # Full dataset prediction\n",
    "    prob_simple_full, prob_complex_full = predict_full_dataset(\n",
    "        trainer, full_df, tokenizer, MODEL_CONFIG['max_length']\n",
    "    )\n",
    "    \n",
    "    # Error analysis on FULL corpus (not anchor test set)\n",
    "    error_analysis = analyze_errors_on_full_corpus(\n",
    "        full_df, prob_simple_full, prob_complex_full\n",
    "    )\n",
    "    \n",
    "    # Final estimates\n",
    "    estimates = compute_final_estimates(\n",
    "        full_df, prob_simple_full, prob_complex_full, tpr, fpr\n",
    "    )\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_df = full_df[['ID', 'Sentence', 'Label', 'source', 'LengthWords']].copy()\n",
    "    pred_df['prob_simple'] = prob_simple_full\n",
    "    pred_df['prob_complex'] = prob_complex_full\n",
    "    pred_df['pred_label'] = (prob_complex_full >= 0.5).astype(int)\n",
    "    pred_df.to_csv(RESULTS_DIR / \"transformer_predictions_en.csv\", index=False)\n",
    "    print(f\"\\nPredictions saved: {RESULTS_DIR / 'transformer_predictions_en.csv'}\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'model': MODEL_CONFIG['name'],\n",
    "        'dataset_info': {\n",
    "            'total_sentences': len(full_df),\n",
    "            'simple_count': int((full_df['Label'] == 0).sum()),\n",
    "            'complex_count': int((full_df['Label'] == 1).sum()),\n",
    "            'wikipedia_count': int((full_df['source'] == 'wiki').sum()),\n",
    "            'vikidia_count': int((full_df['source'] == 'viki').sum())\n",
    "        },\n",
    "        'anchor_info': anchor_info,\n",
    "        'test_metrics': test_metrics,\n",
    "        'calibration': calibration_info,\n",
    "        'estimates': estimates,\n",
    "        'error_analysis': error_analysis\n",
    "    }\n",
    "    \n",
    "    # Save results JSON\n",
    "    results_file = RESULTS_DIR / \"transformer_results_en.json\"\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, default=str, ensure_ascii=False)\n",
    "    print(f\"Results saved: {results_file}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nAnchor Test Metrics:\")\n",
    "    print(f\"  Accuracy:     {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1 (weighted): {test_metrics['f1_weighted']:.4f}\")\n",
    "    print(f\"  ROC-AUC:      {test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"\\nACC Calibration:\")\n",
    "    print(f\"  TPR: {calibration_info['tpr']:.4f}\")\n",
    "    print(f\"  FPR: {calibration_info['fpr']:.4f}\")\n",
    "    print(f\"\\nPrevalence Estimates:\")\n",
    "    print(f\"  Naive Simple:       {estimates['naive']['simple']:.4f}\")\n",
    "    print(f\"  ACC-corrected Simple: {estimates['acc_corrected']['simple']:.4f}\")\n",
    "    print(f\"\\nError Analysis (Full Corpus):\")\n",
    "    print(f\"  Total errors: {error_analysis['total_errors']:,} ({error_analysis['error_rate']:.2%})\")\n",
    "    print(f\"  FP (Simple->Complex): {error_analysis['summary']['total_fp']:,}\")\n",
    "    print(f\"  FN (Complex->Simple): {error_analysis['summary']['total_fn']:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TASK 2 COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5b280",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Cell 16: Run\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
