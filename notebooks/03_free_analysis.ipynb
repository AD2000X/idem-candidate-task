{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":""},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13927639,"sourceType":"datasetVersion","datasetId":8875259}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"03f384fc","cell_type":"code","source":"\"\"\"\niDEM Task 2: Complex vs Simple Classifier using Small Transformer\n==================================================================\n\nThis script fine-tunes DistilBERT to classify sentence complexity\nusing anchor-based training strategy.\n\nOfficial Label Definition (from README):\n    - Label = 0: Simple (sentence annotated as simple)\n    - Label = 1: Complex (sentence annotated as complex)\n\nSource (derived from ID prefix):\n    - wiki-* : Wikipedia\n    - viki-* : Vikidia\n\nKey Features:\n    - Anchor-based training: Clean samples (short Simple + long Complex)\n    - Downsampling: Balance class imbalance in anchors\n    - ACC Calibration: Adjust for classifier bias\n    - Error Analysis: On full corpus to find real misclassifications\n    - Full prediction: P(Simple) and P(Complex) for all sentences\n\nEnvironment: Kaggle Notebook with GPU\nDataset Path: /kaggle/input/dataset-cleaned/\n\nNote: Requires transformers >= 4.46.0 (uses eval_strategy parameter)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.357722Z","iopub.execute_input":"2025-11-30T08:53:02.358496Z","iopub.status.idle":"2025-11-30T08:53:02.363235Z","shell.execute_reply.started":"2025-11-30T08:53:02.358473Z","shell.execute_reply":"2025-11-30T08:53:02.362564Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'\\niDEM Task 2: Complex vs Simple Classifier using Small Transformer\\n==================================================================\\n\\nThis script fine-tunes DistilBERT to classify sentence complexity\\nusing anchor-based training strategy.\\n\\nOfficial Label Definition (from README):\\n    - Label = 0: Simple (sentence annotated as simple)\\n    - Label = 1: Complex (sentence annotated as complex)\\n\\nSource (derived from ID prefix):\\n    - wiki-* : Wikipedia\\n    - viki-* : Vikidia\\n\\nKey Features:\\n    - Anchor-based training: Clean samples (short Simple + long Complex)\\n    - Downsampling: Balance class imbalance in anchors\\n    - ACC Calibration: Adjust for classifier bias\\n    - Error Analysis: On full corpus to find real misclassifications\\n    - Full prediction: P(Simple) and P(Complex) for all sentences\\n\\nEnvironment: Kaggle Notebook with GPU\\nDataset Path: /kaggle/input/dataset-cleaned/\\n\\nNote: Requires transformers >= 4.46.0 (uses eval_strategy parameter)\\n'"},"metadata":{}}],"execution_count":34},{"id":"cd1fcf88","cell_type":"markdown","source":"============================================================\nCell 1: Setup and Imports\n============================================================","metadata":{}},{"id":"9919412b","cell_type":"code","source":"import gc\nimport json\nimport os\nimport random\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.364540Z","iopub.execute_input":"2025-11-30T08:53:02.364727Z","iopub.status.idle":"2025-11-30T08:53:02.377063Z","shell.execute_reply.started":"2025-11-30T08:53:02.364713Z","shell.execute_reply":"2025-11-30T08:53:02.376415Z"}},"outputs":[],"execution_count":35},{"id":"a34d5fc3","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import (\n    accuracy_score,\n    average_precision_score,\n    confusion_matrix,\n    f1_score,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n)\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.377666Z","iopub.execute_input":"2025-11-30T08:53:02.377894Z","iopub.status.idle":"2025-11-30T08:53:02.389178Z","shell.execute_reply.started":"2025-11-30T08:53:02.377878Z","shell.execute_reply":"2025-11-30T08:53:02.388622Z"}},"outputs":[],"execution_count":36},{"id":"0fc5a090","cell_type":"code","source":"# Hugging Face imports\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.390030Z","iopub.execute_input":"2025-11-30T08:53:02.390269Z","iopub.status.idle":"2025-11-30T08:53:02.406602Z","shell.execute_reply.started":"2025-11-30T08:53:02.390249Z","shell.execute_reply":"2025-11-30T08:53:02.405972Z"}},"outputs":[],"execution_count":37},{"id":"619b7b27","cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.408140Z","iopub.execute_input":"2025-11-30T08:53:02.408305Z","iopub.status.idle":"2025-11-30T08:53:02.419669Z","shell.execute_reply.started":"2025-11-30T08:53:02.408293Z","shell.execute_reply":"2025-11-30T08:53:02.419115Z"}},"outputs":[],"execution_count":38},{"id":"1dd3282f","cell_type":"code","source":"# Check GPU availability\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"lines_to_next_cell":2,"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.420303Z","iopub.execute_input":"2025-11-30T08:53:02.420560Z","iopub.status.idle":"2025-11-30T08:53:02.462480Z","shell.execute_reply.started":"2025-11-30T08:53:02.420545Z","shell.execute_reply":"2025-11-30T08:53:02.461795Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":39},{"id":"bc9497d5","cell_type":"markdown","source":"============================================================\nCell 2: Configuration (English Only)\n============================================================","metadata":{}},{"id":"966f7b28","cell_type":"code","source":"# Kaggle dataset path\nINPUT_DIR = Path(\"/kaggle/input/dataset-cleaned\")\nOUTPUT_DIR = Path(\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.463195Z","iopub.execute_input":"2025-11-30T08:53:02.463446Z","iopub.status.idle":"2025-11-30T08:53:02.474904Z","shell.execute_reply.started":"2025-11-30T08:53:02.463421Z","shell.execute_reply":"2025-11-30T08:53:02.474346Z"}},"outputs":[],"execution_count":40},{"id":"a363108b","cell_type":"code","source":"# Create output directories\nRESULTS_DIR = OUTPUT_DIR / \"results\"\nMODELS_DIR = OUTPUT_DIR / \"models\"\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\nMODELS_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.475503Z","iopub.execute_input":"2025-11-30T08:53:02.475725Z","iopub.status.idle":"2025-11-30T08:53:02.487871Z","shell.execute_reply.started":"2025-11-30T08:53:02.475705Z","shell.execute_reply":"2025-11-30T08:53:02.487309Z"}},"outputs":[],"execution_count":41},{"id":"405e064b","cell_type":"code","source":"# Dataset file (English only)\nDATA_FILE = \"En-Dataset_cleaned.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.488495Z","iopub.execute_input":"2025-11-30T08:53:02.488674Z","iopub.status.idle":"2025-11-30T08:53:02.500952Z","shell.execute_reply.started":"2025-11-30T08:53:02.488659Z","shell.execute_reply":"2025-11-30T08:53:02.500287Z"}},"outputs":[],"execution_count":42},{"id":"efeb3651","cell_type":"code","source":"# Model configuration\nMODEL_CONFIG = {\n    \"name\": \"distilbert-base-uncased\",\n    \"max_length\": 128,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.501635Z","iopub.execute_input":"2025-11-30T08:53:02.501883Z","iopub.status.idle":"2025-11-30T08:53:02.513066Z","shell.execute_reply.started":"2025-11-30T08:53:02.501862Z","shell.execute_reply":"2025-11-30T08:53:02.512403Z"}},"outputs":[],"execution_count":43},{"id":"30e2e7f7","cell_type":"code","source":"# Training configuration\nTRAIN_CONFIG = {\n    \"batch_size\": 32,\n    \"learning_rate\": 2e-5,\n    \"num_epochs\": 3,\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"seed\": 42,\n    \"downsample_ratio\": 2.0,  # Majority = N * Minority\n    \"min_anchor_samples\": 100,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.513891Z","iopub.execute_input":"2025-11-30T08:53:02.514183Z","iopub.status.idle":"2025-11-30T08:53:02.526139Z","shell.execute_reply.started":"2025-11-30T08:53:02.514163Z","shell.execute_reply":"2025-11-30T08:53:02.525534Z"}},"outputs":[],"execution_count":44},{"id":"a6e37a43","cell_type":"code","source":"# Analysis settings\nHIGH_CONF_THRESHOLD = 0.9\nLOW_CONF_THRESHOLD = 0.1\nBOUNDARY_RANGE = (0.45, 0.55)\nERROR_SAMPLE_SIZE = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.526714Z","iopub.execute_input":"2025-11-30T08:53:02.527001Z","iopub.status.idle":"2025-11-30T08:53:02.539558Z","shell.execute_reply.started":"2025-11-30T08:53:02.526978Z","shell.execute_reply":"2025-11-30T08:53:02.538926Z"}},"outputs":[],"execution_count":45},{"id":"a3591145","cell_type":"markdown","source":"============================================================\nCell 3: Reproducibility\n============================================================","metadata":{}},{"id":"ef4f8e24","cell_type":"code","source":"def set_seed(seed: int = TRAIN_CONFIG[\"seed\"]) -> None:\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.540145Z","iopub.execute_input":"2025-11-30T08:53:02.540340Z","iopub.status.idle":"2025-11-30T08:53:02.552579Z","shell.execute_reply.started":"2025-11-30T08:53:02.540326Z","shell.execute_reply":"2025-11-30T08:53:02.551934Z"}},"outputs":[],"execution_count":46},{"id":"b02ed751","cell_type":"code","source":"set_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.554370Z","iopub.execute_input":"2025-11-30T08:53:02.554565Z","iopub.status.idle":"2025-11-30T08:53:02.565900Z","shell.execute_reply.started":"2025-11-30T08:53:02.554552Z","shell.execute_reply":"2025-11-30T08:53:02.565321Z"}},"outputs":[],"execution_count":47},{"id":"cbba82d2","cell_type":"markdown","source":"============================================================\nCell 4: Data Loading\n============================================================","metadata":{}},{"id":"4d7b8bee","cell_type":"code","source":"def load_data() -> pd.DataFrame:\n    \"\"\"\n    Load cleaned dataset.\n    \n    Official Label Definition:\n        - Label = 0: Simple\n        - Label = 1: Complex\n    \n    Source (from ID prefix):\n        - wiki-* : Wikipedia\n        - viki-* : Vikidia\n    \"\"\"\n    filepath = INPUT_DIR / DATA_FILE\n    print(f\"\\nLoading: {filepath}\")\n    \n    df = pd.read_csv(filepath)\n    \n    # Add source column based on ID prefix\n    df['source'] = df['ID'].apply(\n        lambda x: 'wiki' if str(x).startswith('wiki-') else 'viki'\n    )\n    \n    # Statistics by Label (complexity)\n    n_simple = (df['Label'] == 0).sum()\n    n_complex = (df['Label'] == 1).sum()\n    \n    # Statistics by Source\n    n_wiki = (df['source'] == 'wiki').sum()\n    n_viki = (df['source'] == 'viki').sum()\n    \n    print(f\"\\n  Total sentences: {len(df):,}\")\n    print(f\"\\n  By Complexity (Label):\")\n    print(f\"    Simple (Label=0):  {n_simple:,} ({n_simple/len(df)*100:.1f}%)\")\n    print(f\"    Complex (Label=1): {n_complex:,} ({n_complex/len(df)*100:.1f}%)\")\n    print(f\"\\n  By Source:\")\n    print(f\"    Wikipedia: {n_wiki:,} ({n_wiki/len(df)*100:.1f}%)\")\n    print(f\"    Vikidia:   {n_viki:,} ({n_viki/len(df)*100:.1f}%)\")\n    \n    # Cross-tabulation\n    print(f\"\\n  Cross-tabulation (Source x Label):\")\n    cross_tab = pd.crosstab(df['source'], df['Label'], margins=True)\n    cross_tab.columns = ['Simple(0)', 'Complex(1)', 'Total']\n    cross_tab.index = ['Vikidia', 'Wikipedia', 'Total']\n    print(cross_tab.to_string())\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.567036Z","iopub.execute_input":"2025-11-30T08:53:02.567203Z","iopub.status.idle":"2025-11-30T08:53:02.576238Z","shell.execute_reply.started":"2025-11-30T08:53:02.567191Z","shell.execute_reply":"2025-11-30T08:53:02.575664Z"}},"outputs":[],"execution_count":48},{"id":"5aa48164","cell_type":"markdown","source":"============================================================\nCell 5: Anchor Selection and Downsampling\n============================================================","metadata":{}},{"id":"78dd4b48","cell_type":"code","source":"def select_anchors(\n    df: pd.DataFrame,\n    downsample_ratio: float = TRAIN_CONFIG[\"downsample_ratio\"],\n    min_samples: int = TRAIN_CONFIG[\"min_anchor_samples\"]\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"\n    Select clean anchor samples for training.\n    \n    Simple Anchor: Label=0 (Simple) with LengthWords <= Q1\n    Complex Anchor: Label=1 (Complex) with LengthWords >= Q3\n    \n    Downsample majority class to balance.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ANCHOR SELECTION\")\n    print(\"=\" * 60)\n    \n    # Calculate quartiles\n    q1 = df['LengthWords'].quantile(0.25)\n    q3 = df['LengthWords'].quantile(0.75)\n    \n    print(f\"LengthWords quartiles: Q1={q1:.1f}, Q3={q3:.1f}\")\n    \n    # Select anchors based on CORRECT label semantics\n    # Label=0 is Simple, Label=1 is Complex\n    simple_mask = (df['Label'] == 0) & (df['LengthWords'] <= q1)\n    complex_mask = (df['Label'] == 1) & (df['LengthWords'] >= q3)\n    \n    simple_anchors = df[simple_mask].copy()\n    complex_anchors = df[complex_mask].copy()\n    \n    n_simple = len(simple_anchors)\n    n_complex = len(complex_anchors)\n    \n    # Validation\n    if n_simple < min_samples:\n        raise ValueError(\n            f\"Insufficient simple anchors: {n_simple} < {min_samples}\"\n        )\n    if n_complex < min_samples:\n        raise ValueError(\n            f\"Insufficient complex anchors: {n_complex} < {min_samples}\"\n        )\n    \n    print(f\"\\nBefore downsampling:\")\n    print(f\"  Simple anchors (Label=0, LengthWords <= {q1:.1f}): {n_simple:,}\")\n    print(f\"  Complex anchors (Label=1, LengthWords >= {q3:.1f}): {n_complex:,}\")\n    \n    # Determine majority class and downsample\n    if n_simple > n_complex:\n        minority_count = n_complex\n        majority_class = \"simple\"\n        max_majority = int(minority_count * downsample_ratio)\n        if n_simple > max_majority:\n            simple_anchors = simple_anchors.sample(\n                n=max_majority,\n                random_state=TRAIN_CONFIG[\"seed\"]\n            )\n            print(f\"\\nDownsampling Simple: {n_simple:,} -> {len(simple_anchors):,}\")\n    else:\n        minority_count = n_simple\n        majority_class = \"complex\"\n        max_majority = int(minority_count * downsample_ratio)\n        if n_complex > max_majority:\n            complex_anchors = complex_anchors.sample(\n                n=max_majority,\n                random_state=TRAIN_CONFIG[\"seed\"]\n            )\n            print(f\"\\nDownsampling Complex: {n_complex:,} -> {len(complex_anchors):,}\")\n    \n    # Combine and shuffle\n    anchor_df = pd.concat([simple_anchors, complex_anchors], ignore_index=True)\n    anchor_df = anchor_df.sample(\n        frac=1, random_state=TRAIN_CONFIG[\"seed\"]\n    ).reset_index(drop=True)\n    \n    n_simple_final = (anchor_df['Label'] == 0).sum()\n    n_complex_final = (anchor_df['Label'] == 1).sum()\n    \n    print(f\"\\nFinal anchor set:\")\n    print(f\"  Total: {len(anchor_df):,}\")\n    print(f\"  Simple (Label=0): {n_simple_final:,}\")\n    print(f\"  Complex (Label=1): {n_complex_final:,}\")\n    print(f\"  Ratio: {max(n_simple_final, n_complex_final) / min(n_simple_final, n_complex_final):.2f}x\")\n    \n    anchor_info = {\n        'q1': float(q1),\n        'q3': float(q3),\n        'n_simple_original': int(n_simple),\n        'n_complex_original': int(n_complex),\n        'n_simple_final': int(n_simple_final),\n        'n_complex_final': int(n_complex_final),\n        'majority_class': majority_class,\n        'downsample_ratio': downsample_ratio\n    }\n    \n    return anchor_df, anchor_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.577679Z","iopub.execute_input":"2025-11-30T08:53:02.577924Z","iopub.status.idle":"2025-11-30T08:53:02.593788Z","shell.execute_reply.started":"2025-11-30T08:53:02.577909Z","shell.execute_reply":"2025-11-30T08:53:02.593250Z"}},"outputs":[],"execution_count":49},{"id":"3c7c58fb","cell_type":"code","source":"def split_anchor_data(\n    anchor_df: pd.DataFrame,\n    train_ratio: float = 0.8,\n    val_ratio: float = 0.1,\n    test_ratio: float = 0.1\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Split anchor data into train/val/test with stratification.\"\"\"\n    print(f\"\\nSplitting anchors ({train_ratio:.0%}/{val_ratio:.0%}/{test_ratio:.0%})...\")\n    \n    train_df, temp_df = train_test_split(\n        anchor_df,\n        test_size=(val_ratio + test_ratio),\n        stratify=anchor_df['Label'],\n        random_state=TRAIN_CONFIG[\"seed\"]\n    )\n    \n    val_df, test_df = train_test_split(\n        temp_df,\n        test_size=test_ratio / (val_ratio + test_ratio),\n        stratify=temp_df['Label'],\n        random_state=TRAIN_CONFIG[\"seed\"]\n    )\n    \n    print(f\"  Train: {len(train_df):,} (Simple: {(train_df['Label']==0).sum():,}, Complex: {(train_df['Label']==1).sum():,})\")\n    print(f\"  Val:   {len(val_df):,} (Simple: {(val_df['Label']==0).sum():,}, Complex: {(val_df['Label']==1).sum():,})\")\n    print(f\"  Test:  {len(test_df):,} (Simple: {(test_df['Label']==0).sum():,}, Complex: {(test_df['Label']==1).sum():,})\")\n    \n    return train_df, val_df, test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.594452Z","iopub.execute_input":"2025-11-30T08:53:02.595036Z","iopub.status.idle":"2025-11-30T08:53:02.609933Z","shell.execute_reply.started":"2025-11-30T08:53:02.595013Z","shell.execute_reply":"2025-11-30T08:53:02.609399Z"}},"outputs":[],"execution_count":50},{"id":"df888fcb","cell_type":"markdown","source":"============================================================\nCell 6: Tokenization and Dataset Preparation\n============================================================","metadata":{}},{"id":"9252506b","cell_type":"code","source":"def prepare_datasets(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    tokenizer,\n    max_length: int\n) -> Tuple[Dataset, Dataset, Dataset]:\n    \"\"\"Prepare HuggingFace datasets with tokenization.\"\"\"\n    print(\"\\nPreparing datasets...\")\n    \n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"Sentence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length,\n        )\n    \n    # Convert to HuggingFace Dataset\n    train_ds = Dataset.from_pandas(train_df[['Sentence', 'Label']].reset_index(drop=True))\n    val_ds = Dataset.from_pandas(val_df[['Sentence', 'Label']].reset_index(drop=True))\n    test_ds = Dataset.from_pandas(test_df[['Sentence', 'Label']].reset_index(drop=True))\n    \n    # Tokenize\n    train_ds = train_ds.map(tokenize_function, batched=True, desc=\"Tokenizing train\")\n    val_ds = val_ds.map(tokenize_function, batched=True, desc=\"Tokenizing val\")\n    test_ds = test_ds.map(tokenize_function, batched=True, desc=\"Tokenizing test\")\n    \n    # Rename label column for Trainer\n    train_ds = train_ds.rename_column(\"Label\", \"labels\")\n    val_ds = val_ds.rename_column(\"Label\", \"labels\")\n    test_ds = test_ds.rename_column(\"Label\", \"labels\")\n    \n    # Set format\n    columns = ['input_ids', 'attention_mask', 'labels']\n    train_ds.set_format(type=\"torch\", columns=columns)\n    val_ds.set_format(type=\"torch\", columns=columns)\n    test_ds.set_format(type=\"torch\", columns=columns)\n    \n    print(f\"  Train: {len(train_ds):,}, Val: {len(val_ds):,}, Test: {len(test_ds):,}\")\n    \n    return train_ds, val_ds, test_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.610555Z","iopub.execute_input":"2025-11-30T08:53:02.610729Z","iopub.status.idle":"2025-11-30T08:53:02.629140Z","shell.execute_reply.started":"2025-11-30T08:53:02.610716Z","shell.execute_reply":"2025-11-30T08:53:02.628595Z"}},"outputs":[],"execution_count":51},{"id":"a1ee23b3","cell_type":"markdown","source":"============================================================\nCell 7: Custom Trainer with Class Weights\n============================================================","metadata":{}},{"id":"20fe5725","cell_type":"code","source":"class WeightedTrainer(Trainer):\n    \"\"\"Custom Trainer with class-weighted loss.\"\"\"\n    \n    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n    \n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        weights = self.class_weights.to(logits.device)\n        loss_fct = nn.CrossEntropyLoss(weight=weights)\n        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.629730Z","iopub.execute_input":"2025-11-30T08:53:02.630359Z","iopub.status.idle":"2025-11-30T08:53:02.644610Z","shell.execute_reply.started":"2025-11-30T08:53:02.630343Z","shell.execute_reply":"2025-11-30T08:53:02.644060Z"}},"outputs":[],"execution_count":52},{"id":"93ca6a13","cell_type":"code","source":"def compute_class_weights(train_df: pd.DataFrame) -> torch.Tensor:\n    \"\"\"Compute balanced class weights.\"\"\"\n    label_counts = train_df['Label'].value_counts().sort_index()\n    n_simple = label_counts[0]  # Label=0 is Simple\n    n_complex = label_counts[1]  # Label=1 is Complex\n    total = n_simple + n_complex\n    \n    weight_simple = total / (2 * n_simple)\n    weight_complex = total / (2 * n_complex)\n    \n    weights = torch.tensor([weight_simple, weight_complex], dtype=torch.float32)\n    print(f\"\\nClass weights: [Simple(0): {weight_simple:.4f}, Complex(1): {weight_complex:.4f}]\")\n    \n    return weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.645286Z","iopub.execute_input":"2025-11-30T08:53:02.645837Z","iopub.status.idle":"2025-11-30T08:53:02.660639Z","shell.execute_reply.started":"2025-11-30T08:53:02.645816Z","shell.execute_reply":"2025-11-30T08:53:02.659958Z"}},"outputs":[],"execution_count":53},{"id":"3627ef1f","cell_type":"markdown","source":"============================================================\nCell 8: Metrics Computation\n============================================================","metadata":{}},{"id":"96a57f63","cell_type":"code","source":"def compute_metrics(eval_pred) -> Dict:\n    \"\"\"\n    Compute evaluation metrics.\n    \n    Label semantics:\n        - Label=0: Simple\n        - Label=1: Complex (positive class for ROC-AUC)\n    \"\"\"\n    logits, labels = eval_pred\n    \n    # Compute probabilities\n    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n    prob_simple = probs[:, 0]\n    prob_complex = probs[:, 1]\n    \n    # Predictions: Complex (Label=1) is positive class\n    preds = (prob_complex >= 0.5).astype(int)\n    \n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n        \"f1_simple\": f1_score(labels, preds, pos_label=0),\n        \"f1_complex\": f1_score(labels, preds, pos_label=1),\n        \"precision_weighted\": precision_score(labels, preds, average=\"weighted\"),\n        \"recall_weighted\": recall_score(labels, preds, average=\"weighted\"),\n        \"roc_auc\": roc_auc_score(labels, prob_complex),  # prob of positive class\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.662407Z","iopub.execute_input":"2025-11-30T08:53:02.662670Z","iopub.status.idle":"2025-11-30T08:53:02.674071Z","shell.execute_reply.started":"2025-11-30T08:53:02.662655Z","shell.execute_reply":"2025-11-30T08:53:02.673403Z"}},"outputs":[],"execution_count":54},{"id":"77a0461b","cell_type":"markdown","source":"============================================================\nCell 9: Training Function\n============================================================","metadata":{}},{"id":"1dd51bfd","cell_type":"code","source":"def train_model(\n    train_ds: Dataset,\n    val_ds: Dataset,\n    tokenizer,\n    class_weights: torch.Tensor\n) -> Tuple[Trainer, object]:\n    \"\"\"Train transformer model.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"TRAINING: {MODEL_CONFIG['name']}\")\n    print(\"=\" * 60)\n    \n    # Load model with CORRECT label mapping\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_CONFIG['name'],\n        num_labels=2,\n        id2label={0: \"Simple\", 1: \"Complex\"},\n        label2id={\"Simple\": 0, \"Complex\": 1}\n    )\n    \n    output_dir = MODELS_DIR / \"transformer_en\"\n    \n    training_args = TrainingArguments(\n        output_dir=str(output_dir),\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=TRAIN_CONFIG[\"learning_rate\"],\n        per_device_train_batch_size=TRAIN_CONFIG[\"batch_size\"],\n        per_device_eval_batch_size=TRAIN_CONFIG[\"batch_size\"] * 2,\n        num_train_epochs=TRAIN_CONFIG[\"num_epochs\"],\n        weight_decay=TRAIN_CONFIG[\"weight_decay\"],\n        warmup_ratio=TRAIN_CONFIG[\"warmup_ratio\"],\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1_weighted\",\n        greater_is_better=True,\n        logging_steps=100,\n        save_total_limit=2,\n        report_to=\"none\",\n        seed=TRAIN_CONFIG[\"seed\"],\n        fp16=torch.cuda.is_available(),\n    )\n    \n    trainer = WeightedTrainer(\n        class_weights=class_weights,\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n    )\n    \n    print(\"\\nStarting training...\")\n    trainer.train()\n    \n    return trainer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.674846Z","iopub.execute_input":"2025-11-30T08:53:02.675074Z","iopub.status.idle":"2025-11-30T08:53:02.691358Z","shell.execute_reply.started":"2025-11-30T08:53:02.675054Z","shell.execute_reply":"2025-11-30T08:53:02.690792Z"}},"outputs":[],"execution_count":55},{"id":"67e88ba3","cell_type":"markdown","source":"============================================================\nCell 10: Evaluation on Test Set\n============================================================","metadata":{}},{"id":"6a45e2ee","cell_type":"code","source":"def evaluate_on_test(\n    trainer: Trainer,\n    test_ds: Dataset,\n    test_df: pd.DataFrame\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict]:\n    \"\"\"\n    Evaluate model on test set.\n    \n    Returns:\n        prob_simple, prob_complex, preds, labels, metrics\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"EVALUATION ON ANCHOR TEST SET\")\n    print(\"=\" * 60)\n    \n    pred_output = trainer.predict(test_ds)\n    logits = pred_output.predictions\n    labels = pred_output.label_ids\n    \n    # Compute probabilities\n    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n    prob_simple = probs[:, 0]\n    prob_complex = probs[:, 1]\n    \n    # Predictions: pred_label matches official Label semantics\n    preds = (prob_complex >= 0.5).astype(int)\n    \n    # Compute metrics\n    metrics = {\n        \"accuracy\": float(accuracy_score(labels, preds)),\n        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\")),\n        \"f1_simple\": float(f1_score(labels, preds, pos_label=0)),\n        \"f1_complex\": float(f1_score(labels, preds, pos_label=1)),\n        \"precision_simple\": float(precision_score(labels, preds, pos_label=0)),\n        \"precision_complex\": float(precision_score(labels, preds, pos_label=1)),\n        \"recall_simple\": float(recall_score(labels, preds, pos_label=0)),\n        \"recall_complex\": float(recall_score(labels, preds, pos_label=1)),\n        \"roc_auc\": float(roc_auc_score(labels, prob_complex)),\n        \"average_precision\": float(average_precision_score(labels, prob_complex)),\n    }\n    \n    print(f\"\\nTest Metrics:\")\n    print(f\"  Accuracy:            {metrics['accuracy']:.4f}\")\n    print(f\"  F1 (weighted):       {metrics['f1_weighted']:.4f}\")\n    print(f\"  F1 (Simple, Label=0):  {metrics['f1_simple']:.4f}\")\n    print(f\"  F1 (Complex, Label=1): {metrics['f1_complex']:.4f}\")\n    print(f\"  ROC-AUC:             {metrics['roc_auc']:.4f}\")\n    \n    # Confusion matrix\n    cm = confusion_matrix(labels, preds)\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"                    Pred Simple(0)  Pred Complex(1)\")\n    print(f\"  Actual Simple(0)  {cm[0,0]:>12,}  {cm[0,1]:>14,}\")\n    print(f\"  Actual Complex(1) {cm[1,0]:>12,}  {cm[1,1]:>14,}\")\n    \n    return prob_simple, prob_complex, preds, labels, metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.692134Z","iopub.execute_input":"2025-11-30T08:53:02.692839Z","iopub.status.idle":"2025-11-30T08:53:02.709087Z","shell.execute_reply.started":"2025-11-30T08:53:02.692817Z","shell.execute_reply":"2025-11-30T08:53:02.708507Z"}},"outputs":[],"execution_count":56},{"id":"2cf644d4","cell_type":"markdown","source":"============================================================\nCell 11: Full Dataset Prediction\n============================================================","metadata":{}},{"id":"4df6319d","cell_type":"code","source":"def predict_full_dataset(\n    trainer: Trainer,\n    full_df: pd.DataFrame,\n    tokenizer,\n    max_length: int\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Predict probabilities for all sentences.\n    \n    Returns:\n        prob_simple, prob_complex\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FULL DATASET PREDICTION\")\n    print(\"=\" * 60)\n    \n    print(f\"Predicting for {len(full_df):,} sentences...\")\n    \n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"Sentence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length,\n        )\n    \n    full_ds = Dataset.from_pandas(full_df[['Sentence']].reset_index(drop=True))\n    full_ds = full_ds.map(tokenize_function, batched=True, desc=\"Tokenizing full\")\n    full_ds.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n    \n    pred_output = trainer.predict(full_ds)\n    logits = pred_output.predictions\n    \n    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n    prob_simple = probs[:, 0]\n    prob_complex = probs[:, 1]\n    \n    print(f\"\\n  P(Simple) - Mean: {prob_simple.mean():.4f}, Std: {prob_simple.std():.4f}\")\n    print(f\"  P(Complex) - Mean: {prob_complex.mean():.4f}, Std: {prob_complex.std():.4f}\")\n    print(f\"  Predicted Simple (P>=0.5): {(prob_simple >= 0.5).mean():.4f}\")\n    print(f\"  Predicted Complex (P>=0.5): {(prob_complex >= 0.5).mean():.4f}\")\n    \n    return prob_simple, prob_complex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.709764Z","iopub.execute_input":"2025-11-30T08:53:02.710013Z","iopub.status.idle":"2025-11-30T08:53:02.725154Z","shell.execute_reply.started":"2025-11-30T08:53:02.709998Z","shell.execute_reply":"2025-11-30T08:53:02.724553Z"}},"outputs":[],"execution_count":57},{"id":"8aae9d27","cell_type":"markdown","source":"============================================================\nCell 12: ACC Calibration\n============================================================","metadata":{}},{"id":"687e1176","cell_type":"code","source":"def compute_acc_from_test(\n    test_labels: np.ndarray,\n    test_preds: np.ndarray\n) -> Tuple[float, float, Dict]:\n    \"\"\"\n    Compute TPR and FPR from anchor test set for ACC calibration.\n    \n    Positive class = Complex (Label=1)\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ACC CALIBRATION (from anchor test set)\")\n    print(\"=\" * 60)\n    \n    cm = confusion_matrix(test_labels, test_preds)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # For Complex as positive class (Label=1):\n    # TPR = TP / (TP + FN) = correctly identified Complex\n    # FPR = FP / (FP + TN) = Simple misclassified as Complex\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n    \n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  TN (Simple correct)={tn:,}, FP (Simple->Complex)={fp:,}\")\n    print(f\"  FN (Complex->Simple)={fn:,}, TP (Complex correct)={tp:,}\")\n    print(f\"\\nRates (Complex as positive):\")\n    print(f\"  TPR (Recall Complex): {tpr:.4f}\")\n    print(f\"  FPR (Fall-out):       {fpr:.4f}\")\n    print(f\"  TNR (Recall Simple):  {tnr:.4f}\")\n    print(f\"  FNR (Miss rate):      {fnr:.4f}\")\n    \n    calibration_info = {\n        'method': 'anchor_test_set',\n        'n_samples': int(len(test_labels)),\n        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n        'tpr': float(tpr),\n        'fpr': float(fpr),\n        'tnr': float(tnr),\n        'fnr': float(fnr)\n    }\n    \n    return tpr, fpr, calibration_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.725835Z","iopub.execute_input":"2025-11-30T08:53:02.726213Z","iopub.status.idle":"2025-11-30T08:53:02.741701Z","shell.execute_reply.started":"2025-11-30T08:53:02.726198Z","shell.execute_reply":"2025-11-30T08:53:02.741226Z"}},"outputs":[],"execution_count":58},{"id":"45c546f4","cell_type":"code","source":"def apply_acc_correction(p_pred: float, tpr: float, fpr: float) -> float:\n    \"\"\"Apply ACC formula: p_true = (p_pred - FPR) / (TPR - FPR)\"\"\"\n    denominator = tpr - fpr\n    if abs(denominator) < 1e-10:\n        print(\"  [WARNING] TPR ~= FPR, using p_pred as fallback\")\n        return p_pred\n    p_true = (p_pred - fpr) / denominator\n    return max(0.0, min(1.0, p_true))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.742451Z","iopub.execute_input":"2025-11-30T08:53:02.742676Z","iopub.status.idle":"2025-11-30T08:53:02.757685Z","shell.execute_reply.started":"2025-11-30T08:53:02.742654Z","shell.execute_reply":"2025-11-30T08:53:02.757080Z"}},"outputs":[],"execution_count":59},{"id":"1463b6cb","cell_type":"markdown","source":"============================================================\nCell 13: Error Analysis on Full Corpus\n============================================================","metadata":{}},{"id":"22ab7bc7","cell_type":"code","source":"def analyze_errors_on_full_corpus(\n    full_df: pd.DataFrame,\n    prob_simple: np.ndarray,\n    prob_complex: np.ndarray,\n    sample_size: int = ERROR_SAMPLE_SIZE\n) -> Dict:\n    \"\"\"\n    Analyze prediction errors on FULL corpus.\n    \n    This finds real misclassifications, unlike anchor test set\n    which may have 100% accuracy due to clean separation.\n    \n    Label semantics:\n        - Label=0: Simple\n        - Label=1: Complex\n        - pred_label = (prob_complex >= 0.5)\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ERROR ANALYSIS ON FULL CORPUS\")\n    print(\"=\" * 60)\n    \n    # Build analysis dataframe\n    analysis_df = full_df.copy()\n    analysis_df['prob_simple'] = prob_simple\n    analysis_df['prob_complex'] = prob_complex\n    analysis_df['pred_label'] = (prob_complex >= 0.5).astype(int)\n    analysis_df['correct'] = analysis_df['Label'] == analysis_df['pred_label']\n    \n    errors_df = analysis_df[~analysis_df['correct']]\n    \n    n_total = len(analysis_df)\n    n_errors = len(errors_df)\n    error_rate = n_errors / n_total if n_total > 0 else 0\n    \n    print(f\"\\nTotal errors: {n_errors:,} / {n_total:,} ({error_rate:.2%})\")\n    \n    results = {\n        'total_samples': n_total,\n        'total_errors': n_errors,\n        'error_rate': float(error_rate),\n        'false_positives': [],  # Pred=Complex but True=Simple\n        'false_negatives': [],  # Pred=Simple but True=Complex\n        'boundary_cases': []\n    }\n    \n    # False Positives: pred_label=1 (Complex) but Label=0 (Simple)\n    # Simple sentences misclassified as Complex\n    fp_df = errors_df[(errors_df['pred_label'] == 1) & (errors_df['Label'] == 0)]\n    fp_high_conf = fp_df[fp_df['prob_complex'] >= HIGH_CONF_THRESHOLD]\n    fp_sample = fp_df.sort_values('prob_complex', ascending=False).head(sample_size)\n    \n    print(f\"\\n[1] FALSE POSITIVES (pred=Complex, true=Simple)\")\n    print(f\"    Total FP: {len(fp_df):,}\")\n    print(f\"    High confidence FP (P(Complex) >= {HIGH_CONF_THRESHOLD}): {len(fp_high_conf):,}\")\n    print(f\"    These Simple sentences were predicted as Complex\")\n    \n    # By source breakdown\n    fp_wiki = len(fp_df[fp_df['source'] == 'wiki'])\n    fp_viki = len(fp_df[fp_df['source'] == 'viki'])\n    print(f\"    By source: Wikipedia={fp_wiki:,}, Vikidia={fp_viki:,}\")\n    \n    for i, (_, row) in enumerate(fp_sample.iterrows(), 1):\n        sample = {\n            'sentence': row['Sentence'][:200],\n            'source': row['source'],\n            'true_label': int(row['Label']),\n            'pred_label': int(row['pred_label']),\n            'prob_simple': float(row['prob_simple']),\n            'prob_complex': float(row['prob_complex']),\n            'length_words': int(row['LengthWords']) if 'LengthWords' in row else None\n        }\n        results['false_positives'].append(sample)\n        print(f\"\\n    FP-{i} [P(Complex)={row['prob_complex']:.3f}] [Source={row['source']}] [Len={row.get('LengthWords', 'N/A')}]:\")\n        print(f\"    {row['Sentence'][:150]}...\")\n    \n    # False Negatives: pred_label=0 (Simple) but Label=1 (Complex)\n    # Complex sentences misclassified as Simple\n    fn_df = errors_df[(errors_df['pred_label'] == 0) & (errors_df['Label'] == 1)]\n    fn_high_conf = fn_df[fn_df['prob_simple'] >= HIGH_CONF_THRESHOLD]\n    fn_sample = fn_df.sort_values('prob_simple', ascending=False).head(sample_size)\n    \n    print(f\"\\n[2] FALSE NEGATIVES (pred=Simple, true=Complex)\")\n    print(f\"    Total FN: {len(fn_df):,}\")\n    print(f\"    High confidence FN (P(Simple) >= {HIGH_CONF_THRESHOLD}): {len(fn_high_conf):,}\")\n    print(f\"    These Complex sentences were predicted as Simple\")\n    \n    fn_wiki = len(fn_df[fn_df['source'] == 'wiki'])\n    fn_viki = len(fn_df[fn_df['source'] == 'viki'])\n    print(f\"    By source: Wikipedia={fn_wiki:,}, Vikidia={fn_viki:,}\")\n    \n    for i, (_, row) in enumerate(fn_sample.iterrows(), 1):\n        sample = {\n            'sentence': row['Sentence'][:200],\n            'source': row['source'],\n            'true_label': int(row['Label']),\n            'pred_label': int(row['pred_label']),\n            'prob_simple': float(row['prob_simple']),\n            'prob_complex': float(row['prob_complex']),\n            'length_words': int(row['LengthWords']) if 'LengthWords' in row else None\n        }\n        results['false_negatives'].append(sample)\n        print(f\"\\n    FN-{i} [P(Simple)={row['prob_simple']:.3f}] [Source={row['source']}] [Len={row.get('LengthWords', 'N/A')}]:\")\n        print(f\"    {row['Sentence'][:150]}...\")\n    \n    # Boundary cases: P close to 0.5\n    boundary_df = analysis_df[\n        (analysis_df['prob_simple'] >= BOUNDARY_RANGE[0]) &\n        (analysis_df['prob_simple'] <= BOUNDARY_RANGE[1])\n    ].copy()\n    \n    if len(boundary_df) > 0:\n        boundary_df['dist_from_05'] = (boundary_df['prob_simple'] - 0.5).abs()\n        boundary_sample = boundary_df.nsmallest(sample_size, 'dist_from_05')\n    else:\n        boundary_sample = boundary_df\n    \n    print(f\"\\n[3] BOUNDARY CASES (P(Simple) in {BOUNDARY_RANGE})\")\n    print(f\"    Total boundary: {len(boundary_df):,}\")\n    \n    for i, (_, row) in enumerate(boundary_sample.iterrows(), 1):\n        sample = {\n            'sentence': row['Sentence'][:200],\n            'source': row['source'],\n            'true_label': int(row['Label']),\n            'pred_label': int(row['pred_label']),\n            'prob_simple': float(row['prob_simple']),\n            'correct': bool(row['correct'])\n        }\n        results['boundary_cases'].append(sample)\n        status = \"CORRECT\" if row['correct'] else \"WRONG\"\n        label_name = \"Simple\" if row['Label'] == 0 else \"Complex\"\n        print(f\"\\n    B-{i} [P(Simple)={row['prob_simple']:.3f}] [{status}] [True={label_name}]:\")\n        print(f\"    {row['Sentence'][:150]}...\")\n    \n    # Summary\n    results['summary'] = {\n        'total_fp': len(fp_df),\n        'total_fn': len(fn_df),\n        'high_conf_fp': len(fp_high_conf),\n        'high_conf_fn': len(fn_high_conf),\n        'total_boundary': len(boundary_df),\n        'fp_by_source': {'wiki': fp_wiki, 'viki': fp_viki},\n        'fn_by_source': {'wiki': fn_wiki, 'viki': fn_viki}\n    }\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.758322Z","iopub.execute_input":"2025-11-30T08:53:02.758639Z","iopub.status.idle":"2025-11-30T08:53:02.775013Z","shell.execute_reply.started":"2025-11-30T08:53:02.758616Z","shell.execute_reply":"2025-11-30T08:53:02.774484Z"}},"outputs":[],"execution_count":60},{"id":"e3da0fe6","cell_type":"markdown","source":"============================================================\nCell 14: Final Estimates\n============================================================","metadata":{}},{"id":"1f95ca2d","cell_type":"code","source":"def compute_final_estimates(\n    full_df: pd.DataFrame,\n    prob_simple: np.ndarray,\n    prob_complex: np.ndarray,\n    tpr: float,\n    fpr: float\n) -> Dict:\n    \"\"\"\n    Compute final prevalence estimates with ACC correction.\n    \n    Label semantics:\n        - Label=0: Simple\n        - Label=1: Complex (positive class for ACC)\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FINAL ESTIMATES\")\n    print(\"=\" * 60)\n    \n    # Naive estimate from labels\n    naive_simple = (full_df['Label'] == 0).mean()\n    naive_complex = (full_df['Label'] == 1).mean()\n    \n    # Predicted proportions\n    pred_simple = (prob_simple >= 0.5).mean()\n    pred_complex = (prob_complex >= 0.5).mean()\n    \n    # ACC correction for Complex proportion\n    # p_true_complex = (p_pred_complex - FPR) / (TPR - FPR)\n    acc_complex = apply_acc_correction(pred_complex, tpr, fpr)\n    acc_simple = 1.0 - acc_complex\n    \n    print(f\"\\n1. GLOBAL SENTENCE COMPLEXITY:\")\n    print(f\"   Naive (from Label):\")\n    print(f\"     Simple (Label=0): {naive_simple:.4f} ({naive_simple*100:.2f}%)\")\n    print(f\"     Complex (Label=1): {naive_complex:.4f} ({naive_complex*100:.2f}%)\")\n    print(f\"   Predicted (P >= 0.5):\")\n    print(f\"     Simple: {pred_simple:.4f} ({pred_simple*100:.2f}%)\")\n    print(f\"     Complex: {pred_complex:.4f} ({pred_complex*100:.2f}%)\")\n    print(f\"   ACC-corrected:\")\n    print(f\"     Simple: {acc_simple:.4f} ({acc_simple*100:.2f}%)\")\n    print(f\"     Complex: {acc_complex:.4f} ({acc_complex*100:.2f}%)\")\n    \n    # Wikipedia internal analysis\n    wiki_mask = full_df['source'] == 'wiki'\n    wiki_prob_simple = prob_simple[wiki_mask]\n    wiki_prob_complex = prob_complex[wiki_mask]\n    n_wiki = wiki_mask.sum()\n    \n    wiki_pred_simple = (wiki_prob_simple >= 0.5).mean()\n    wiki_high_simple = (wiki_prob_simple >= HIGH_CONF_THRESHOLD).mean()\n    \n    print(f\"\\n2. WIKIPEDIA INTERNAL (Source=wiki):\")\n    print(f\"   Total sentences: {n_wiki:,}\")\n    print(f\"   Soft estimate (mean P(Simple)): {wiki_prob_simple.mean():.4f}\")\n    print(f\"   Predicted Simple (P >= 0.5): {wiki_pred_simple:.4f} ({wiki_pred_simple*100:.2f}%)\")\n    print(f\"   High conf Simple (P >= {HIGH_CONF_THRESHOLD}): {wiki_high_simple:.4f} ({wiki_high_simple*100:.2f}%)\")\n    \n    # Vikidia internal analysis\n    viki_mask = full_df['source'] == 'viki'\n    viki_prob_simple = prob_simple[viki_mask]\n    viki_prob_complex = prob_complex[viki_mask]\n    n_viki = viki_mask.sum()\n    \n    viki_pred_complex = (viki_prob_complex >= 0.5).mean()\n    viki_high_complex = (viki_prob_complex >= HIGH_CONF_THRESHOLD).mean()\n    \n    print(f\"\\n3. VIKIDIA INTERNAL (Source=viki):\")\n    print(f\"   Total sentences: {n_viki:,}\")\n    print(f\"   Soft estimate (mean P(Complex)): {viki_prob_complex.mean():.4f}\")\n    print(f\"   Predicted Complex (P >= 0.5): {viki_pred_complex:.4f} ({viki_pred_complex*100:.2f}%)\")\n    print(f\"   High conf Complex (P >= {HIGH_CONF_THRESHOLD}): {viki_high_complex:.4f} ({viki_high_complex*100:.2f}%)\")\n    \n    estimates = {\n        'naive': {\n            'simple': float(naive_simple),\n            'complex': float(naive_complex)\n        },\n        'predicted': {\n            'simple': float(pred_simple),\n            'complex': float(pred_complex)\n        },\n        'acc_corrected': {\n            'simple': float(acc_simple),\n            'complex': float(acc_complex)\n        },\n        'wikipedia_analysis': {\n            'n_sentences': int(n_wiki),\n            'mean_prob_simple': float(wiki_prob_simple.mean()),\n            'pred_simple_rate': float(wiki_pred_simple),\n            'high_conf_simple_rate': float(wiki_high_simple)\n        },\n        'vikidia_analysis': {\n            'n_sentences': int(n_viki),\n            'mean_prob_complex': float(viki_prob_complex.mean()),\n            'pred_complex_rate': float(viki_pred_complex),\n            'high_conf_complex_rate': float(viki_high_complex)\n        }\n    }\n    \n    return estimates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.776913Z","iopub.execute_input":"2025-11-30T08:53:02.777395Z","iopub.status.idle":"2025-11-30T08:53:02.792374Z","shell.execute_reply.started":"2025-11-30T08:53:02.777378Z","shell.execute_reply":"2025-11-30T08:53:02.791740Z"}},"outputs":[],"execution_count":61},{"id":"75e45d8b","cell_type":"markdown","source":"============================================================\nCell 15: Main Pipeline\n============================================================","metadata":{}},{"id":"07d3c6ce","cell_type":"code","source":"def main() -> Dict:\n    \"\"\"Main execution pipeline for English only.\"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"iDEM TASK 2: TRANSFORMER FINE-TUNING (English)\")\n    print(\"Anchor-based Training + ACC Calibration\")\n    print(\"=\" * 70)\n    print(f\"Input:  {INPUT_DIR}\")\n    print(f\"Output: {OUTPUT_DIR}\")\n    print(f\"Model:  {MODEL_CONFIG['name']}\")\n    print(f\"Seed:   {TRAIN_CONFIG['seed']}\")\n    print(\"=\" * 70)\n    \n    set_seed()\n    \n    # Load data\n    full_df = load_data()\n    \n    # Select and split anchors\n    anchor_df, anchor_info = select_anchors(full_df)\n    train_df, val_df, test_df = split_anchor_data(anchor_df)\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['name'])\n    \n    # Prepare datasets\n    train_ds, val_ds, test_ds = prepare_datasets(\n        train_df, val_df, test_df,\n        tokenizer, MODEL_CONFIG['max_length']\n    )\n    \n    # Compute class weights\n    class_weights = compute_class_weights(train_df)\n    \n    # Train model\n    trainer, model = train_model(train_ds, val_ds, tokenizer, class_weights)\n    \n    # Evaluate on anchor test set\n    prob_simple_test, prob_complex_test, preds_test, labels_test, test_metrics = \\\n        evaluate_on_test(trainer, test_ds, test_df)\n    \n    # ACC calibration\n    tpr, fpr, calibration_info = compute_acc_from_test(labels_test, preds_test)\n    \n    # Full dataset prediction\n    prob_simple_full, prob_complex_full = predict_full_dataset(\n        trainer, full_df, tokenizer, MODEL_CONFIG['max_length']\n    )\n    \n    # Error analysis on FULL corpus (not anchor test set)\n    error_analysis = analyze_errors_on_full_corpus(\n        full_df, prob_simple_full, prob_complex_full\n    )\n    \n    # Final estimates\n    estimates = compute_final_estimates(\n        full_df, prob_simple_full, prob_complex_full, tpr, fpr\n    )\n    \n    # Save predictions\n    pred_df = full_df[['ID', 'Sentence', 'Label', 'source', 'LengthWords']].copy()\n    pred_df['prob_simple'] = prob_simple_full\n    pred_df['prob_complex'] = prob_complex_full\n    pred_df['pred_label'] = (prob_complex_full >= 0.5).astype(int)\n    pred_df.to_csv(RESULTS_DIR / \"transformer_predictions_en.csv\", index=False)\n    print(f\"\\nPredictions saved: {RESULTS_DIR / 'transformer_predictions_en.csv'}\")\n    \n    # Compile results\n    results = {\n        'model': MODEL_CONFIG['name'],\n        'dataset_info': {\n            'total_sentences': len(full_df),\n            'simple_count': int((full_df['Label'] == 0).sum()),\n            'complex_count': int((full_df['Label'] == 1).sum()),\n            'wikipedia_count': int((full_df['source'] == 'wiki').sum()),\n            'vikidia_count': int((full_df['source'] == 'viki').sum())\n        },\n        'anchor_info': anchor_info,\n        'test_metrics': test_metrics,\n        'calibration': calibration_info,\n        'estimates': estimates,\n        'error_analysis': error_analysis\n    }\n    \n    # Save results JSON\n    results_file = RESULTS_DIR / \"transformer_results_en.json\"\n    with open(results_file, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, default=str, ensure_ascii=False)\n    print(f\"Results saved: {results_file}\")\n    \n    # Print final summary\n    print(\"\\n\" + \"=\" * 70)\n    print(\"FINAL SUMMARY\")\n    print(\"=\" * 70)\n    print(f\"\\nAnchor Test Metrics:\")\n    print(f\"  Accuracy:     {test_metrics['accuracy']:.4f}\")\n    print(f\"  F1 (weighted): {test_metrics['f1_weighted']:.4f}\")\n    print(f\"  ROC-AUC:      {test_metrics['roc_auc']:.4f}\")\n    print(f\"\\nACC Calibration:\")\n    print(f\"  TPR: {calibration_info['tpr']:.4f}\")\n    print(f\"  FPR: {calibration_info['fpr']:.4f}\")\n    print(f\"\\nPrevalence Estimates:\")\n    print(f\"  Naive Simple:       {estimates['naive']['simple']:.4f}\")\n    print(f\"  ACC-corrected Simple: {estimates['acc_corrected']['simple']:.4f}\")\n    print(f\"\\nError Analysis (Full Corpus):\")\n    print(f\"  Total errors: {error_analysis['total_errors']:,} ({error_analysis['error_rate']:.2%})\")\n    print(f\"  FP (Simple->Complex): {error_analysis['summary']['total_fp']:,}\")\n    print(f\"  FN (Complex->Simple): {error_analysis['summary']['total_fn']:,}\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"TASK 2 COMPLETE!\")\n    print(\"=\" * 70)\n    \n    # Cleanup\n    del model, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.793075Z","iopub.execute_input":"2025-11-30T08:53:02.793309Z","iopub.status.idle":"2025-11-30T08:53:02.809964Z","shell.execute_reply.started":"2025-11-30T08:53:02.793287Z","shell.execute_reply":"2025-11-30T08:53:02.809297Z"}},"outputs":[],"execution_count":62},{"id":"58e5b280","cell_type":"markdown","source":"============================================================\nCell 16: Run\n============================================================","metadata":{}},{"id":"3edc6cc5","cell_type":"code","source":"if __name__ == \"__main__\":\n    results = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:53:02.810742Z","iopub.execute_input":"2025-11-30T08:53:02.811056Z","iopub.status.idle":"2025-11-30T09:08:56.752165Z","shell.execute_reply.started":"2025-11-30T08:53:02.811024Z","shell.execute_reply":"2025-11-30T09:08:56.751566Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\niDEM TASK 2: TRANSFORMER FINE-TUNING (English)\nAnchor-based Training + ACC Calibration\n======================================================================\nInput:  /kaggle/input/dataset-cleaned\nOutput: /kaggle/working\nModel:  distilbert-base-uncased\nSeed:   42\n======================================================================\n\nLoading: /kaggle/input/dataset-cleaned/En-Dataset_cleaned.csv\n\n  Total sentences: 289,781\n\n  By Complexity (Label):\n    Simple (Label=0):  17,305 (6.0%)\n    Complex (Label=1): 272,476 (94.0%)\n\n  By Source:\n    Wikipedia: 272,476 (94.0%)\n    Vikidia:   17,305 (6.0%)\n\n  Cross-tabulation (Source x Label):\n           Simple(0)  Complex(1)   Total\nVikidia        17305           0   17305\nWikipedia          0      272476  272476\nTotal          17305      272476  289781\n\n============================================================\nANCHOR SELECTION\n============================================================\nLengthWords quartiles: Q1=16.0, Q3=29.0\n\nBefore downsampling:\n  Simple anchors (Label=0, LengthWords <= 16.0): 8,773\n  Complex anchors (Label=1, LengthWords >= 29.0): 73,032\n\nDownsampling Complex: 73,032 -> 17,546\n\nFinal anchor set:\n  Total: 26,319\n  Simple (Label=0): 8,773\n  Complex (Label=1): 17,546\n  Ratio: 2.00x\n\nSplitting anchors (80%/10%/10%)...\n  Train: 21,055 (Simple: 7,018, Complex: 14,037)\n  Val:   2,632 (Simple: 877, Complex: 1,755)\n  Test:  2,632 (Simple: 878, Complex: 1,754)\n\nPreparing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing train:   0%|          | 0/21055 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7029e4eb80496badeb81e7e1817dd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing val:   0%|          | 0/2632 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74572b8ffa964c46a9659d9855b309ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing test:   0%|          | 0/2632 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be914099f4df4a9f92a20ef42e425827"}},"metadata":{}},{"name":"stdout","text":"  Train: 21,055, Val: 2,632, Test: 2,632\n\nClass weights: [Simple(0): 1.5001, Complex(1): 0.7500]\n\n============================================================\nTRAINING: distilbert-base-uncased\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1974' max='1974' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1974/1974 06:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Weighted</th>\n      <th>F1 Simple</th>\n      <th>F1 Complex</th>\n      <th>Precision Weighted</th>\n      <th>Recall Weighted</th>\n      <th>Roc Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000700</td>\n      <td>0.004415</td>\n      <td>0.998480</td>\n      <td>0.998481</td>\n      <td>0.997725</td>\n      <td>0.998859</td>\n      <td>0.998487</td>\n      <td>0.998480</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000100</td>\n      <td>0.000080</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000300</td>\n      <td>0.000064</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nEVALUATION ON ANCHOR TEST SET\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nTest Metrics:\n  Accuracy:            1.0000\n  F1 (weighted):       1.0000\n  F1 (Simple, Label=0):  1.0000\n  F1 (Complex, Label=1): 1.0000\n  ROC-AUC:             1.0000\n\nConfusion Matrix:\n                    Pred Simple(0)  Pred Complex(1)\n  Actual Simple(0)           878               0\n  Actual Complex(1)            0           1,754\n\n============================================================\nACC CALIBRATION (from anchor test set)\n============================================================\n\nConfusion Matrix:\n  TN (Simple correct)=878, FP (Simple->Complex)=0\n  FN (Complex->Simple)=0, TP (Complex correct)=1,754\n\nRates (Complex as positive):\n  TPR (Recall Complex): 1.0000\n  FPR (Fall-out):       0.0000\n  TNR (Recall Simple):  1.0000\n  FNR (Miss rate):      0.0000\n\n============================================================\nFULL DATASET PREDICTION\n============================================================\nPredicting for 289,781 sentences...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing full:   0%|          | 0/289781 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f0c7d5affd248568a657f5a3b2d9b21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n  P(Simple) - Mean: 0.5697, Std: 0.4921\n  P(Complex) - Mean: 0.4303, Std: 0.4921\n  Predicted Simple (P>=0.5): 0.5697\n  Predicted Complex (P>=0.5): 0.4303\n\n============================================================\nERROR ANALYSIS ON FULL CORPUS\n============================================================\n\nTotal errors: 154,526 / 289,781 (53.33%)\n\n[1] FALSE POSITIVES (pred=Complex, true=Simple)\n    Total FP: 3,372\n    High confidence FP (P(Complex) >= 0.9): 3,287\n    These Simple sentences were predicted as Complex\n    By source: Wikipedia=0, Vikidia=3,372\n\n    FP-1 [P(Complex)=1.000] [Source=viki] [Len=39]:\n    The secondary waves are recorded by the seismograph for the second because they have lower speed to the first and also the transverse, as they vibrate...\n\n    FP-2 [P(Complex)=1.000] [Source=viki] [Len=34]:\n    However, rather that settle into a legal career he became more involved in (talk or information that tries to change people's minds) efforts, and the ...\n\n    FP-3 [P(Complex)=1.000] [Source=viki] [Len=53]:\n    Efn According to the historian Francisco Vidal Castro, gold coins bearing Yusuf's name had particularly beautiful designs, many of which are still fou...\n\n    FP-4 [P(Complex)=1.000] [Source=viki] [Len=66]:\n    The ratio of a number with the precedent (for example 34/21) approaches the golden ratio, which is approximately 1.618 and is found in the nature and ...\n\n    FP-5 [P(Complex)=1.000] [Source=viki] [Len=37]:\n    Bells are also associated with Santa Claus, as a special type of bell known as a Sleigh bell is said to be part of the dressing of his Sleigh, and wor...\n\n    FP-6 [P(Complex)=1.000] [Source=viki] [Len=35]:\n    In England, a similar protest against the Pope, first political and later religious, led to the Church of England who call themselves Protestant, but ...\n\n    FP-7 [P(Complex)=1.000] [Source=viki] [Len=38]:\n    It is thought by some researchers that the original source of this script was the Egyptian hieratic script, which by the late Middle Kingdom (about 19...\n\n    FP-8 [P(Complex)=1.000] [Source=viki] [Len=35]:\n    Both were often referred to simply as \"the gymnasium\".[2] Greek historian, biographer, and essayistPlutarch writes that a palaestra was used solely fo...\n\n    FP-9 [P(Complex)=1.000] [Source=viki] [Len=48]:\n    The four journeys of Columbus  With the invention of the printing press, Columbus was fascinated with the small Earth theory, the theory that the Eart...\n\n    FP-10 [P(Complex)=1.000] [Source=viki] [Len=41]:\n    There were many wars between the cities, and it was not until 338 BC (338 'before Christ') when they were all united to fight against the Persians und...\n\n[2] FALSE NEGATIVES (pred=Simple, true=Complex)\n    Total FN: 151,154\n    High confidence FN (P(Simple) >= 0.9): 149,382\n    These Complex sentences were predicted as Simple\n    By source: Wikipedia=151,154, Vikidia=0\n\n    FN-1 [P(Simple)=1.000] [Source=wiki] [Len=14]:\n    The famous painting of Juan Luna, the Spoliarium, can be found in the complex....\n\n    FN-2 [P(Simple)=1.000] [Source=wiki] [Len=14]:\n    Beginning in 1834, it visited the American Rendezvous to buy furs at low prices....\n\n    FN-3 [P(Simple)=1.000] [Source=wiki] [Len=12]:\n    The policy brought the Dutch enormous wealth and made the colony self-sufficient....\n\n    FN-4 [P(Simple)=1.000] [Source=wiki] [Len=14]:\n    In the eighteenth century, Russia and Prussia became the leading powers over the sea....\n\n    FN-5 [P(Simple)=1.000] [Source=wiki] [Len=14]:\n    The plantations were highly profitable, due to the heavy European demand for raw cotton....\n\n    FN-6 [P(Simple)=1.000] [Source=wiki] [Len=13]:\n    In literature, the most successful figure of the mid-19th century was Walter Scott....\n\n    FN-7 [P(Simple)=1.000] [Source=wiki] [Len=14]:\n    , The city experiences the typical climate of northern Vietnam, with four distinct seasons....\n\n    FN-8 [P(Simple)=1.000] [Source=wiki] [Len=13]:\n    In recent years, those bicycle makers have greatly changed their methods of production....\n\n    FN-9 [P(Simple)=1.000] [Source=wiki] [Len=15]:\n    In the late 18th century, the blue uniform became a symbol of liberty and revolution....\n\n    FN-10 [P(Simple)=1.000] [Source=wiki] [Len=16]:\n    In a few words, that is, Taylor pronounced an epitaph for the entire Whig economic program....\n\n[3] BOUNDARY CASES (P(Simple) in (0.45, 0.55))\n    Total boundary: 330\n\n    B-1 [P(Simple)=0.500] [CORRECT] [True=Complex]:\n    Anguish and grief, like darkness and rain, may be depicted; but gladness and joy, like the rainbow, defy the skill of pen or pencil....\n\n    B-2 [P(Simple)=0.500] [CORRECT] [True=Complex]:\n    Italian immigrants brought New York-style pizza and Italian cuisine into the city, while Jewish immigrants and Irish immigrants brought pastrami and c...\n\n    B-3 [P(Simple)=0.500] [CORRECT] [True=Complex]:\n    Judaism   The history of Judaism in Turkey goes back to the Romaniote Jews of Anatolia who have been present since at least the 5th century BC....\n\n    B-4 [P(Simple)=0.500] [CORRECT] [True=Complex]:\n    At the head of the procession rode Lincoln on a gray horse preceded by a military band that was the first the young boy had ever seen....\n\n    B-5 [P(Simple)=0.499] [CORRECT] [True=Complex]:\n    Breaches occurred regardless: one at Henglong in 1034 divided the course in three and repeatedly flooded the northern regions of Dezhou and Bozhou....\n\n    B-6 [P(Simple)=0.499] [CORRECT] [True=Complex]:\n    Political beginnings   Trudeau supported the Liberal Party from a young age, offering his support to party leader John Turner in the 1988 federal elec...\n\n    B-7 [P(Simple)=0.499] [CORRECT] [True=Complex]:\n    However, this appellation is traditionally given to Sir Robert Walpole, who became First Lord of the Treasury of Great Britain in 1721....\n\n    B-8 [P(Simple)=0.501] [WRONG] [True=Complex]:\n    Tolkien undertook various textual revisions to produce a version of the book that would be published with his consent and establish an unquestioned US...\n\n    B-9 [P(Simple)=0.501] [WRONG] [True=Complex]:\n    The reason behind this was the division among the tribes and the absence of a strong central authority in Yemen during the days of the prophet....\n\n    B-10 [P(Simple)=0.499] [CORRECT] [True=Complex]:\n    A small hole is left for the passage of urine and menstrual blood, and the vagina is opened up for sexual intercourse and childbirth....\n\n============================================================\nFINAL ESTIMATES\n============================================================\n\n1. GLOBAL SENTENCE COMPLEXITY:\n   Naive (from Label):\n     Simple (Label=0): 0.0597 (5.97%)\n     Complex (Label=1): 0.9403 (94.03%)\n   Predicted (P >= 0.5):\n     Simple: 0.5697 (56.97%)\n     Complex: 0.4303 (43.03%)\n   ACC-corrected:\n     Simple: 0.5697 (56.97%)\n     Complex: 0.4303 (43.03%)\n\n2. WIKIPEDIA INTERNAL (Source=wiki):\n   Total sentences: 272,476\n   Soft estimate (mean P(Simple)): 0.5547\n   Predicted Simple (P >= 0.5): 0.5547 (55.47%)\n   High conf Simple (P >= 0.9): 0.5482 (54.82%)\n\n3. VIKIDIA INTERNAL (Source=viki):\n   Total sentences: 17,305\n   Soft estimate (mean P(Complex)): 0.1949\n   Predicted Complex (P >= 0.5): 0.1949 (19.49%)\n   High conf Complex (P >= 0.9): 0.1899 (18.99%)\n\nPredictions saved: /kaggle/working/results/transformer_predictions_en.csv\nResults saved: /kaggle/working/results/transformer_results_en.json\n\n======================================================================\nFINAL SUMMARY\n======================================================================\n\nAnchor Test Metrics:\n  Accuracy:     1.0000\n  F1 (weighted): 1.0000\n  ROC-AUC:      1.0000\n\nACC Calibration:\n  TPR: 1.0000\n  FPR: 0.0000\n\nPrevalence Estimates:\n  Naive Simple:       0.0597\n  ACC-corrected Simple: 0.5697\n\nError Analysis (Full Corpus):\n  Total errors: 154,526 (53.33%)\n  FP (Simple->Complex): 3,372\n  FN (Complex->Simple): 151,154\n\n======================================================================\nTASK 2 COMPLETE!\n======================================================================\n","output_type":"stream"}],"execution_count":63}]}