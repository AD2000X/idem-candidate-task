{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a87305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\niDEM Task 1 — Estimate the true proportion of simple sentences\\n============================================================================\\n\\nThis script estimates the true proportion of simple sentences in the dataset,\\naccounting for label noise using anchor-based training and ACC calibration.\\n\\nLabel Definition (Used in this script):\\n    - Label = 0: Simple (Vikidia-style)\\n    - Label = 1: Complex (Wikipedia-style)\\n\\nNaive estimate implementation:\\n    - Naive proportion = #(Label = 0) / total\\n\\nPipeline:\\n    1. Naive Estimate: Direct label proportion (baseline)\\n    2. Anchor Selection: Use extreme samples for clean training data\\n    3. Model Training: LR + RF with full ML pipeline\\n    4. Full Prediction: Predict P(Simple) for all sentences\\n    5. ACC Calibration: Adjust for classifier bias\\n    6. Final Estimates: True proportion + Wikipedia internal simple ratio\\n    7. Additional Analysis: Noise candidates, anchor quality, and stratified prevalence\\n\\nUsage:\\n    python 02_estimate_simplified_proportion.py\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "iDEM Task 1 — Estimate the true proportion of simple sentences\n",
    "============================================================================\n",
    "\n",
    "This script estimates the true proportion of simple sentences in the dataset,\n",
    "accounting for label noise using anchor-based training and ACC calibration.\n",
    "\n",
    "Label Definition (Used in this script):\n",
    "    - Label = 0: Simple (Vikidia-style)\n",
    "    - Label = 1: Complex (Wikipedia-style)\n",
    "\n",
    "Naive estimate implementation:\n",
    "    - Naive proportion = #(Label = 0) / total\n",
    "\n",
    "Pipeline:\n",
    "    1. Naive Estimate: Direct label proportion (baseline)\n",
    "    2. Anchor Selection: Use extreme samples for clean training data\n",
    "    3. Model Training: LR + RF with full ML pipeline\n",
    "    4. Full Prediction: Predict P(Simple) for all sentences\n",
    "    5. ACC Calibration: Adjust for classifier bias\n",
    "    6. Final Estimates: True proportion + Wikipedia internal simple ratio\n",
    "    7. Additional Analysis: Noise candidates, anchor quality, and stratified prevalence\n",
    "\n",
    "Usage:\n",
    "    python 02_estimate_simplified_proportion.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b3ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b38041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_predict,\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "511f113a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0980007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "FEATURES_DIR = BASE_DIR / \"features\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "LOGS_DIR = BASE_DIR / \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28f697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d65f1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required local files (no download - must exist locally)\n",
    "# Now using only feature files which contain all necessary data\n",
    "FEATURE_FILES = {\n",
    "    \"en\": \"en_full_features.csv\",\n",
    "    \"fr\": \"fr_full_features.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a293a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop from features (metadata + target + anchor-selection-only)\n",
    "DROP_COLUMNS = [\n",
    "    'Index', 'ID', 'Name', 'Sentence',\n",
    "    'Label',\n",
    "    'LengthWords', 'LengthChars',\n",
    "    'source'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9de7f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation threshold for feature removal\n",
    "CORRELATION_THRESHOLD = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3244357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f42dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC calibration settings\n",
    "ACC_CV_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a03d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High confidence threshold for Wikipedia analysis (hard estimate)\n",
    "HIGH_CONF_THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0848b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High confidence threshold for mislabel candidate mining\n",
    "MISLABEL_HIGH_CONF_THRESHOLD = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2f0f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "def set_seed(seed: int = RANDOM_SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ea2ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Detection\n",
    "def find_file_recursive(\n",
    "    filename: str,\n",
    "    search_dir: Path,\n",
    "    max_depth: int = 3\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"Recursively search for a file in directory tree.\"\"\"\n",
    "    target = search_dir / filename\n",
    "    if target.exists():\n",
    "        return target\n",
    "\n",
    "    if max_depth > 0:\n",
    "        try:\n",
    "            for subdir in search_dir.iterdir():\n",
    "                if subdir.is_dir() and not subdir.name.startswith('.'):\n",
    "                    result = find_file_recursive(filename, subdir, max_depth - 1)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except PermissionError:\n",
    "            pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea0dd444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_local_files_exist() -> None:\n",
    "    \"\"\"Check that all required local files exist.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CHECKING LOCAL FILES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    required_files = [\n",
    "        (FEATURES_DIR, FEATURE_FILES[\"en\"]),\n",
    "        (FEATURES_DIR, FEATURE_FILES[\"fr\"]),\n",
    "    ]\n",
    "\n",
    "    for target_dir, filename in required_files:\n",
    "        target_path = target_dir / filename\n",
    "\n",
    "        if target_path.exists():\n",
    "            print(f\"  [OK] {target_path}\")\n",
    "            continue\n",
    "\n",
    "        # Search recursively\n",
    "        print(f\"  [SEARCH] Looking for {filename}...\")\n",
    "        found = find_file_recursive(filename, BASE_DIR)\n",
    "\n",
    "        if found:\n",
    "            target_dir.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(found, target_path)\n",
    "            print(f\"  [COPIED] {found} -> {target_path}\")\n",
    "            continue\n",
    "\n",
    "        # Not found - raise error\n",
    "        raise FileNotFoundError(\n",
    "            f\"\\n  [ERROR] Missing required file: {filename}\\n\"\n",
    "            f\"  Please place it in: {target_dir}\\n\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nAll required files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cad506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "def load_features(lang: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load pre-extracted features.\n",
    "    This file contains all necessary data including metadata, labels, and features.\n",
    "    \"\"\"\n",
    "    filename = FEATURE_FILES[lang]\n",
    "    filepath = FEATURES_DIR / filename\n",
    "\n",
    "    print(f\"\\nLoading features: {filepath}\")\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        total_rows = sum(1 for _ in f) - 1\n",
    "\n",
    "    chunks = []\n",
    "    with tqdm(total=total_rows, desc=f\"Reading {filename}\",\n",
    "              ncols=80, unit=\" rows\") as pbar:\n",
    "        for chunk in pd.read_csv(filepath, chunksize=50000):\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    features_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    # Add source column based on ID prefix\n",
    "    features_df['source'] = features_df['ID'].apply(\n",
    "        lambda x: 'wiki' if str(x).startswith('wiki-') else 'viki'\n",
    "    )\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c1eaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Naive Estimate\n",
    "def compute_naive_proportion(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute naive estimate: Count(Label=0) / Total.\n",
    "\n",
    "    Label Definition:\n",
    "        - Label = 0: Simple (Vikidia-style)\n",
    "        - Label = 1: Complex (Wikipedia-style)\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    simple_count = (df['Label'] == 0).sum()\n",
    "\n",
    "    if total == 0:\n",
    "        raise ValueError(\"No rows in dataset\")\n",
    "\n",
    "    return simple_count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e31ceb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Feature Preprocessing\n",
    "def prepare_features(\n",
    "    features_df: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    print(\"\\nPreparing features...\")\n",
    "\n",
    "    # Keep LengthWords temporarily for anchor selection\n",
    "    cols_to_drop_now = [c for c in DROP_COLUMNS if c != 'LengthWords']\n",
    "    cols_to_drop_now = [c for c in cols_to_drop_now if c in features_df.columns]\n",
    "\n",
    "    features_clean = features_df.drop(columns=cols_to_drop_now, errors='ignore')\n",
    "\n",
    "    print(f\"  Dropped columns: {cols_to_drop_now}\")\n",
    "    print(f\"  Remaining columns: {list(features_clean.columns)}\")\n",
    "\n",
    "    return features_clean, features_clean.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "272c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(\n",
    "    features_df: pd.DataFrame,\n",
    "    multiplier: float = 3.0\n",
    ") -> pd.DataFrame:\n",
    "    print(\"\\nHandling outliers (IQR clipping)...\")\n",
    "\n",
    "    numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "    n_clipped = 0\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        Q1 = features_df[col].quantile(0.25)\n",
    "        Q3 = features_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower = Q1 - multiplier * IQR\n",
    "        upper = Q3 + multiplier * IQR\n",
    "\n",
    "        outliers = ((features_df[col] < lower) | (features_df[col] > upper)).sum()\n",
    "        if outliers > 0:\n",
    "            features_df[col] = features_df[col].clip(lower, upper)\n",
    "            n_clipped += 1\n",
    "\n",
    "    print(f\"  Clipped outliers in {n_clipped} columns\")\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5fa88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features_df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    print(\"\\nPreprocessing features...\")\n",
    "\n",
    "    original_count = len(features_df.columns)\n",
    "\n",
    "    # Remove blank columns\n",
    "    features_df = features_df.loc[:, features_df.notna().any()]\n",
    "\n",
    "    # Remove zero-variance columns (excluding LengthWords to keep anchor feature)\n",
    "    variance = features_df.drop(columns=['LengthWords'], errors='ignore').var()\n",
    "    zero_var = variance[variance == 0].index.tolist()\n",
    "    if zero_var:\n",
    "        print(f\"  Removed zero-variance: {zero_var}\")\n",
    "        features_df = features_df.drop(columns=zero_var)\n",
    "\n",
    "    # Fill NaN with median\n",
    "    nan_cols = features_df.columns[features_df.isna().any()].tolist()\n",
    "    if nan_cols:\n",
    "        print(f\"  Filling NaN in: {nan_cols}\")\n",
    "        for col in nan_cols:\n",
    "            features_df[col] = features_df[col].fillna(features_df[col].median())\n",
    "\n",
    "    print(f\"  Features: {original_count} -> {len(features_df.columns)}\")\n",
    "\n",
    "    return features_df, features_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f64c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_correlation(\n",
    "    features_df: pd.DataFrame,\n",
    "    threshold: float = CORRELATION_THRESHOLD,\n",
    "    protected_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    print(f\"\\nRemoving high correlation (threshold={threshold})...\")\n",
    "\n",
    "    # Protect certain columns from removal (e.g., LengthWords needed for anchor selection)\n",
    "    if protected_cols is None:\n",
    "        protected_cols = ['LengthWords']\n",
    "\n",
    "    corr_matrix = features_df.corr().abs()\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "\n",
    "    cols_to_drop = set()\n",
    "    for col in upper_tri.columns:\n",
    "        correlated = upper_tri.index[upper_tri[col] > threshold].tolist()\n",
    "        cols_to_drop.update(correlated)\n",
    "\n",
    "    # Remove protected columns from drop list\n",
    "    cols_to_drop = cols_to_drop - set(protected_cols)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        print(f\"  Removing {len(cols_to_drop)} columns: {cols_to_drop}\")\n",
    "        features_df = features_df.drop(columns=list(cols_to_drop))\n",
    "    else:\n",
    "        print(\"  No highly correlated features found\")\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa994e77",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Part 2: Anchor Selection\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af1e80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_anchors(\n",
    "    df: pd.DataFrame,\n",
    "    features_df: pd.DataFrame\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.Index, Dict]:\n",
    "    \"\"\"\n",
    "    Select clean anchor samples for training.\n",
    "\n",
    "    Label Definition:\n",
    "        - Label = 0: Simple (Vikidia-style)\n",
    "        - Label = 1: Complex (Wikipedia-style)\n",
    "\n",
    "    Simple Anchor: Vikidia (Label=0) with LengthWords <= Q1\n",
    "    Complex Anchor: Wikipedia (Label=1) with LengthWords >= Q3\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANCHOR SELECTION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get LengthWords from features\n",
    "    if 'LengthWords' not in features_df.columns:\n",
    "        raise ValueError(\"LengthWords column required for anchor selection\")\n",
    "\n",
    "    length_words = features_df['LengthWords']\n",
    "\n",
    "    # Calculate quartiles\n",
    "    q1 = length_words.quantile(0.25)\n",
    "    q3 = length_words.quantile(0.75)\n",
    "\n",
    "    print(f\"\\nLengthWords quartiles: Q1={q1:.1f}, Q3={q3:.1f}\")\n",
    "\n",
    "    # Simple anchors: short Vikidia sentences (Label=0)\n",
    "    simple_mask = (df['Label'] == 0) & (length_words <= q1)\n",
    "    n_simple = simple_mask.sum()\n",
    "\n",
    "    # Complex anchors: long Wikipedia sentences (Label=1)\n",
    "    complex_mask = (df['Label'] == 1) & (length_words >= q3)\n",
    "    n_complex = complex_mask.sum()\n",
    "\n",
    "    print(f\"\\nAnchor selection:\")\n",
    "    print(f\"  Simple anchors (Vikidia/Label=0, LengthWords <= {q1:.1f}): {n_simple:,}\")\n",
    "    print(f\"  Complex anchors (Wikipedia/Label=1, LengthWords >= {q3:.1f}): {n_complex:,}\")\n",
    "\n",
    "    # Combine anchors\n",
    "    anchor_mask = simple_mask | complex_mask\n",
    "    anchor_indices = df.index[anchor_mask]\n",
    "\n",
    "    training_features = features_df.drop(columns=['LengthWords'], errors='ignore')\n",
    "    X_anchor = training_features.loc[anchor_mask].values\n",
    "    y_anchor = df.loc[anchor_mask, 'Label'].values\n",
    "\n",
    "    print(f\"  Total anchor samples: {len(anchor_indices):,}\")\n",
    "    print(f\"  Class distribution: Simple(0)={sum(y_anchor==0):,}, Complex(1)={sum(y_anchor==1):,}\")\n",
    "\n",
    "    anchor_info = {\n",
    "        'q1': float(q1),\n",
    "        'q3': float(q3),\n",
    "        'n_simple_anchors': int(n_simple),\n",
    "        'n_complex_anchors': int(n_complex),\n",
    "        'total_anchors': int(len(anchor_indices))\n",
    "    }\n",
    "\n",
    "    return X_anchor, y_anchor, anchor_indices, anchor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9cab480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Model Building and Training\n",
    "def build_lr_pipeline() -> Pipeline:\n",
    "    \"\"\"Build Logistic Regression pipeline.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "560a6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rf_pipeline() -> Pipeline:\n",
    "    \"\"\"Build Random Forest pipeline.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1faa505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(\n",
    "    pipeline: Pipeline,\n",
    "    param_distributions: Dict,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    model_name: str\n",
    ") -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"Tune model hyperparameters using RandomizedSearchCV.\"\"\"\n",
    "    print(f\"\\n  Tuning {model_name}...\")\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, param_distributions,\n",
    "        n_iter=12,\n",
    "        cv=cv, scoring='f1',\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Validation metrics\n",
    "    val_pred = search.predict(X_val)\n",
    "    val_proba = search.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    results = {\n",
    "        'best_params': search.best_params_,\n",
    "        'cv_f1': float(search.best_score_),\n",
    "        'val_f1': float(f1_score(y_val, val_pred, pos_label=0)),\n",
    "        'val_roc_auc': float(roc_auc_score(y_val, val_proba))\n",
    "    }\n",
    "\n",
    "    print(f\"    Best params: {search.best_params_}\")\n",
    "    print(f\"    CV F1: {results['cv_f1']:.4f} | Val F1: {results['val_f1']:.4f}\")\n",
    "\n",
    "    return search.best_estimator_, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b4926bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(\n",
    "    X_anchor: np.ndarray,\n",
    "    y_anchor: np.ndarray\n",
    ") -> Tuple[Pipeline, str, Dict]:\n",
    "    \"\"\"Train LR and RF models on anchor data, return best model.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL TRAINING (on anchor samples)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Split anchors into train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_anchor, y_anchor,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=y_anchor\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAnchor split: Train={len(X_train):,}, Val={len(X_val):,}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_params = {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'classifier__penalty': ['l2'],\n",
    "        'classifier__solver': ['lbfgs', 'saga']\n",
    "    }\n",
    "    lr_pipeline, lr_results = tune_model(\n",
    "        build_lr_pipeline(), lr_params,\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        \"Logistic Regression\"\n",
    "    )\n",
    "    results['logistic_regression'] = lr_results\n",
    "\n",
    "    # Random Forest\n",
    "    rf_params = {\n",
    "        'classifier__n_estimators': [50, 100, 150, 200],\n",
    "        'classifier__max_depth': [5, 10, 15, 20, 25, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10, 20],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    rf_pipeline, rf_results = tune_model(\n",
    "        build_rf_pipeline(), rf_params,\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        \"Random Forest\"\n",
    "    )\n",
    "    results['random_forest'] = rf_results\n",
    "\n",
    "    # Select best model by validation F1 (Simple=0 as positive)\n",
    "    if rf_results['val_f1'] >= lr_results['val_f1']:\n",
    "        best_model = rf_pipeline\n",
    "        best_name = 'random_forest'\n",
    "    else:\n",
    "        best_model = lr_pipeline\n",
    "        best_name = 'logistic_regression'\n",
    "\n",
    "    print(f\"\\nBest model: {best_name} (Val F1: {results[best_name]['val_f1']:.4f})\")\n",
    "\n",
    "    return best_model, best_name, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ae2da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor quality / leakage sanity check\n",
    "def label_shuffle_sanity_check(\n",
    "    X_anchor: np.ndarray,\n",
    "    y_anchor: np.ndarray,\n",
    "    max_samples: int = 50000\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Sanity check for potential leakage:\n",
    "    Shuffle labels and measure how well a RandomForest can fit.\n",
    "    If F1 remains very high, this suggests serious leakage.\n",
    "    \"\"\"\n",
    "    print(\"\\nRunning label-shuffle sanity check on anchors...\")\n",
    "\n",
    "    n = len(y_anchor)\n",
    "    if n == 0:\n",
    "        return {'n_samples_used': 0, 'mean_f1': 0.0, 'std_f1': 0.0, 'n_splits': 0}\n",
    "\n",
    "    if n > max_samples:\n",
    "        rng = np.random.RandomState(RANDOM_SEED)\n",
    "        idx = rng.choice(n, size=max_samples, replace=False)\n",
    "        X_sub = X_anchor[idx]\n",
    "        y_sub = y_anchor[idx]\n",
    "    else:\n",
    "        X_sub = X_anchor\n",
    "        y_sub = y_anchor\n",
    "\n",
    "    y_shuffled = np.random.permutation(y_sub)\n",
    "\n",
    "    model = build_rf_pipeline()\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED + 1)\n",
    "    scores = cross_val_score(\n",
    "        model, X_sub, y_shuffled,\n",
    "        cv=cv, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "\n",
    "    mean_f1 = float(scores.mean())\n",
    "    std_f1 = float(scores.std())\n",
    "\n",
    "    print(f\"  Used samples: {len(y_sub):,} / {n:,}\")\n",
    "    print(f\"  Label-shuffle RF F1 (Simple=0 as positive): mean={mean_f1:.4f}, std={std_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'n_samples_used': int(len(y_sub)),\n",
    "        'n_total_anchor_samples': int(n),\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'n_splits': 3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddbd16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_anchor_quality(\n",
    "    df: pd.DataFrame,\n",
    "    anchor_idx: pd.Index,\n",
    "    y_anchor: np.ndarray\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Summarise anchor quality: ratio, length distribution, and source breakdown.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANCHOR QUALITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    total_n = len(df)\n",
    "    anchor_n = len(anchor_idx)\n",
    "    anchor_ratio = anchor_n / total_n if total_n > 0 else 0.0\n",
    "\n",
    "    length_global = df['LengthWords']\n",
    "    length_anchor = df.loc[anchor_idx, 'LengthWords']\n",
    "\n",
    "    source_anchor = df.loc[anchor_idx, 'source'].value_counts().to_dict()\n",
    "\n",
    "    print(f\"  Anchor ratio: {anchor_n:,} / {total_n:,} = {anchor_ratio:.4f}\")\n",
    "    print(f\"  LengthWords mean (global): {length_global.mean():.2f}, std: {length_global.std():.2f}\")\n",
    "    print(f\"  LengthWords mean (anchors): {length_anchor.mean():.2f}, std: {length_anchor.std():.2f}\")\n",
    "    print(f\"  Anchor source breakdown: {source_anchor}\")\n",
    "\n",
    "    # Basic class balance in anchors\n",
    "    simple_anchors = int((y_anchor == 0).sum())\n",
    "    complex_anchors = int((y_anchor == 1).sum())\n",
    "    print(f\"  Anchor label breakdown: Simple(0)={simple_anchors:,}, Complex(1)={complex_anchors:,}\")\n",
    "\n",
    "    anchor_quality = {\n",
    "        'anchor_ratio': float(anchor_ratio),\n",
    "        'n_total': int(total_n),\n",
    "        'n_anchor': int(anchor_n),\n",
    "        'lengthwords_global_mean': float(length_global.mean()),\n",
    "        'lengthwords_global_std': float(length_global.std()),\n",
    "        'lengthwords_anchor_mean': float(length_anchor.mean()),\n",
    "        'lengthwords_anchor_std': float(length_anchor.std()),\n",
    "        'anchor_source_breakdown': source_anchor,\n",
    "        'anchor_label_breakdown': {\n",
    "            'simple_0': simple_anchors,\n",
    "            'complex_1': complex_anchors\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return anchor_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f61afe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Full Prediction\n",
    "def predict_full_dataset(\n",
    "    model: Pipeline,\n",
    "    features_df: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Predict P(Simple) for all sentences.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL DATASET PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Remove LengthWords for prediction\n",
    "    X_full = features_df.drop(columns=['LengthWords'], errors='ignore').values\n",
    "\n",
    "    print(f\"\\nPredicting probabilities for {len(X_full):,} sentences...\")\n",
    "\n",
    "    # Note: predict_proba returns [P(class_0), P(class_1)]\n",
    "    # Label=0 is Simple, so P(Simple) = predict_proba[:, 0]\n",
    "    probabilities = model.predict_proba(X_full)[:, 0]\n",
    "\n",
    "    print(f\"  Mean P(Simple): {probabilities.mean():.4f}\")\n",
    "    print(f\"  Std P(Simple): {probabilities.std():.4f}\")\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "474bb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: ACC Calibration\n",
    "def compute_acc_calibration(\n",
    "    model: Pipeline,\n",
    "    X_anchor: np.ndarray,\n",
    "    y_anchor: np.ndarray,\n",
    "    n_folds: int = ACC_CV_FOLDS\n",
    ") -> Tuple[float, float, Dict]:\n",
    "    \"\"\"\n",
    "    Compute TPR and FPR via cross-validation on anchor data.\n",
    "\n",
    "    These rates are used for ACC (Adjusted Classify and Count) calibration.\n",
    "\n",
    "    For Simple class (Label=0):\n",
    "        - TP: Correctly predicted as Simple (Label=0)\n",
    "        - FP: Incorrectly predicted as Simple (actually Complex/Label=1)\n",
    "        - TN: Correctly predicted as Complex (Label=1)\n",
    "        - FN: Incorrectly predicted as Complex (actually Simple/Label=0)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ACC CALIBRATION (Cross-Validation)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Get cross-validated predictions\n",
    "    y_pred_cv = cross_val_predict(model, X_anchor, y_anchor, cv=cv)\n",
    "\n",
    "    # Compute confusion matrix with labels=[1, 0] so that Simple(0) is positive class\n",
    "    # This gives: [[TN, FP], [FN, TP]] where positive = Simple(0)\n",
    "    cm = confusion_matrix(y_anchor, y_pred_cv, labels=[1, 0])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate rates for Simple class (Label=0)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # True Positive Rate (Recall for Simple)\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0  # False Positive Rate\n",
    "\n",
    "    print(f\"\\nConfusion Matrix (Simple=0 as positive, {n_folds}-fold CV):\")\n",
    "    print(f\"  TN={tn:,} (Complex->Complex), FP={fp:,} (Complex->Simple)\")\n",
    "    print(f\"  FN={fn:,} (Simple->Complex), TP={tp:,} (Simple->Simple)\")\n",
    "    print(f\"\\nRates for Simple class:\")\n",
    "    print(f\"  TPR (True Positive Rate): {tpr:.4f}\")\n",
    "    print(f\"  FPR (False Positive Rate): {fpr:.4f}\")\n",
    "\n",
    "    calibration_info = {\n",
    "        'n_folds': n_folds,\n",
    "        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n",
    "        'tpr': float(tpr),\n",
    "        'fpr': float(fpr)\n",
    "    }\n",
    "\n",
    "    return tpr, fpr, calibration_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16e0015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_acc_correction(\n",
    "    p_pred: float,\n",
    "    tpr: float,\n",
    "    fpr: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Apply ACC formula to correct predicted proportion.\n",
    "\n",
    "    Formula: p_true = (p_pred - FPR) / (TPR - FPR)\n",
    "    \"\"\"\n",
    "    denominator = tpr - fpr\n",
    "\n",
    "    if abs(denominator) < 1e-10:\n",
    "        print(\"  [WARNING] TPR ~= FPR, ACC correction undefined. Using p_pred.\")\n",
    "        return p_pred\n",
    "\n",
    "    p_true = (p_pred - fpr) / denominator\n",
    "\n",
    "    # Clip to valid range [0, 1]\n",
    "    p_true = max(0.0, min(1.0, p_true))\n",
    "\n",
    "    return p_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b7aeba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Final Estimates\n",
    "def compute_final_estimates(\n",
    "    df: pd.DataFrame,\n",
    "    probabilities: np.ndarray,\n",
    "    tpr: float,\n",
    "    fpr: float,\n",
    "    naive_proportion: float\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compute final prevalence estimates.\n",
    "\n",
    "    1. Adjusted true proportion (ACC-corrected)\n",
    "    2. Wikipedia internal Simple-style proportion\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL ESTIMATES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1. Predicted proportion (before ACC)\n",
    "    p_pred = (probabilities >= 0.5).mean()\n",
    "\n",
    "    # 2. ACC-corrected true proportion\n",
    "    p_true = apply_acc_correction(p_pred, tpr, fpr)\n",
    "\n",
    "    print(f\"\\n1. GLOBAL SIMPLE SENTENCE PROPORTION:\")\n",
    "    print(f\"   Naive estimate (Label=0 / Total):     {naive_proportion:.4f} ({naive_proportion*100:.2f}%)\")\n",
    "    print(f\"   Predicted proportion (P >= 0.5):      {p_pred:.4f} ({p_pred*100:.2f}%)\")\n",
    "    print(f\"   ACC-corrected true proportion:        {p_true:.4f} ({p_true*100:.2f}%)\")\n",
    "\n",
    "    # 3. Wikipedia internal analysis\n",
    "    wiki_mask = df['source'] == 'wiki'\n",
    "    wiki_probs = probabilities[wiki_mask]\n",
    "    n_wiki = wiki_mask.sum()\n",
    "\n",
    "    # Soft count: average probability\n",
    "    wiki_simple_soft = wiki_probs.mean()\n",
    "\n",
    "    # Hard count: high confidence threshold\n",
    "    wiki_simple_hard = (wiki_probs >= HIGH_CONF_THRESHOLD).mean()\n",
    "\n",
    "    print(f\"\\n2. WIKIPEDIA INTERNAL SIMPLE-STYLE PROPORTION:\")\n",
    "    print(f\"   Wikipedia sentences:                  {n_wiki:,}\")\n",
    "    print(f\"   Soft estimate (mean P(Simple)):       {wiki_simple_soft:.4f} ({wiki_simple_soft*100:.2f}%)\")\n",
    "    print(f\"   Hard estimate (P >= {HIGH_CONF_THRESHOLD}):            {wiki_simple_hard:.4f} ({wiki_simple_hard*100:.2f}%)\")\n",
    "\n",
    "    estimates = {\n",
    "        'naive_proportion': float(naive_proportion),\n",
    "        'predicted_proportion': float(p_pred),\n",
    "        # Adjusted true proportion of simple sentences\n",
    "        'acc_corrected_proportion': float(p_true),\n",
    "        # Wikipedia internal: proportion of Vikidia-like simple sentences inside Wikipedia\n",
    "        'wikipedia_analysis': {\n",
    "            'n_sentences': int(n_wiki),\n",
    "            'soft_estimate': float(wiki_simple_soft),\n",
    "            'hard_estimate': float(wiki_simple_hard),\n",
    "            'hard_threshold': float(HIGH_CONF_THRESHOLD)\n",
    "        },\n",
    "        # Descriptions for JSON consumers\n",
    "        'descriptions': {\n",
    "            'acc_corrected_proportion': (\n",
    "                'Adjusted true proportion of simple sentences in the full dataset '\n",
    "                'using ACC calibration.'\n",
    "            ),\n",
    "            'wikipedia_soft_estimate': (\n",
    "                'Proportion of Vikidia-like simple sentences among complex-labelled '\n",
    "                'Wikipedia sentences (soft estimate: mean P(Simple) over Wikipedia sentences).'\n",
    "            ),\n",
    "            'wikipedia_hard_estimate': (\n",
    "                f'Proportion of Vikidia-like simple sentences among complex-labelled '\n",
    "                f'Wikipedia sentences (hard estimate: fraction with P(Simple) >= {HIGH_CONF_THRESHOLD}).'\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b70bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_full_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    probabilities: np.ndarray,\n",
    "    lang: str,\n",
    "    threshold: float = 0.5,\n",
    "    high_conf_mislabel: float = MISLABEL_HIGH_CONF_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Additional analysis on full dataset:\n",
    "    - Confusion matrix (overall and by source)\n",
    "    - Vikidia vs Wikipedia naive + predicted simple proportions\n",
    "    - Length-bin prevalence profile\n",
    "    - High-confidence disagreement candidates (potential label noise)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL DATASET PREDICTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    y_true = df['Label'].values\n",
    "    # Predict label: Simple(0) if P(Simple) >= threshold, else Complex(1)\n",
    "    y_pred = np.where(probabilities >= threshold, 0, 1)\n",
    "\n",
    "    # Overall confusion matrix (Simple=0 as positive)\n",
    "    cm_full = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    tn, fp, fn, tp = cm_full.ravel()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_simple = f1_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "    print(\"\\nOverall confusion matrix (Simple=0 as positive):\")\n",
    "    print(f\"  TN={tn:,} (Complex->Complex), FP={fp:,} (Complex->Simple)\")\n",
    "    print(f\"  FN={fn:,} (Simple->Complex), TP={tp:,} (Simple->Simple)\")\n",
    "    print(f\"  Accuracy: {acc:.4f}, F1(Simple=0): {f1_simple:.4f}\")\n",
    "\n",
    "    confusion_by_source: Dict[str, Dict[str, int]] = {}\n",
    "    prevalence_by_source: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    for src in ['viki', 'wiki']:\n",
    "        mask = (df['source'] == src).values\n",
    "        n_src = int(mask.sum())\n",
    "        if n_src == 0:\n",
    "            continue\n",
    "\n",
    "        y_true_src = y_true[mask]\n",
    "        y_pred_src = y_pred[mask]\n",
    "        probs_src = probabilities[mask]\n",
    "\n",
    "        cm_src = confusion_matrix(y_true_src, y_pred_src, labels=[1, 0])\n",
    "        tn_s, fp_s, fn_s, tp_s = cm_src.ravel()\n",
    "\n",
    "        confusion_by_source[src] = {\n",
    "            'tn': int(tn_s),\n",
    "            'fp': int(fp_s),\n",
    "            'fn': int(fn_s),\n",
    "            'tp': int(tp_s),\n",
    "            'n_sentences': n_src\n",
    "        }\n",
    "\n",
    "        naive_simple = float((y_true_src == 0).mean())\n",
    "        soft_pred = float(probs_src.mean())\n",
    "        hard_pred = float((probs_src >= threshold).mean())\n",
    "\n",
    "        prevalence_by_source[src] = {\n",
    "            'n_sentences': n_src,\n",
    "            'naive_simple_proportion_label0': naive_simple,\n",
    "            'predicted_simple_soft_mean_prob': soft_pred,\n",
    "            'predicted_simple_hard_prop_p>=threshold': hard_pred\n",
    "        }\n",
    "\n",
    "        print(f\"\\nSource='{src}' ({n_src:,} sentences):\")\n",
    "        print(f\"  Naive simple proportion (Label=0): {naive_simple:.4f}\")\n",
    "        print(f\"  Predicted simple (soft, mean P(Simple)): {soft_pred:.4f}\")\n",
    "        print(f\"  Predicted simple (hard, P(Simple) >= {threshold}): {hard_pred:.4f}\")\n",
    "\n",
    "    # Length-bin prevalence profile\n",
    "    length = df['LengthWords']\n",
    "    bins = [0, 10, 20, 30, 40, np.inf]\n",
    "    bin_labels = ['<=10', '11-20', '21-30', '31-40', '>=41']\n",
    "    length_bins = pd.cut(length, bins=bins, labels=bin_labels, include_lowest=True, right=True)\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        'length_bin': length_bins,\n",
    "        'p_simple': probabilities,\n",
    "        'hard_simple': (probabilities >= threshold).astype(int)\n",
    "    })\n",
    "\n",
    "    length_profile_df = tmp.groupby('length_bin', observed=True).agg(\n",
    "        n_sentences=('p_simple', 'size'),\n",
    "        mean_p_simple=('p_simple', 'mean'),\n",
    "        hard_simple_prop=('hard_simple', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    length_profile = length_profile_df.to_dict(orient='records')\n",
    "\n",
    "    print(\"\\nLength-bin prevalence profile (using LengthWords):\")\n",
    "    for row in length_profile:\n",
    "        print(\n",
    "            f\"  Bin={row['length_bin']}: n={row['n_sentences']}, \"\n",
    "            f\"mean P(Simple)={row['mean_p_simple']:.4f}, \"\n",
    "            f\"hard simple prop (P>= {threshold})={row['hard_simple_prop']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # High-confidence disagreement (potential label noise)\n",
    "    hi = high_conf_mislabel\n",
    "    lo = 1.0 - hi\n",
    "\n",
    "    mask_complex_high_simple = (df['Label'] == 1) & (probabilities >= hi)\n",
    "    mask_simple_low_simple = (df['Label'] == 0) & (probabilities <= lo)\n",
    "    mask_candidates = mask_complex_high_simple | mask_simple_low_simple\n",
    "\n",
    "    candidates = df.loc[mask_candidates, ['ID', 'Name', 'Sentence', 'Label', 'source', 'LengthWords']].copy()\n",
    "    candidates['p_simple'] = probabilities[mask_candidates]\n",
    "    candidates['pred_label_0.5'] = np.where(candidates['p_simple'] >= threshold, 0, 1)\n",
    "\n",
    "    candidate_type = np.where(\n",
    "        mask_complex_high_simple[mask_candidates],\n",
    "        'complex_label_high_simple_prob',\n",
    "        'simple_label_low_simple_prob'\n",
    "    )\n",
    "    candidates['candidate_type'] = candidate_type\n",
    "\n",
    "    mislabel_file = RESULTS_DIR / f\"mislabel_candidates_{lang}.csv\"\n",
    "    candidates.to_csv(mislabel_file, index=False)\n",
    "\n",
    "    n_complex_high_simple = int(mask_complex_high_simple.sum())\n",
    "    n_simple_low_simple = int(mask_simple_low_simple.sum())\n",
    "    total_candidates = int(mask_candidates.sum())\n",
    "\n",
    "    print(\"\\nHigh-confidence disagreement candidates (potential label noise):\")\n",
    "    print(f\"  complex-label, P(Simple) >= {hi}: {n_complex_high_simple:,}\")\n",
    "    print(f\"  simple-label,  P(Simple) <= {lo}: {n_simple_low_simple:,}\")\n",
    "    print(f\"  total candidates saved to: {mislabel_file}\")\n",
    "\n",
    "    mislabel_stats = {\n",
    "        'high_conf_threshold': hi,\n",
    "        'low_conf_threshold': lo,\n",
    "        'complex_label_high_simple_prob_count': n_complex_high_simple,\n",
    "        'simple_label_low_simple_prob_count': n_simple_low_simple,\n",
    "        'total_candidates': total_candidates,\n",
    "        'output_file': str(mislabel_file)\n",
    "    }\n",
    "\n",
    "    analysis = {\n",
    "        'threshold_for_hard_simple': threshold,\n",
    "        'confusion_matrix_full': {\n",
    "            'tn': int(tn),\n",
    "            'fp': int(fp),\n",
    "            'fn': int(fn),\n",
    "            'tp': int(tp),\n",
    "            'n_sentences': int(len(y_true))\n",
    "        },\n",
    "        'metrics_full': {\n",
    "            'accuracy': float(acc),\n",
    "            'f1_simple_label0': float(f1_simple)\n",
    "        },\n",
    "        'confusion_matrix_by_source': confusion_by_source,\n",
    "        'by_source_prevalence': prevalence_by_source,\n",
    "        'length_bin_profile': length_profile,\n",
    "        'mislabel_stats': mislabel_stats\n",
    "    }\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e68f8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def plot_results(\n",
    "    df: pd.DataFrame,\n",
    "    probabilities: np.ndarray,\n",
    "    estimates: Dict,\n",
    "    lang: str\n",
    ") -> None:\n",
    "    \"\"\"Generate visualization plots.\"\"\"\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\n",
    "        f\"Prevalence Estimation Results - {lang.upper()}\",\n",
    "        fontsize=14, fontweight='bold'\n",
    "    )\n",
    "\n",
    "    # 1. Probability distribution by source\n",
    "    ax1 = axes[0, 0]\n",
    "    wiki_probs = probabilities[df['source'] == 'wiki']\n",
    "    viki_probs = probabilities[df['source'] == 'viki']\n",
    "\n",
    "    ax1.hist(wiki_probs, bins=50, alpha=0.6, label='Wikipedia (Complex)', color='blue', density=True)\n",
    "    ax1.hist(viki_probs, bins=50, alpha=0.6, label='Vikidia (Simple)', color='green', density=True)\n",
    "    ax1.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "    ax1.set_xlabel('P(Simple)')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Probability Distribution by Source')\n",
    "    ax1.legend()\n",
    "\n",
    "    # 2. Proportion comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    proportions = [\n",
    "        estimates['naive_proportion'],\n",
    "        estimates['predicted_proportion'],\n",
    "        estimates['acc_corrected_proportion']\n",
    "    ]\n",
    "    labels = ['Naive\\n(Label=0/Total)', 'Predicted\\n(P>=0.5)', 'ACC-Corrected\\n(True Est.)']\n",
    "    colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "\n",
    "    bars = ax2.bar(labels, proportions, color=colors)\n",
    "    ax2.set_ylabel('Proportion')\n",
    "    ax2.set_title('Simple Sentence Proportion Estimates')\n",
    "    ax2.set_ylim(0, max(proportions) * 1.2)\n",
    "\n",
    "    for bar, prop in zip(bars, proportions):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{prop:.2%}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # 3. Wikipedia internal analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    wiki_est = estimates['wikipedia_analysis']\n",
    "    wiki_labels = ['Soft Estimate\\n(Mean P)', f'Hard Estimate\\n(P>={HIGH_CONF_THRESHOLD})']\n",
    "    wiki_values = [wiki_est['soft_estimate'], wiki_est['hard_estimate']]\n",
    "\n",
    "    bars = ax3.bar(wiki_labels, wiki_values, color=['#9467bd', '#d62728'])\n",
    "    ax3.set_ylabel('Proportion')\n",
    "    ax3.set_title('Wikipedia Internal Simple-Style Proportion')\n",
    "    ax3.set_ylim(0, max(wiki_values) * 1.4 if max(wiki_values) > 0 else 0.1)\n",
    "\n",
    "    for bar, val in zip(bars, wiki_values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{val:.2%}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # 4. Probability boxplot by label\n",
    "    ax4 = axes[1, 1]\n",
    "    df_plot = pd.DataFrame({\n",
    "        'P(Simple)': probabilities,\n",
    "        'Original Label': df['Label'].map({0: 'Simple/Vikidia (0)', 1: 'Complex/Wikipedia (1)'})\n",
    "    })\n",
    "    sns.boxplot(data=df_plot, x='Original Label', y='P(Simple)', ax=ax4)\n",
    "    ax4.axhline(y=0.5, color='red', linestyle='--', alpha=0.7)\n",
    "    ax4.set_title('Predicted Probability by Original Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plot_file = RESULTS_DIR / f\"prevalence_estimation_{lang}.png\"\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"  Plot saved: {plot_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb3b7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Pipeline\n",
    "def process_language(lang: str) -> Dict:\n",
    "    \"\"\"Complete pipeline for one language.\"\"\"\n",
    "    lang_name = \"English\" if lang == \"en\" else \"French\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"PREVALENCE ESTIMATION - {lang_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load features (contains all data including metadata and labels)\n",
    "    df = load_features(lang)\n",
    "\n",
    "    print(f\"\\nDataset: {len(df):,} sentences\")\n",
    "    print(f\"  Simple/Vikidia (Label=0):   {(df['Label']==0).sum():,}\")\n",
    "    print(f\"  Complex/Wikipedia (Label=1): {(df['Label']==1).sum():,}\")\n",
    "\n",
    "    # Part 1: Naive estimate\n",
    "    naive_prop = compute_naive_proportion(df)\n",
    "    print(f\"\\nNaive estimate (Simple proportion, Label=0/Total): {naive_prop:.4f} ({naive_prop*100:.2f}%)\")\n",
    "\n",
    "    # Part 2: Feature preparation\n",
    "    features_df, _ = prepare_features(df.copy())\n",
    "    features_df = handle_outliers(features_df.copy())\n",
    "    features_df, feature_names = preprocess_features(features_df)\n",
    "    features_df = remove_high_correlation(features_df)\n",
    "\n",
    "    # Part 2: Anchor selection\n",
    "    X_anchor, y_anchor, anchor_idx, anchor_info = select_anchors(df, features_df)\n",
    "\n",
    "    # Anchor quality summary\n",
    "    anchor_quality = analyze_anchor_quality(df, anchor_idx, y_anchor)\n",
    "\n",
    "    # Label-shuffle sanity check on anchors\n",
    "    shuffle_check = label_shuffle_sanity_check(X_anchor, y_anchor)\n",
    "\n",
    "    # Part 2: Model training\n",
    "    best_model, best_name, training_results = train_models(X_anchor, y_anchor)\n",
    "\n",
    "    # Part 2: Full prediction\n",
    "    probabilities = predict_full_dataset(best_model, features_df)\n",
    "\n",
    "    # Part 2: ACC calibration\n",
    "    tpr, fpr, calibration_info = compute_acc_calibration(best_model, X_anchor, y_anchor)\n",
    "\n",
    "    # Part 3: Final estimates\n",
    "    estimates = compute_final_estimates(df, probabilities, tpr, fpr, naive_prop)\n",
    "\n",
    "    # Additional full-dataset analysis (noise & stratified profiles)\n",
    "    prediction_analysis = analyze_full_predictions(df, probabilities, lang)\n",
    "\n",
    "    # Visualization\n",
    "    plot_results(df, probabilities, estimates, lang)\n",
    "\n",
    "    # Compile all results\n",
    "    results = {\n",
    "        'language': lang_name,\n",
    "        'dataset_info': {\n",
    "            'total_sentences': len(df),\n",
    "            'simple_vikidia_count': int((df['Label'] == 0).sum()),\n",
    "            'complex_wikipedia_count': int((df['Label'] == 1).sum())\n",
    "        },\n",
    "        'anchor_selection': anchor_info,\n",
    "        'anchor_quality': anchor_quality,\n",
    "        'anchor_shuffle_sanity_check': shuffle_check,\n",
    "        'training': {\n",
    "            'best_model': best_name,\n",
    "            'model_results': training_results\n",
    "        },\n",
    "        'calibration': calibration_info,\n",
    "        'estimates': estimates,\n",
    "        'prediction_analysis': prediction_analysis\n",
    "    }\n",
    "\n",
    "    # Save results to JSON\n",
    "    results_file = RESULTS_DIR / f\"prevalence_estimation_{lang}.json\"\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nResults saved: {results_file}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del df, features_df, probabilities\n",
    "    gc.collect()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a44e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_final_summary(en_results: Dict, fr_results: Dict) -> None:\n",
    "    \"\"\"Print final comparison summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL SUMMARY: ENGLISH vs FRENCH\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"\\n{'Metric':<45} {'English':<15} {'French':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    # Dataset info\n",
    "    print(f\"{'Total sentences':<45} \"\n",
    "          f\"{en_results['dataset_info']['total_sentences']:<15,} \"\n",
    "          f\"{fr_results['dataset_info']['total_sentences']:<15,}\")\n",
    "\n",
    "    # Naive estimate\n",
    "    print(f\"{'Naive estimate (Label=0/Total)':<45} \"\n",
    "          f\"{en_results['estimates']['naive_proportion']:<15.4f} \"\n",
    "          f\"{fr_results['estimates']['naive_proportion']:<15.4f}\")\n",
    "\n",
    "    # ACC-corrected estimate\n",
    "    print(f\"{'ACC-corrected true proportion':<45} \"\n",
    "          f\"{en_results['estimates']['acc_corrected_proportion']:<15.4f} \"\n",
    "          f\"{fr_results['estimates']['acc_corrected_proportion']:<15.4f}\")\n",
    "\n",
    "    # Wikipedia internal\n",
    "    print(f\"{'Wikipedia internal (soft estimate)':<45} \"\n",
    "          f\"{en_results['estimates']['wikipedia_analysis']['soft_estimate']:<15.4f} \"\n",
    "          f\"{fr_results['estimates']['wikipedia_analysis']['soft_estimate']:<15.4f}\")\n",
    "\n",
    "    print(f\"{'Wikipedia internal (hard estimate, P>=' + str(HIGH_CONF_THRESHOLD) + ')':<45} \"\n",
    "          f\"{en_results['estimates']['wikipedia_analysis']['hard_estimate']:<15.4f} \"\n",
    "          f\"{fr_results['estimates']['wikipedia_analysis']['hard_estimate']:<15.4f}\")\n",
    "\n",
    "    # Calibration info\n",
    "    print(f\"\\n{'Calibration (TPR)':<45} \"\n",
    "          f\"{en_results['calibration']['tpr']:<15.4f} \"\n",
    "          f\"{fr_results['calibration']['tpr']:<15.4f}\")\n",
    "    print(f\"{'Calibration (FPR)':<45} \"\n",
    "          f\"{en_results['calibration']['fpr']:<15.4f} \"\n",
    "          f\"{fr_results['calibration']['fpr']:<15.4f}\")\n",
    "\n",
    "    # Best model\n",
    "    print(f\"\\n{'Best model':<45} \"\n",
    "          f\"{en_results['training']['best_model']:<15} \"\n",
    "          f\"{fr_results['training']['best_model']:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05490741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"iDEM TASK 1: PREVALENCE ESTIMATION\")\n",
    "    print(\"Anchor Training + ACC Calibration Pipeline\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Features directory: {FEATURES_DIR}\")\n",
    "    print(f\"Results directory:  {RESULTS_DIR}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed(RANDOM_SEED)\n",
    "\n",
    "    # Check local files\n",
    "    ensure_local_files_exist()\n",
    "\n",
    "    # Process both languages\n",
    "    en_results = process_language('en')\n",
    "    gc.collect()\n",
    "\n",
    "    fr_results = process_language('fr')\n",
    "    gc.collect()\n",
    "\n",
    "    # Final summary\n",
    "    print_final_summary(en_results, fr_results)\n",
    "\n",
    "    # Save combined results\n",
    "    combined = pd.DataFrame([\n",
    "        {\n",
    "            'language': 'English',\n",
    "            'naive_proportion': en_results['estimates']['naive_proportion'],\n",
    "            'acc_corrected_proportion': en_results['estimates']['acc_corrected_proportion'],\n",
    "            'wiki_soft_estimate': en_results['estimates']['wikipedia_analysis']['soft_estimate'],\n",
    "            'wiki_hard_estimate': en_results['estimates']['wikipedia_analysis']['hard_estimate'],\n",
    "            'tpr': en_results['calibration']['tpr'],\n",
    "            'fpr': en_results['calibration']['fpr'],\n",
    "            'best_model': en_results['training']['best_model']\n",
    "        },\n",
    "        {\n",
    "            'language': 'French',\n",
    "            'naive_proportion': fr_results['estimates']['naive_proportion'],\n",
    "            'acc_corrected_proportion': fr_results['estimates']['acc_corrected_proportion'],\n",
    "            'wiki_soft_estimate': fr_results['estimates']['wikipedia_analysis']['soft_estimate'],\n",
    "            'wiki_hard_estimate': fr_results['estimates']['wikipedia_analysis']['hard_estimate'],\n",
    "            'tpr': fr_results['calibration']['tpr'],\n",
    "            'fpr': fr_results['calibration']['fpr'],\n",
    "            'best_model': fr_results['training']['best_model']\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    results_file = RESULTS_DIR / \"prevalence_estimation_summary.csv\"\n",
    "    combined.to_csv(results_file, index=False)\n",
    "    print(f\"\\nSummary saved: {results_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TASK 1 COMPLETE!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d66851a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "iDEM TASK 1: PREVALENCE ESTIMATION\n",
      "Anchor Training + ACC Calibration Pipeline\n",
      "======================================================================\n",
      "Features directory: c:\\Users\\chang\\iDEMRA\\features\n",
      "Results directory:  c:\\Users\\chang\\iDEMRA\\results\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "CHECKING LOCAL FILES\n",
      "============================================================\n",
      "  [OK] c:\\Users\\chang\\iDEMRA\\features\\en_full_features.csv\n",
      "  [OK] c:\\Users\\chang\\iDEMRA\\features\\fr_full_features.csv\n",
      "\n",
      "All required files found.\n",
      "\n",
      "======================================================================\n",
      "PREVALENCE ESTIMATION - ENGLISH\n",
      "======================================================================\n",
      "\n",
      "Loading features: c:\\Users\\chang\\iDEMRA\\features\\en_full_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading en_full_features.csv: 100%|█| 289781/289781 [00:03<00:00, 78827.47 rows/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: 289,781 sentences\n",
      "  Simple/Vikidia (Label=0):   17,305\n",
      "  Complex/Wikipedia (Label=1): 272,476\n",
      "\n",
      "Naive estimate (Simple proportion, Label=0/Total): 0.0597 (5.97%)\n",
      "\n",
      "Preparing features...\n",
      "  Dropped columns: ['Index', 'ID', 'Name', 'Sentence', 'Label', 'LengthChars', 'source']\n",
      "  Remaining columns: ['LengthWords', 'words_chars_ratio', 'cos_simi', 'avg_word_len', 'long_word_ratio', 'ttr', 'punct_density', 'comma_density', 'digit_ratio', 'upper_ratio', 'has_parens', 'n_tokens', 'max_depth', 'avg_depth', 'avg_dependency_distance', 'func_word_ratio', 'n_clauses', 'clause_ratio', 'noun_ratio', 'verb_ratio']\n",
      "\n",
      "Handling outliers (IQR clipping)...\n",
      "  Clipped outliers in 19 columns\n",
      "\n",
      "Preprocessing features...\n",
      "  Removed zero-variance: ['has_parens']\n",
      "  Features: 20 -> 19\n",
      "\n",
      "Removing high correlation (threshold=0.95)...\n",
      "  Removing 1 columns: {'words_chars_ratio'}\n",
      "\n",
      "============================================================\n",
      "ANCHOR SELECTION\n",
      "============================================================\n",
      "\n",
      "LengthWords quartiles: Q1=16.0, Q3=29.0\n",
      "\n",
      "Anchor selection:\n",
      "  Simple anchors (Vikidia/Label=0, LengthWords <= 16.0): 8,773\n",
      "  Complex anchors (Wikipedia/Label=1, LengthWords >= 29.0): 73,032\n",
      "  Total anchor samples: 81,805\n",
      "  Class distribution: Simple(0)=8,773, Complex(1)=73,032\n",
      "\n",
      "============================================================\n",
      "ANCHOR QUALITY SUMMARY\n",
      "============================================================\n",
      "  Anchor ratio: 81,805 / 289,781 = 0.2823\n",
      "  LengthWords mean (global): 23.75, std: 13.03\n",
      "  LengthWords mean (anchors): 36.22, std: 17.51\n",
      "  Anchor source breakdown: {'wiki': 73032, 'viki': 8773}\n",
      "  Anchor label breakdown: Simple(0)=8,773, Complex(1)=73,032\n",
      "\n",
      "Running label-shuffle sanity check on anchors...\n",
      "  Used samples: 50,000 / 81,805\n",
      "  Label-shuffle RF F1 (Simple=0 as positive): mean=0.9403, std=0.0011\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING (on anchor samples)\n",
      "============================================================\n",
      "\n",
      "Anchor split: Train=65,444, Val=16,361\n",
      "\n",
      "  Tuning Logistic Regression...\n",
      "    Best params: {'classifier__solver': 'saga', 'classifier__penalty': 'l2', 'classifier__C': 10.0}\n",
      "    CV F1: 1.0000 | Val F1: 1.0000\n",
      "\n",
      "  Tuning Random Forest...\n",
      "    Best params: {'classifier__n_estimators': 100, 'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 5}\n",
      "    CV F1: 1.0000 | Val F1: 1.0000\n",
      "\n",
      "Best model: random_forest (Val F1: 1.0000)\n",
      "\n",
      "============================================================\n",
      "FULL DATASET PREDICTION\n",
      "============================================================\n",
      "\n",
      "Predicting probabilities for 289,781 sentences...\n",
      "  Mean P(Simple): 0.5844\n",
      "  Std P(Simple): 0.4202\n",
      "\n",
      "============================================================\n",
      "ACC CALIBRATION (Cross-Validation)\n",
      "============================================================\n",
      "\n",
      "Confusion Matrix (Simple=0 as positive, 5-fold CV):\n",
      "  TN=73,030 (Complex->Complex), FP=2 (Complex->Simple)\n",
      "  FN=4 (Simple->Complex), TP=8,769 (Simple->Simple)\n",
      "\n",
      "Rates for Simple class:\n",
      "  TPR (True Positive Rate): 0.9995\n",
      "  FPR (False Positive Rate): 0.0000\n",
      "\n",
      "============================================================\n",
      "FINAL ESTIMATES\n",
      "============================================================\n",
      "\n",
      "1. GLOBAL SIMPLE SENTENCE PROPORTION:\n",
      "   Naive estimate (Label=0 / Total):     0.0597 (5.97%)\n",
      "   Predicted proportion (P >= 0.5):      0.6334 (63.34%)\n",
      "   ACC-corrected true proportion:        0.6336 (63.36%)\n",
      "\n",
      "2. WIKIPEDIA INTERNAL SIMPLE-STYLE PROPORTION:\n",
      "   Wikipedia sentences:                  272,476\n",
      "   Soft estimate (mean P(Simple)):       0.5723 (57.23%)\n",
      "   Hard estimate (P >= 0.8):            0.4894 (48.94%)\n",
      "\n",
      "============================================================\n",
      "FULL DATASET PREDICTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Overall confusion matrix (Simple=0 as positive):\n",
      "  TN=103,195 (Complex->Complex), FP=169,281 (Complex->Simple)\n",
      "  FN=3,050 (Simple->Complex), TP=14,255 (Simple->Simple)\n",
      "  Accuracy: 0.4053, F1(Simple=0): 0.1420\n",
      "\n",
      "Source='viki' (17,305 sentences):\n",
      "  Naive simple proportion (Label=0): 1.0000\n",
      "  Predicted simple (soft, mean P(Simple)): 0.7744\n",
      "  Predicted simple (hard, P(Simple) >= 0.5): 0.8238\n",
      "\n",
      "Source='wiki' (272,476 sentences):\n",
      "  Naive simple proportion (Label=0): 0.0000\n",
      "  Predicted simple (soft, mean P(Simple)): 0.5723\n",
      "  Predicted simple (hard, P(Simple) >= 0.5): 0.6213\n",
      "\n",
      "Length-bin prevalence profile (using LengthWords):\n",
      "  Bin=<=10: n=10327, mean P(Simple)=0.9908, hard simple prop (P>= 0.5)=0.9999\n",
      "  Bin=11-20: n=126442, mean P(Simple)=0.9323, hard simple prop (P>= 0.5)=0.9957\n",
      "  Bin=21-30: n=91470, mean P(Simple)=0.4398, hard simple prop (P>= 0.5)=0.5173\n",
      "  Bin=31-40: n=39362, mean P(Simple)=0.0204, hard simple prop (P>= 0.5)=0.0000\n",
      "  Bin=>=41: n=22180, mean P(Simple)=0.0091, hard simple prop (P>= 0.5)=0.0000\n",
      "\n",
      "High-confidence disagreement candidates (potential label noise):\n",
      "  complex-label, P(Simple) >= 0.9: 100,587\n",
      "  simple-label,  P(Simple) <= 0.09999999999999998: 2,530\n",
      "  total candidates saved to: c:\\Users\\chang\\iDEMRA\\results\\mislabel_candidates_en.csv\n",
      "\n",
      "Generating visualizations...\n",
      "  Plot saved: c:\\Users\\chang\\iDEMRA\\results\\prevalence_estimation_en.png\n",
      "\n",
      "Results saved: c:\\Users\\chang\\iDEMRA\\results\\prevalence_estimation_en.json\n",
      "\n",
      "======================================================================\n",
      "PREVALENCE ESTIMATION - FRENCH\n",
      "======================================================================\n",
      "\n",
      "Loading features: c:\\Users\\chang\\iDEMRA\\features\\fr_full_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading fr_full_features.csv: 100%|█| 1653175/1653175 [00:16<00:00, 102775.21 ro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: 1,653,175 sentences\n",
      "  Simple/Vikidia (Label=0):   188,856\n",
      "  Complex/Wikipedia (Label=1): 1,464,319\n",
      "\n",
      "Naive estimate (Simple proportion, Label=0/Total): 0.1142 (11.42%)\n",
      "\n",
      "Preparing features...\n",
      "  Dropped columns: ['Index', 'ID', 'Name', 'Sentence', 'Label', 'LengthChars', 'source']\n",
      "  Remaining columns: ['LengthWords', 'words_chars_ratio', 'cos_simi', 'avg_word_len', 'long_word_ratio', 'ttr', 'punct_density', 'comma_density', 'digit_ratio', 'upper_ratio', 'has_parens', 'n_tokens', 'max_depth', 'avg_depth', 'avg_dependency_distance', 'func_word_ratio', 'n_clauses', 'clause_ratio', 'noun_ratio', 'verb_ratio']\n",
      "\n",
      "Handling outliers (IQR clipping)...\n",
      "  Clipped outliers in 19 columns\n",
      "\n",
      "Preprocessing features...\n",
      "  Removed zero-variance: ['has_parens']\n",
      "  Features: 20 -> 19\n",
      "\n",
      "Removing high correlation (threshold=0.95)...\n",
      "  Removing 1 columns: {'words_chars_ratio'}\n",
      "\n",
      "============================================================\n",
      "ANCHOR SELECTION\n",
      "============================================================\n",
      "\n",
      "LengthWords quartiles: Q1=15.0, Q3=30.0\n",
      "\n",
      "Anchor selection:\n",
      "  Simple anchors (Vikidia/Label=0, LengthWords <= 15.0): 67,064\n",
      "  Complex anchors (Wikipedia/Label=1, LengthWords >= 30.0): 387,517\n",
      "  Total anchor samples: 454,581\n",
      "  Class distribution: Simple(0)=67,064, Complex(1)=387,517\n",
      "\n",
      "============================================================\n",
      "ANCHOR QUALITY SUMMARY\n",
      "============================================================\n",
      "  Anchor ratio: 454,581 / 1,653,175 = 0.2750\n",
      "  LengthWords mean (global): 24.25, std: 14.10\n",
      "  LengthWords mean (anchors): 37.55, std: 19.04\n",
      "  Anchor source breakdown: {'wiki': 387517, 'viki': 67064}\n",
      "  Anchor label breakdown: Simple(0)=67,064, Complex(1)=387,517\n",
      "\n",
      "Running label-shuffle sanity check on anchors...\n",
      "  Used samples: 50,000 / 454,581\n",
      "  Label-shuffle RF F1 (Simple=0 as positive): mean=0.9110, std=0.0014\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING (on anchor samples)\n",
      "============================================================\n",
      "\n",
      "Anchor split: Train=363,664, Val=90,917\n",
      "\n",
      "  Tuning Logistic Regression...\n",
      "    Best params: {'classifier__solver': 'saga', 'classifier__penalty': 'l2', 'classifier__C': 1.0}\n",
      "    CV F1: 1.0000 | Val F1: 0.9999\n",
      "\n",
      "  Tuning Random Forest...\n",
      "    Best params: {'classifier__n_estimators': 100, 'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 5}\n",
      "    CV F1: 1.0000 | Val F1: 1.0000\n",
      "\n",
      "Best model: random_forest (Val F1: 1.0000)\n",
      "\n",
      "============================================================\n",
      "FULL DATASET PREDICTION\n",
      "============================================================\n",
      "\n",
      "Predicting probabilities for 1,653,175 sentences...\n",
      "  Mean P(Simple): 0.5795\n",
      "  Std P(Simple): 0.4217\n",
      "\n",
      "============================================================\n",
      "ACC CALIBRATION (Cross-Validation)\n",
      "============================================================\n",
      "\n",
      "Confusion Matrix (Simple=0 as positive, 5-fold CV):\n",
      "  TN=387,517 (Complex->Complex), FP=0 (Complex->Simple)\n",
      "  FN=6 (Simple->Complex), TP=67,058 (Simple->Simple)\n",
      "\n",
      "Rates for Simple class:\n",
      "  TPR (True Positive Rate): 0.9999\n",
      "  FPR (False Positive Rate): 0.0000\n",
      "\n",
      "============================================================\n",
      "FINAL ESTIMATES\n",
      "============================================================\n",
      "\n",
      "1. GLOBAL SIMPLE SENTENCE PROPORTION:\n",
      "   Naive estimate (Label=0 / Total):     0.1142 (11.42%)\n",
      "   Predicted proportion (P >= 0.5):      0.6264 (62.64%)\n",
      "   ACC-corrected true proportion:        0.6264 (62.64%)\n",
      "\n",
      "2. WIKIPEDIA INTERNAL SIMPLE-STYLE PROPORTION:\n",
      "   Wikipedia sentences:                  1,464,319\n",
      "   Soft estimate (mean P(Simple)):       0.5668 (56.68%)\n",
      "   Hard estimate (P >= 0.8):            0.4923 (49.23%)\n",
      "\n",
      "============================================================\n",
      "FULL DATASET PREDICTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Overall confusion matrix (Simple=0 as positive):\n",
      "  TN=567,225 (Complex->Complex), FP=897,094 (Complex->Simple)\n",
      "  FN=50,412 (Simple->Complex), TP=138,444 (Simple->Simple)\n",
      "  Accuracy: 0.4269, F1(Simple=0): 0.2261\n",
      "\n",
      "Source='viki' (188,856 sentences):\n",
      "  Naive simple proportion (Label=0): 1.0000\n",
      "  Predicted simple (soft, mean P(Simple)): 0.6787\n",
      "  Predicted simple (hard, P(Simple) >= 0.5): 0.7331\n",
      "\n",
      "Source='wiki' (1,464,319 sentences):\n",
      "  Naive simple proportion (Label=0): 0.0000\n",
      "  Predicted simple (soft, mean P(Simple)): 0.5668\n",
      "  Predicted simple (hard, P(Simple) >= 0.5): 0.6126\n",
      "\n",
      "Length-bin prevalence profile (using LengthWords):\n",
      "  Bin=<=10: n=71015, mean P(Simple)=0.9825, hard simple prop (P>= 0.5)=0.9999\n",
      "  Bin=11-20: n=725282, mean P(Simple)=0.9239, hard simple prop (P>= 0.5)=0.9959\n",
      "  Bin=21-30: n=472806, mean P(Simple)=0.4404, hard simple prop (P>= 0.5)=0.5123\n",
      "  Bin=31-40: n=222086, mean P(Simple)=0.0329, hard simple prop (P>= 0.5)=0.0000\n",
      "  Bin=>=41: n=161986, mean P(Simple)=0.0167, hard simple prop (P>= 0.5)=0.0000\n",
      "\n",
      "High-confidence disagreement candidates (potential label noise):\n",
      "  complex-label, P(Simple) >= 0.9: 542,865\n",
      "  simple-label,  P(Simple) <= 0.09999999999999998: 42,700\n",
      "  total candidates saved to: c:\\Users\\chang\\iDEMRA\\results\\mislabel_candidates_fr.csv\n",
      "\n",
      "Generating visualizations...\n",
      "  Plot saved: c:\\Users\\chang\\iDEMRA\\results\\prevalence_estimation_fr.png\n",
      "\n",
      "Results saved: c:\\Users\\chang\\iDEMRA\\results\\prevalence_estimation_fr.json\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY: ENGLISH vs FRENCH\n",
      "======================================================================\n",
      "\n",
      "Metric                                        English         French         \n",
      "---------------------------------------------------------------------------\n",
      "Total sentences                               289,781         1,653,175      \n",
      "Naive estimate (Label=0/Total)                0.0597          0.1142         \n",
      "ACC-corrected true proportion                 0.6336          0.6264         \n",
      "Wikipedia internal (soft estimate)            0.5723          0.5668         \n",
      "Wikipedia internal (hard estimate, P>=0.8)    0.4894          0.4923         \n",
      "\n",
      "Calibration (TPR)                             0.9995          0.9999         \n",
      "Calibration (FPR)                             0.0000          0.0000         \n",
      "\n",
      "Best model                                    random_forest   random_forest  \n",
      "\n",
      "Summary saved: c:\\Users\\chang\\iDEMRA\\results\\prevalence_estimation_summary.csv\n",
      "\n",
      "======================================================================\n",
      "TASK 1 COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
