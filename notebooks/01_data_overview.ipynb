{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c20c6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 0: Data Overview and Preprocessing for iDEM Research Task\\n\\nThis script provides:\\n    1. Data download and validation\\n    2. Duplicate detection and reporting\\n    3. Cleaned dataset generation (En-Dataset_cleaned.csv, Fr-Dataset_cleaned.csv)\\n    4. Comprehensive statistics and visualizations using CLEANED data\\n\\nOfficial Label Definition (from README):\\n    - Label = 0: Simple (sentence annotated as simple, from Vikidia)\\n    - Label = 1: Complex (sentence annotated as complex, from Wikipedia)\\n\\nSource (derived from ID prefix):\\n    - wiki-* : Wikipedia\\n    - viki-* : Vikidia\\n\\nCleaning Strategy (aligned with feature_extraction_kaggle_multi.py):\\n    1. Basic cleaning: Remove NaN and blank sentences\\n    2. V-V: Remove Vikidia internal duplicates (keep first occurrence)\\n    3. W-W: Remove Wikipedia internal duplicates (keep first occurrence)\\n    4. V-W: Remove cross-dataset duplicates from Vikidia (keep Wikipedia version)\\n         This prevents data leakage between train/test sets.\\n\\nDuplicate Types (for reporting):\\n    - V-V: Vikidia internal duplicates\\n    - V-W: Cross-dataset duplicates (Vikidia ∩ Wikipedia)\\n    - W-W: Wikipedia internal duplicates\\n\\nOutput Files:\\n    - data/En-Dataset_cleaned.csv\\n    - data/Fr-Dataset_cleaned.csv\\n    - output/en_duplicates.csv (duplicate report)\\n    - output/fr_duplicates.csv (duplicate report)\\n    - output/*.png (visualizations)\\n\\nUsage:\\n    python 01_data_overview.py\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 0: Data Overview and Preprocessing for iDEM Research Task\n",
    "\n",
    "This script provides:\n",
    "    1. Data download and validation\n",
    "    2. Duplicate detection and reporting\n",
    "    3. Cleaned dataset generation (En-Dataset_cleaned.csv, Fr-Dataset_cleaned.csv)\n",
    "    4. Comprehensive statistics and visualizations using CLEANED data\n",
    "\n",
    "Official Label Definition (from README):\n",
    "    - Label = 0: Simple (sentence annotated as simple, from Vikidia)\n",
    "    - Label = 1: Complex (sentence annotated as complex, from Wikipedia)\n",
    "\n",
    "Source (derived from ID prefix):\n",
    "    - wiki-* : Wikipedia\n",
    "    - viki-* : Vikidia\n",
    "\n",
    "Cleaning Strategy (aligned with feature_extraction_kaggle_multi.py):\n",
    "    1. Basic cleaning: Remove NaN and blank sentences\n",
    "    2. V-V: Remove Vikidia internal duplicates (keep first occurrence)\n",
    "    3. W-W: Remove Wikipedia internal duplicates (keep first occurrence)\n",
    "    4. V-W: Remove cross-dataset duplicates from Vikidia (keep Wikipedia version)\n",
    "         This prevents data leakage between train/test sets.\n",
    "\n",
    "Duplicate Types (for reporting):\n",
    "    - V-V: Vikidia internal duplicates\n",
    "    - V-W: Cross-dataset duplicates (Vikidia ∩ Wikipedia)\n",
    "    - W-W: Wikipedia internal duplicates\n",
    "\n",
    "Output Files:\n",
    "    - data/En-Dataset_cleaned.csv\n",
    "    - data/Fr-Dataset_cleaned.csv\n",
    "    - output/en_duplicates.csv (duplicate report)\n",
    "    - output/fr_duplicates.csv (duplicate report)\n",
    "    - output/*.png (visualizations)\n",
    "\n",
    "Usage:\n",
    "    python 01_data_overview.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c6c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import textwrap\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f8c24b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b175d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For Jupyter Notebook (use current working directory)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m BASE_DIR = Path(\u001b[43mos\u001b[49m.getcwd())\n\u001b[32m      3\u001b[39m DATA_DIR = BASE_DIR / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m OUTPUT_DIR = BASE_DIR / \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# For Jupyter Notebook (use current working directory)\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URLS = {\n",
    "    \"En-Dataset.csv\": \"https://github.com/Nouran-Khallaf/idem-candidate-task/releases/download/data/En-Dataset.csv\",\n",
    "    \"Fr-Dataset.csv\": \"https://github.com/Nouran-Khallaf/idem-candidate-task/releases/download/data/Fr-Dataset.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official label semantics\n",
    "LABEL_NAMES = {0: \"Simple\", 1: \"Complex\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Search and Download\n",
    "class DownloadProgressBar:\n",
    "    \"\"\"Progress bar for urllib downloads.\"\"\"\n",
    "    \n",
    "    def __init__(self, filename: str):\n",
    "        self.pbar = None\n",
    "        self.filename = filename\n",
    "    \n",
    "    def __call__(self, block_num: int, block_size: int, total_size: int):\n",
    "        if self.pbar is None:\n",
    "            self.pbar = tqdm(\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "                desc=f\"Downloading {self.filename}\",\n",
    "                ncols=80\n",
    "            )\n",
    "        downloaded = block_num * block_size\n",
    "        if downloaded < total_size:\n",
    "            self.pbar.update(block_size)\n",
    "        else:\n",
    "            self.pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6acd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_recursive(filename: str, search_dir: Path, max_depth: int = 3) -> Path | None:\n",
    "    \"\"\"\n",
    "    Recursively search for a file in the given directory and subdirectories.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the file to find\n",
    "        search_dir: Directory to start searching from\n",
    "        max_depth: Maximum depth to search (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the file if found, None otherwise\n",
    "    \"\"\"\n",
    "    target = search_dir / filename\n",
    "    if target.exists():\n",
    "        return target\n",
    "    \n",
    "    if max_depth > 0:\n",
    "        try:\n",
    "            for subdir in search_dir.iterdir():\n",
    "                if subdir.is_dir() and not subdir.name.startswith('.'):\n",
    "                    result = find_file_recursive(filename, subdir, max_depth - 1)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except PermissionError:\n",
    "            pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_exists() -> None:\n",
    "    \"\"\"\n",
    "    Check if dataset files exist locally. Search in current directory,\n",
    "    subdirectories, and data/ folder. If not found, download from GitHub.\n",
    "    \"\"\"\n",
    "    print(\"\\nChecking dataset files...\")\n",
    "\n",
    "    for filename, url in DATA_URLS.items():\n",
    "        target_path = DATA_DIR / filename\n",
    "\n",
    "        if target_path.exists():\n",
    "            print(f\"  Found: {target_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Searching for {filename}...\")\n",
    "        found_path = find_file_recursive(filename, BASE_DIR)\n",
    "\n",
    "        if found_path:\n",
    "            print(f\"  Found: {found_path}\")\n",
    "            if found_path != target_path:\n",
    "                print(f\"  Copying to: {target_path}\")\n",
    "                shutil.copy2(found_path, target_path)\n",
    "            continue\n",
    "\n",
    "        print(f\"  File not found locally: {filename}\")\n",
    "        print(f\"  Downloading from GitHub...\")\n",
    "\n",
    "        try:\n",
    "            progress_bar = DownloadProgressBar(filename)\n",
    "            urllib.request.urlretrieve(url, target_path, reporthook=progress_bar)\n",
    "            print(f\"  Download complete: {target_path}\")\n",
    "        except urllib.error.URLError as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to download {filename}: {e}\\n\"\n",
    "                f\"Please manually download the file from:\\n\"\n",
    "                f\"  {url}\\n\"\n",
    "                f\"and place it in: {DATA_DIR}/\"\n",
    "            )\n",
    "        except PermissionError as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Permission denied when writing to {target_path}: {e}\\n\"\n",
    "                f\"Please check write permissions for: {DATA_DIR}/\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fdacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "def load_raw_data(lang: str) -> pd.DataFrame:\n",
    "    \"\"\"Load raw dataset for specified language.\"\"\"\n",
    "    filename = \"En-Dataset.csv\" if lang == 'en' else \"Fr-Dataset.csv\"\n",
    "    filepath = DATA_DIR / filename\n",
    "    return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ac734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_data(lang: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load cleaned dataset for specified language.\n",
    "    Adds 'source' column if not present.\n",
    "    \"\"\"\n",
    "    lang_prefix = 'En' if lang == 'en' else 'Fr'\n",
    "    filename = f\"{lang_prefix}-Dataset_cleaned.csv\"\n",
    "    filepath = DATA_DIR / filename\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Cleaned dataset not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Add source column if not present\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = df['ID'].apply(\n",
    "            lambda x: 'wiki' if str(x).lower().startswith('wiki') else 'viki'\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727815a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Detection (Integrated from detect_duplicates.py)\n",
    "def assign_unique_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Assign unique Index as first column.\"\"\"\n",
    "    print(\"  Assigning unique Index...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['source'] = df['ID'].apply(\n",
    "        lambda x: 'viki' if str(x).lower().startswith('viki') else 'wiki'\n",
    "    )\n",
    "    \n",
    "    viki_mask = df['source'] == 'viki'\n",
    "    wiki_mask = df['source'] == 'wiki'\n",
    "    \n",
    "    df.loc[viki_mask, 'Index'] = [f\"viki-{i:06d}\" for i in range(1, viki_mask.sum() + 1)]\n",
    "    df.loc[wiki_mask, 'Index'] = [f\"wiki-{i:06d}\" for i in range(1, wiki_mask.sum() + 1)]\n",
    "    \n",
    "    cols = ['Index'] + [c for c in df.columns if c != 'Index']\n",
    "    df = df[cols]\n",
    "    \n",
    "    print(f\"    Vikidia: {viki_mask.sum():,}\")\n",
    "    print(f\"    Wikipedia: {wiki_mask.sum():,}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Detect duplicates and assign dup_type_1, dup_type_2.\n",
    "    \n",
    "    Duplicate types:\n",
    "        - V-V: Vikidia internal duplicates\n",
    "        - V-W: Cross-dataset duplicates (Vikidia and Wikipedia)\n",
    "        - W-W: Wikipedia internal duplicates\n",
    "    \n",
    "    Priority: V-W > V-V > W-W\n",
    "    \"\"\"\n",
    "    print(\"  Detecting duplicates...\")\n",
    "    \n",
    "    # Group sentences by text\n",
    "    sentence_to_indices: Dict[str, List[str]] = defaultdict(list)\n",
    "    index_to_source: Dict[str, str] = {}\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"    Grouping sentences\"):\n",
    "        sent = row['Sentence']\n",
    "        idx = row['Index']\n",
    "        source = row['source']\n",
    "        sentence_to_indices[sent].append(idx)\n",
    "        index_to_source[idx] = source\n",
    "    \n",
    "    # Find duplicate types for each Index\n",
    "    index_dup_types: Dict[str, Set[str]] = defaultdict(set)\n",
    "    \n",
    "    for sent, indices in tqdm(sentence_to_indices.items(), desc=\"    Classifying duplicates\"):\n",
    "        if len(indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        sources = [index_to_source[idx] for idx in indices]\n",
    "        has_viki = 'viki' in sources\n",
    "        has_wiki = 'wiki' in sources\n",
    "        viki_count = sources.count('viki')\n",
    "        wiki_count = sources.count('wiki')\n",
    "        \n",
    "        for idx in indices:\n",
    "            src = index_to_source[idx]\n",
    "            \n",
    "            if has_viki and has_wiki:\n",
    "                index_dup_types[idx].add('V-W')\n",
    "            \n",
    "            if src == 'viki' and viki_count >= 2:\n",
    "                index_dup_types[idx].add('V-V')\n",
    "            \n",
    "            if src == 'wiki' and wiki_count >= 2:\n",
    "                index_dup_types[idx].add('W-W')\n",
    "    \n",
    "    # Build result DataFrame\n",
    "    dup_indices = set(index_dup_types.keys())\n",
    "    dup_df = df[df['Index'].isin(dup_indices)].copy()\n",
    "    \n",
    "    # Assign dup_type_1 and dup_type_2 with priority\n",
    "    priority = {'V-W': 1, 'V-V': 2, 'W-W': 3}\n",
    "    \n",
    "    def get_dup_types(idx: str) -> Tuple[str, str]:\n",
    "        types = index_dup_types.get(idx, set())\n",
    "        if not types:\n",
    "            return ('', '')\n",
    "        sorted_types = sorted(types, key=lambda x: priority.get(x, 99))\n",
    "        type1 = sorted_types[0] if len(sorted_types) >= 1 else ''\n",
    "        type2 = sorted_types[1] if len(sorted_types) >= 2 else ''\n",
    "        return (type1, type2)\n",
    "    \n",
    "    dup_df['dup_type_1'] = dup_df['Index'].apply(lambda x: get_dup_types(x)[0])\n",
    "    dup_df['dup_type_2'] = dup_df['Index'].apply(lambda x: get_dup_types(x)[1])\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total_duplicates': len(dup_df),\n",
    "        'vv_only': ((dup_df['dup_type_1'] == 'V-V') & (dup_df['dup_type_2'] == '')).sum(),\n",
    "        'vw_only': ((dup_df['dup_type_1'] == 'V-W') & (dup_df['dup_type_2'] == '')).sum(),\n",
    "        'ww_only': ((dup_df['dup_type_1'] == 'W-W') & (dup_df['dup_type_2'] == '')).sum(),\n",
    "        'multiple_types': (dup_df['dup_type_2'] != '').sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n    Total duplicate rows: {stats['total_duplicates']:,}\")\n",
    "    print(f\"    V-V only: {stats['vv_only']:,}\")\n",
    "    print(f\"    V-W only: {stats['vw_only']:,}\")\n",
    "    print(f\"    W-W only: {stats['ww_only']:,}\")\n",
    "    print(f\"    Multiple types: {stats['multiple_types']:,}\")\n",
    "    \n",
    "    return dup_df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58092759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Basic cleaning: remove NaN and blank sentences.\n",
    "    \n",
    "    This aligns with the feature_extraction_kaggle_multi.py pipeline.\n",
    "    \"\"\"\n",
    "    print(\"  Basic cleaning...\")\n",
    "    \n",
    "    stats = {'original_rows': len(df)}\n",
    "    \n",
    "    # Drop NaN in critical columns\n",
    "    critical_cols = ['Index', 'Sentence', 'Label']\n",
    "    nan_before = len(df)\n",
    "    df = df.dropna(subset=critical_cols)\n",
    "    stats['nan_removed'] = nan_before - len(df)\n",
    "    print(f\"    NaN removed: {stats['nan_removed']:,}\")\n",
    "    \n",
    "    # Remove blank sentences\n",
    "    blank_before = len(df)\n",
    "    df = df[df['Sentence'].str.strip().str.len() > 0]\n",
    "    stats['blank_removed'] = blank_before - len(df)\n",
    "    print(f\"    Blank removed: {stats['blank_removed']:,}\")\n",
    "    \n",
    "    return df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_dataset(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Create cleaned dataset using the same logic as feature_extraction_kaggle_multi.py.\n",
    "    \n",
    "    Cleaning Strategy:\n",
    "        1. Basic cleaning: remove NaN and blank sentences\n",
    "        2. V-V: Remove Vikidia internal duplicates (keep first)\n",
    "        3. W-W: Remove Wikipedia internal duplicates (keep first)\n",
    "        4. V-W: Remove cross-dataset duplicates from Vikidia (keep Wikipedia version)\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_df: Cleaned dataset\n",
    "        stats: Dictionary of cleaning statistics\n",
    "    \"\"\"\n",
    "    print(\"  Creating cleaned dataset (aligned with feature extraction pipeline)...\")\n",
    "    \n",
    "    # Step 1: Basic cleaning\n",
    "    df, stats = basic_cleaning(df)\n",
    "    \n",
    "    # Step 2: Split by source\n",
    "    viki_df = df[df['source'] == 'viki'].copy()\n",
    "    wiki_df = df[df['source'] == 'wiki'].copy()\n",
    "    \n",
    "    print(f\"\\n    Before duplicate removal:\")\n",
    "    print(f\"      Vikidia: {len(viki_df):,}\")\n",
    "    print(f\"      Wikipedia: {len(wiki_df):,}\")\n",
    "    \n",
    "    # Step 3a: Vikidia internal duplicates (V-V)\n",
    "    viki_before = len(viki_df)\n",
    "    viki_df = viki_df.drop_duplicates(subset=['Sentence'], keep='first')\n",
    "    stats['viki_internal_dup'] = viki_before - len(viki_df)\n",
    "    print(f\"    V-V (Vikidia internal duplicates): {stats['viki_internal_dup']:,}\")\n",
    "    \n",
    "    # Step 3b: Wikipedia internal duplicates (W-W)\n",
    "    wiki_before = len(wiki_df)\n",
    "    wiki_df = wiki_df.drop_duplicates(subset=['Sentence'], keep='first')\n",
    "    stats['wiki_internal_dup'] = wiki_before - len(wiki_df)\n",
    "    print(f\"    W-W (Wikipedia internal duplicates): {stats['wiki_internal_dup']:,}\")\n",
    "    \n",
    "    # Step 3c: Cross-dataset leakage (V-W) - remove from Vikidia\n",
    "    wiki_sentences = set(wiki_df['Sentence'].values)\n",
    "    leakage_mask = viki_df['Sentence'].isin(wiki_sentences)\n",
    "    stats['leakage_removed'] = int(leakage_mask.sum())\n",
    "    viki_df = viki_df[~leakage_mask]\n",
    "    print(f\"    V-W (Leakage, removed from Vikidia): {stats['leakage_removed']:,}\")\n",
    "    \n",
    "    # Combine back\n",
    "    cleaned_df = pd.concat([viki_df, wiki_df], ignore_index=True)\n",
    "    cleaned_df = cleaned_df.sort_values('Index').reset_index(drop=True)\n",
    "    \n",
    "    # Keep only output columns\n",
    "    output_cols = ['Index', 'ID', 'Name', 'Sentence', 'Label', 'LengthWords', 'LengthChars']\n",
    "    output_cols = [c for c in output_cols if c in cleaned_df.columns]\n",
    "    cleaned_df = cleaned_df[output_cols]\n",
    "    \n",
    "    stats['after_dedup'] = len(cleaned_df)\n",
    "    stats['viki_final'] = len(viki_df)\n",
    "    stats['wiki_final'] = len(wiki_df)\n",
    "    \n",
    "    n_removed = stats['original_rows'] - len(cleaned_df)\n",
    "    print(f\"\\n    Original rows: {stats['original_rows']:,}\")\n",
    "    print(f\"    Total removed: {n_removed:,}\")\n",
    "    print(f\"    Cleaned rows: {len(cleaned_df):,}\")\n",
    "    print(f\"      Vikidia: {stats['viki_final']:,}\")\n",
    "    print(f\"      Wikipedia: {stats['wiki_final']:,}\")\n",
    "    \n",
    "    return cleaned_df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(lang: str) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for one language.\n",
    "    \n",
    "    Pipeline (aligned with feature_extraction_kaggle_multi.py):\n",
    "        1. Load raw data\n",
    "        2. Assign unique index\n",
    "        3. Detect duplicates (for reporting only)\n",
    "        4. Create cleaned dataset:\n",
    "           - Basic cleaning (NaN, blank)\n",
    "           - V-V removal\n",
    "           - W-W removal\n",
    "           - V-W leakage removal (from Vikidia)\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_df: Cleaned dataset\n",
    "        dup_df: DataFrame of duplicates (for analysis)\n",
    "        stats: Dictionary of statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PREPROCESSING {lang.upper()} DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load raw data\n",
    "    print(f\"\\nLoading raw data...\")\n",
    "    raw_df = load_raw_data(lang)\n",
    "    print(f\"  Loaded {len(raw_df):,} rows\")\n",
    "    \n",
    "    # Assign unique index\n",
    "    indexed_df = assign_unique_index(raw_df)\n",
    "    \n",
    "    # Detect duplicates (for reporting - uses original data before cleaning)\n",
    "    dup_df, dup_stats = detect_duplicates(indexed_df)\n",
    "    \n",
    "    # Create cleaned dataset (with new cleaning logic)\n",
    "    cleaned_df, cleaning_stats = create_cleaned_dataset(indexed_df)\n",
    "    \n",
    "    # Merge stats\n",
    "    all_stats = {**dup_stats, **cleaning_stats}\n",
    "    \n",
    "    # Save outputs\n",
    "    lang_prefix = 'En' if lang == 'en' else 'Fr'\n",
    "    \n",
    "    # Save duplicates report\n",
    "    dup_output = OUTPUT_DIR / f\"{lang}_duplicates.csv\"\n",
    "    output_cols = ['Index', 'ID', 'Name', 'Sentence', 'Label', \n",
    "                   'LengthWords', 'LengthChars', 'dup_type_1', 'dup_type_2']\n",
    "    output_cols = [c for c in output_cols if c in dup_df.columns]\n",
    "    dup_df[output_cols].to_csv(dup_output, index=False)\n",
    "    print(f\"\\n  Saved duplicates report: {dup_output}\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    cleaned_output = DATA_DIR / f\"{lang_prefix}-Dataset_cleaned.csv\"\n",
    "    cleaned_df.to_csv(cleaned_output, index=False)\n",
    "    print(f\"  Saved cleaned dataset: {cleaned_output}\")\n",
    "    \n",
    "    # Print cleaning summary\n",
    "    print(f\"\\n  Cleaning Summary:\")\n",
    "    print(f\"    NaN removed: {cleaning_stats.get('nan_removed', 0):,}\")\n",
    "    print(f\"    Blank removed: {cleaning_stats.get('blank_removed', 0):,}\")\n",
    "    print(f\"    V-V duplicates removed: {cleaning_stats.get('viki_internal_dup', 0):,}\")\n",
    "    print(f\"    W-W duplicates removed: {cleaning_stats.get('wiki_internal_dup', 0):,}\")\n",
    "    print(f\"    V-W leakage removed: {cleaning_stats.get('leakage_removed', 0):,}\")\n",
    "    \n",
    "    return cleaned_df, dup_df, all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics Functions (Using Cleaned Data)\n",
    "def print_descriptive_statistics(name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print descriptive statistics using pandas describe().\"\"\"\n",
    "    print(f\"\\n{name} Descriptive Statistics (Cleaned Data)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df.describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e311c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Return label counts sorted by label.\"\"\"\n",
    "    return df[\"Label\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iqr(df: pd.DataFrame, column: str = \"LengthWords\") -> Tuple[float, float]:\n",
    "    \"\"\"Return the (Q1, Q3) interquartile range for specified column.\"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    return q1, q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5692f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(name: str, df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Generate summary statistics for a cleaned dataset.\n",
    "    \n",
    "    Uses official label semantics:\n",
    "        - Label = 0: Simple\n",
    "        - Label = 1: Complex\n",
    "    \"\"\"\n",
    "    total_sentences = len(df)\n",
    "    \n",
    "    # Add source if not present\n",
    "    if 'source' not in df.columns:\n",
    "        df = df.copy()\n",
    "        df['source'] = df['ID'].apply(\n",
    "            lambda x: 'wiki' if str(x).lower().startswith('wiki') else 'viki'\n",
    "        )\n",
    "    \n",
    "    # Statistics by Label (complexity)\n",
    "    n_simple = (df['Label'] == 0).sum()\n",
    "    n_complex = (df['Label'] == 1).sum()\n",
    "    \n",
    "    # Statistics by Source\n",
    "    n_wiki = (df['source'] == 'wiki').sum()\n",
    "    n_viki = (df['source'] == 'viki').sum()\n",
    "    \n",
    "    print(f\"\\n{name} Dataset (Cleaned)\")\n",
    "    print(\"=\" * (len(name) + 18))\n",
    "    print(f\"Total sentences: {total_sentences:,}\")\n",
    "    \n",
    "    print(f\"\\nBy Complexity (Label):\")\n",
    "    print(f\"  Simple (Label=0):  {n_simple:,} ({n_simple/total_sentences*100:.2f}%)\")\n",
    "    print(f\"  Complex (Label=1): {n_complex:,} ({n_complex/total_sentences*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nBy Source:\")\n",
    "    print(f\"  Wikipedia: {n_wiki:,} ({n_wiki/total_sentences*100:.2f}%)\")\n",
    "    print(f\"  Vikidia:   {n_viki:,} ({n_viki/total_sentences*100:.2f}%)\")\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    print(f\"\\nCross-tabulation (Source x Label):\")\n",
    "    cross_tab = pd.crosstab(df['source'], df['Label'], margins=True)\n",
    "    cross_tab.columns = ['Simple(0)', 'Complex(1)', 'Total']\n",
    "    cross_tab.index = ['viki', 'wiki', 'Total']\n",
    "    print(cross_tab.to_string())\n",
    "    \n",
    "    print(\"\\nIQR for LengthWords:\")\n",
    "    subsets = [\n",
    "        (\"all\", df),\n",
    "        (\"Simple (Label=0)\", df[df[\"Label\"] == 0]),\n",
    "        (\"Complex (Label=1)\", df[df[\"Label\"] == 1]),\n",
    "    ]\n",
    "\n",
    "    iqr_results = {}\n",
    "    for subset_name, subset_df in subsets:\n",
    "        if subset_df.empty:\n",
    "            print(f\"  {subset_name}: no data\")\n",
    "            continue\n",
    "        q1, q3 = compute_iqr(subset_df)\n",
    "        iqr_results[subset_name] = (q1, q3)\n",
    "        print(f\"  {subset_name}: Q1={q1:.0f}, Q3={q3:.0f}\")\n",
    "\n",
    "    return {\n",
    "        \"total_sentences\": total_sentences,\n",
    "        \"n_simple\": n_simple,\n",
    "        \"n_complex\": n_complex,\n",
    "        \"n_wiki\": n_wiki,\n",
    "        \"n_viki\": n_viki,\n",
    "        \"iqr\": iqr_results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions (Using Cleaned Data)\n",
    "def plot_descriptive_statistics(\n",
    "    en_df: pd.DataFrame, fr_df: pd.DataFrame, output_dir: Path\n",
    ") -> None:\n",
    "    \"\"\"Generate a table image of descriptive statistics.\"\"\"\n",
    "    en_desc = en_df.describe().round(2)\n",
    "    fr_desc = fr_df.describe().round(2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(\n",
    "        \"Descriptive Statistics (Cleaned Data)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_title(\"English Dataset\", fontsize=12, fontweight=\"bold\")\n",
    "    en_table = axes[0].table(\n",
    "        cellText=en_desc.values,\n",
    "        rowLabels=en_desc.index,\n",
    "        colLabels=en_desc.columns,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    en_table.auto_set_font_size(False)\n",
    "    en_table.set_fontsize(9)\n",
    "    en_table.scale(1.2, 1.5)\n",
    "\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"French Dataset\", fontsize=12, fontweight=\"bold\")\n",
    "    fr_table = axes[1].table(\n",
    "        cellText=fr_desc.values,\n",
    "        rowLabels=fr_desc.index,\n",
    "        colLabels=fr_desc.columns,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    fr_table.auto_set_font_size(False)\n",
    "    fr_table.set_fontsize(9)\n",
    "    fr_table.scale(1.2, 1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"descriptive_statistics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {output_dir}/descriptive_statistics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_statistics(\n",
    "    en_df: pd.DataFrame, fr_df: pd.DataFrame, output_dir: Path\n",
    ") -> None:\n",
    "    \"\"\"Generate a table image of summary statistics.\"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    for name, df in [(\"English\", en_df), (\"French\", fr_df)]:\n",
    "        total = len(df)\n",
    "        # Label=0 is Simple, Label=1 is Complex\n",
    "        n_simple = (df[\"Label\"] == 0).sum()\n",
    "        n_complex = (df[\"Label\"] == 1).sum()\n",
    "        pct_simple = n_simple / total * 100\n",
    "        pct_complex = n_complex / total * 100\n",
    "\n",
    "        q1_all, q3_all = compute_iqr(df)\n",
    "        q1_simple, q3_simple = compute_iqr(df[df[\"Label\"] == 0])\n",
    "        q1_complex, q3_complex = compute_iqr(df[df[\"Label\"] == 1])\n",
    "\n",
    "        summary_data.append([\n",
    "            name,\n",
    "            f\"{total:,}\",\n",
    "            f\"{n_simple:,} ({pct_simple:.2f}%)\",\n",
    "            f\"{n_complex:,} ({pct_complex:.2f}%)\",\n",
    "            f\"Q1={q1_all:.0f}, Q3={q3_all:.0f}\",\n",
    "            f\"Q1={q1_simple:.0f}, Q3={q3_simple:.0f}\",\n",
    "            f\"Q1={q1_complex:.0f}, Q3={q3_complex:.0f}\",\n",
    "        ])\n",
    "\n",
    "    col_labels = [\n",
    "        \"Dataset\",\n",
    "        \"Total Sentences\",\n",
    "        \"Simple (Label=0)\",\n",
    "        \"Complex (Label=1)\",\n",
    "        \"IQR (all)\",\n",
    "        \"IQR (Simple)\",\n",
    "        \"IQR (Complex)\",\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 3))\n",
    "    fig.suptitle(\n",
    "        \"Dataset Summary Statistics (Cleaned Data)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.95,\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    table = ax.table(\n",
    "        cellText=summary_data,\n",
    "        colLabels=col_labels,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2.0)\n",
    "\n",
    "    for j in range(len(col_labels)):\n",
    "        table[(0, j)].set_facecolor(\"#4472C4\")\n",
    "        table[(0, j)].set_text_props(color=\"white\", fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"summary_statistics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {output_dir}/summary_statistics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77556d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distributions(\n",
    "    en_df: pd.DataFrame, fr_df: pd.DataFrame, output_dir: Path\n",
    ") -> None:\n",
    "    \"\"\"Generate unified 2x2 visualization for sentence length distributions.\"\"\"\n",
    "    en_plot = en_df.copy()\n",
    "    fr_plot = fr_df.copy()\n",
    "    en_plot[\"Language\"] = \"English\"\n",
    "    fr_plot[\"Language\"] = \"French\"\n",
    "\n",
    "    combined_df = pd.concat(\n",
    "        [\n",
    "            en_plot[[\"Language\", \"LengthWords\", \"LengthChars\"]],\n",
    "            fr_plot[[\"Language\", \"LengthWords\", \"LengthChars\"]],\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    colors = {\"English\": \"#3274A1\", \"French\": \"#E1812C\"}\n",
    "    palette = [colors[\"English\"], colors[\"French\"]]\n",
    "\n",
    "    xlim_words = combined_df[\"LengthWords\"].quantile(0.99)\n",
    "    xlim_chars = combined_df[\"LengthChars\"].quantile(0.99)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\n",
    "        \"Sentence Length Distribution (Cleaned Data): English vs French\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "\n",
    "    vp1 = sns.violinplot(\n",
    "        x=\"Language\",\n",
    "        y=\"LengthWords\",\n",
    "        data=combined_df,\n",
    "        ax=axes[0, 0],\n",
    "        palette=palette,\n",
    "        inner=\"quartile\",\n",
    "        cut=0,\n",
    "    )\n",
    "    for violin in vp1.collections:\n",
    "        violin.set_alpha(0.5)\n",
    "    axes[0, 0].set_title(\"Distribution by Words\")\n",
    "    axes[0, 0].set_xlabel(\"\")\n",
    "    axes[0, 0].set_ylabel(\"Number of Words\")\n",
    "    axes[0, 0].set_ylim(0, xlim_words)\n",
    "\n",
    "    vp2 = sns.violinplot(\n",
    "        x=\"Language\",\n",
    "        y=\"LengthChars\",\n",
    "        data=combined_df,\n",
    "        ax=axes[0, 1],\n",
    "        palette=palette,\n",
    "        inner=\"quartile\",\n",
    "        cut=0,\n",
    "    )\n",
    "    for violin in vp2.collections:\n",
    "        violin.set_alpha(0.5)\n",
    "    axes[0, 1].set_title(\"Distribution by Characters\")\n",
    "    axes[0, 1].set_xlabel(\"\")\n",
    "    axes[0, 1].set_ylabel(\"Number of Characters\")\n",
    "    axes[0, 1].set_ylim(0, xlim_chars)\n",
    "\n",
    "    sns.histplot(\n",
    "        en_plot[\"LengthWords\"],\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        ax=axes[1, 0],\n",
    "        color=colors[\"English\"],\n",
    "        label=\"English\",\n",
    "        alpha=0.5,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    sns.histplot(\n",
    "        fr_plot[\"LengthWords\"],\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        ax=axes[1, 0],\n",
    "        color=colors[\"French\"],\n",
    "        label=\"French\",\n",
    "        alpha=0.5,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    axes[1, 0].set_title(\"Density Distribution by Words\")\n",
    "    axes[1, 0].set_xlabel(\"Number of Words\")\n",
    "    axes[1, 0].set_ylabel(\"Density\")\n",
    "    axes[1, 0].set_xlim(0, xlim_words)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    sns.histplot(\n",
    "        en_plot[\"LengthChars\"],\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        ax=axes[1, 1],\n",
    "        color=colors[\"English\"],\n",
    "        label=\"English\",\n",
    "        alpha=0.5,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    sns.histplot(\n",
    "        fr_plot[\"LengthChars\"],\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        ax=axes[1, 1],\n",
    "        color=colors[\"French\"],\n",
    "        label=\"French\",\n",
    "        alpha=0.5,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Density Distribution by Characters\")\n",
    "    axes[1, 1].set_xlabel(\"Number of Characters\")\n",
    "    axes[1, 1].set_ylabel(\"Density\")\n",
    "    axes[1, 1].set_xlim(0, xlim_chars)\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"length_distributions.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {output_dir}/length_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_source_analysis(\n",
    "    en_df: pd.DataFrame, fr_df: pd.DataFrame, output_dir: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the relationship between Label and Source.\n",
    "    This reveals that Label = Source in the original data.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(\n",
    "        \"Label vs Source Analysis (Cleaned Data)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    \n",
    "    for idx, (name, df) in enumerate([(\"English\", en_df), (\"French\", fr_df)]):\n",
    "        df_copy = df.copy()\n",
    "        if 'source' not in df_copy.columns:\n",
    "            df_copy['source'] = df_copy['ID'].apply(\n",
    "                lambda x: 'wiki' if str(x).lower().startswith('wiki') else 'viki'\n",
    "            )\n",
    "        \n",
    "        cross_tab = pd.crosstab(df_copy['source'], df_copy['Label'])\n",
    "        cross_tab.columns = ['Simple (0)', 'Complex (1)']\n",
    "        cross_tab.index = ['Vikidia', 'Wikipedia']\n",
    "        \n",
    "        sns.heatmap(\n",
    "            cross_tab,\n",
    "            annot=True,\n",
    "            fmt=',d',\n",
    "            cmap='Blues',\n",
    "            ax=axes[idx],\n",
    "            cbar=True\n",
    "        )\n",
    "        axes[idx].set_title(f\"{name}: Source x Label\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[idx].set_xlabel(\"Label (Complexity)\")\n",
    "        axes[idx].set_ylabel(\"Source\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"label_source_analysis.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: {output_dir}/label_source_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2534e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_samples(\n",
    "    en_df: pd.DataFrame, fr_df: pd.DataFrame, output_dir: Path, n_samples: int = 10\n",
    ") -> None:\n",
    "    \"\"\"Display sample sentences from each label category and save as image.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Sample Sentences Inspection\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for name, df in [(\"English\", en_df), (\"French\", fr_df)]:\n",
    "        print(f\"\\n{name} samples:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        sample_texts = []\n",
    "\n",
    "        for label in [0, 1]:\n",
    "            label_name = LABEL_NAMES[label]\n",
    "            subset = df[df[\"Label\"] == label]\n",
    "            samples = subset.sample(n=min(n_samples, len(subset)), random_state=42)\n",
    "\n",
    "            print(f\"\\nLabel {label} ({label_name}):\")\n",
    "            sample_texts.append(f\"Label {label} ({label_name}):\")\n",
    "            sample_texts.append(\"-\" * 50)\n",
    "\n",
    "            for idx, row in samples.iterrows():\n",
    "                sentence = row[\"Sentence\"]\n",
    "                line = f\"[{row['LengthWords']} words] {sentence}\"\n",
    "                print(f\"  {line[:100]}...\")\n",
    "                sample_texts.append(line)\n",
    "\n",
    "            sample_texts.append(\"\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 14))\n",
    "        fig.suptitle(\n",
    "            f\"Sample Sentences: {name} (Cleaned Data)\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            y=0.98,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        wrapped_lines = []\n",
    "        for line in sample_texts:\n",
    "            if line.startswith(\"[\"):\n",
    "                wrapped = textwrap.fill(line, width=100)\n",
    "                wrapped_lines.append(wrapped)\n",
    "            else:\n",
    "                wrapped_lines.append(line)\n",
    "\n",
    "        full_text = \"\\n\".join(wrapped_lines)\n",
    "        ax.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            full_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=9,\n",
    "            fontfamily=\"monospace\",\n",
    "            verticalalignment=\"top\",\n",
    "            horizontalalignment=\"left\",\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f\"sample_sentences_{name.lower()}.png\"\n",
    "        plt.savefig(output_dir / filename, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Saved: {output_dir}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Entry Point\n",
    "def main() -> None:\n",
    "    \"\"\"Main entry point for Task 0 data overview.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"iDEM TASK 0: DATA OVERVIEW AND PREPROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Data directory: {DATA_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Ensure raw data files exist\n",
    "    ensure_data_exists()\n",
    "\n",
    "    # Check if cleaned datasets already exist\n",
    "    en_cleaned_path = DATA_DIR / \"En-Dataset_cleaned.csv\"\n",
    "    fr_cleaned_path = DATA_DIR / \"Fr-Dataset_cleaned.csv\"\n",
    "    \n",
    "    if en_cleaned_path.exists() and fr_cleaned_path.exists():\n",
    "        print(\"\\nCleaned datasets already exist.\")\n",
    "        print(f\"  {en_cleaned_path}\")\n",
    "        print(f\"  {fr_cleaned_path}\")\n",
    "        print(\"Loading cleaned data for analysis...\")\n",
    "        \n",
    "        en_df = load_cleaned_data('en')\n",
    "        fr_df = load_cleaned_data('fr')\n",
    "        \n",
    "        print(f\"  English: {len(en_df):,} sentences\")\n",
    "        print(f\"  French:  {len(fr_df):,} sentences\")\n",
    "    else:\n",
    "        print(\"\\nCleaned datasets not found. Running preprocessing...\")\n",
    "        \n",
    "        # Preprocess English\n",
    "        en_cleaned, en_dup_df, en_dup_stats = preprocess_dataset('en')\n",
    "        \n",
    "        # Preprocess French\n",
    "        fr_cleaned, fr_dup_df, fr_dup_stats = preprocess_dataset('fr')\n",
    "        \n",
    "        # Reload with source column\n",
    "        en_df = load_cleaned_data('en')\n",
    "        fr_df = load_cleaned_data('fr')\n",
    "\n",
    "    # Descriptive statistics\n",
    "    print_descriptive_statistics(\"English\", en_df)\n",
    "    print_descriptive_statistics(\"French\", fr_df)\n",
    "\n",
    "    # Summary statistics\n",
    "    en_stats = summarize_dataset(\"English\", en_df)\n",
    "    fr_stats = summarize_dataset(\"French\", fr_df)\n",
    "\n",
    "    # Generate visualizations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    plot_descriptive_statistics(en_df, fr_df, OUTPUT_DIR)\n",
    "    plot_summary_statistics(en_df, fr_df, OUTPUT_DIR)\n",
    "    plot_length_distributions(en_df, fr_df, OUTPUT_DIR)\n",
    "    plot_label_source_analysis(en_df, fr_df, OUTPUT_DIR)\n",
    "    inspect_samples(en_df, fr_df, OUTPUT_DIR, n_samples=10)\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 0 COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"  - {DATA_DIR}/En-Dataset_cleaned.csv\")\n",
    "    print(f\"  - {DATA_DIR}/Fr-Dataset_cleaned.csv\")\n",
    "    print(f\"  - {OUTPUT_DIR}/en_duplicates.csv\")\n",
    "    print(f\"  - {OUTPUT_DIR}/fr_duplicates.csv\")\n",
    "    print(f\"  - {OUTPUT_DIR}/*.png (visualizations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cleaned, en_dup_df, en_stats = preprocess_dataset('en')\n",
    "fr_cleaned, fr_dup_df, fr_stats = preprocess_dataset('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae60929",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
