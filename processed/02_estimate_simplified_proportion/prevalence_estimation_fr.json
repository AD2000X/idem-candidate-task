{
  "language": "French",
  "dataset_info": {
    "total_sentences": 1653175,
    "simple_vikidia_count": 188856,
    "complex_wikipedia_count": 1464319
  },
  "anchor_selection": {
    "q1": 15.0,
    "q3": 30.0,
    "n_simple_anchors": 67064,
    "n_complex_anchors": 387517,
    "total_anchors": 454581
  },
  "anchor_quality": {
    "anchor_ratio": 0.27497451872911216,
    "n_total": 1653175,
    "n_anchor": 454581,
    "lengthwords_global_mean": 24.25327082734738,
    "lengthwords_global_std": 14.098006472718748,
    "lengthwords_anchor_mean": 37.55319514014004,
    "lengthwords_anchor_std": 19.043374317535505,
    "anchor_source_breakdown": {
      "wiki": 387517,
      "viki": 67064
    },
    "anchor_label_breakdown": {
      "simple_0": 67064,
      "complex_1": 387517
    }
  },
  "anchor_shuffle_sanity_check": {
    "n_samples_used": 50000,
    "n_total_anchor_samples": 454581,
    "mean_f1": 0.9109834459916734,
    "std_f1": 0.0013769593784531697,
    "n_splits": 3
  },
  "training": {
    "best_model": "random_forest",
    "model_results": {
      "logistic_regression": {
        "best_params": {
          "classifier__solver": "saga",
          "classifier__penalty": "l2",
          "classifier__C": 1.0
        },
        "cv_f1": 0.9999854842533678,
        "val_f1": 0.999925451021321,
        "val_roc_auc": 1.0
      },
      "random_forest": {
        "best_params": {
          "classifier__n_estimators": 100,
          "classifier__min_samples_split": 20,
          "classifier__min_samples_leaf": 4,
          "classifier__max_depth": 5
        },
        "cv_f1": 0.9999903230800989,
        "val_f1": 0.9999627213420317,
        "val_roc_auc": 0.9999999797991681
      }
    }
  },
  "calibration": {
    "n_folds": 5,
    "confusion_matrix": {
      "tn": 387517,
      "fp": 0,
      "fn": 6,
      "tp": 67058
    },
    "tpr": 0.9999105332219969,
    "fpr": 0.0
  },
  "estimates": {
    "naive_proportion": 0.11423835952031697,
    "predicted_proportion": 0.6263934550183737,
    "acc_corrected_proportion": 0.626449501436849,
    "wikipedia_analysis": {
      "n_sentences": 1464319,
      "soft_estimate": 0.5667590656130436,
      "hard_estimate": 0.49234558863198524,
      "hard_threshold": 0.8
    },
    "descriptions": {
      "acc_corrected_proportion": "Adjusted true proportion of simple sentences in the full dataset using ACC calibration.",
      "wikipedia_soft_estimate": "Proportion of Vikidia-like simple sentences among complex-labelled Wikipedia sentences (soft estimate: mean P(Simple) over Wikipedia sentences).",
      "wikipedia_hard_estimate": "Proportion of Vikidia-like simple sentences among complex-labelled Wikipedia sentences (hard estimate: fraction with P(Simple) >= 0.8)."
    }
  },
  "prediction_analysis": {
    "threshold_for_hard_simple": 0.5,
    "confusion_matrix_full": {
      "tn": 567225,
      "fp": 897094,
      "fn": 50412,
      "tp": 138444,
      "n_sentences": 1653175
    },
    "metrics_full": {
      "accuracy": 0.42685680584330155,
      "f1_simple_label0": 0.22614289191224393
    },
    "confusion_matrix_by_source": {
      "viki": {
        "tn": 0,
        "fp": 0,
        "fn": 50412,
        "tp": 138444,
        "n_sentences": 188856
      },
      "wiki": {
        "tn": 567225,
        "fp": 897094,
        "fn": 0,
        "tp": 0,
        "n_sentences": 1464319
      }
    },
    "by_source_prevalence": {
      "viki": {
        "n_sentences": 188856,
        "naive_simple_proportion_label0": 1.0,
        "predicted_simple_soft_mean_prob": 0.6786756246360777,
        "predicted_simple_hard_prop_p>=threshold": 0.7330664633371458
      },
      "wiki": {
        "n_sentences": 1464319,
        "naive_simple_proportion_label0": 0.0,
        "predicted_simple_soft_mean_prob": 0.5667590656130436,
        "predicted_simple_hard_prop_p>=threshold": 0.6126356347216693
      }
    },
    "length_bin_profile": [
      {
        "length_bin": "<=10",
        "n_sentences": 71015,
        "mean_p_simple": 0.9825424713217403,
        "hard_simple_prop": 0.9998873477434345
      },
      {
        "length_bin": "11-20",
        "n_sentences": 725282,
        "mean_p_simple": 0.9238634772980281,
        "hard_simple_prop": 0.9959160712660731
      },
      {
        "length_bin": "21-30",
        "n_sentences": 472806,
        "mean_p_simple": 0.44044213864145465,
        "hard_simple_prop": 0.5122841080696946
      },
      {
        "length_bin": "31-40",
        "n_sentences": 222086,
        "mean_p_simple": 0.0328728060008576,
        "hard_simple_prop": 0.0
      },
      {
        "length_bin": ">=41",
        "n_sentences": 161986,
        "mean_p_simple": 0.01671102457680092,
        "hard_simple_prop": 0.0
      }
    ],
    "mislabel_stats": {
      "high_conf_threshold": 0.9,
      "low_conf_threshold": 0.09999999999999998,
      "complex_label_high_simple_prob_count": 542865,
      "simple_label_low_simple_prob_count": 42700,
      "total_candidates": 585565,
      "output_file": "c:\\Users\\chang\\iDEMRA\\results\\mislabel_candidates_fr.csv"
    }
  }
}