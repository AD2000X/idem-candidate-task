{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":""},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13904575,"sourceType":"datasetVersion","datasetId":8858906}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bc95e86f","cell_type":"code","source":"\"\"\"\niDEM Feature Engineering - Kaggle Notebook Version (T4 GPU + 4 CPU)\n====================================================================\nComplete feature extraction pipeline for text complexity classification.\nOptimized for Kaggle Notebook with T4 GPU and 4 CPU cores.\n\n** WITH CHECKPOINT SUPPORT **\nIf disconnected, re-run and it will resume from last checkpoint.\n\nHardware Utilization:\n    - 4 CPU cores: Basic feature extraction (via joblib)\n    - T4 GPU: Sentence-BERT embeddings (single GPU, large batch size)\n    - Single-thread: spaCy parsing (Notebook compatibility mode)\n\nNote: Multi-process encoding for spaCy and SBERT is disabled due to \nKaggle Notebook (Jupyter/IPython) multiprocessing limitations.\n\nPipeline Steps:\n    1. Load raw En-Dataset.csv / Fr-Dataset.csv\n    2. Assign unique Index: viki-000001, wiki-000001...\n    3. Basic cleaning: remove NaN, blank rows\n    4. Duplicate detection and removal:\n       - 4a: Vikidia internal duplicates\n       - 4b: Wikipedia internal duplicates\n       - 4c: Cross-dataset duplicates (Leakage) - remove from Vikidia\n    5. Save En-Dataset_cleaned.csv / Fr-Dataset_cleaned.csv\n    6. Extract ALL features for ENTIRE dataset\n    7. Compute Sentence-BERT embeddings + Cosine Similarity (GPU)\n    8. Output en_cleaned_features.csv / fr_cleaned_features.csv\n\nCheckpoints saved:\n    - checkpoint_{lang}_cleaned.csv      (after Step 4)\n    - checkpoint_{lang}_basic.csv        (after basic features)\n    - checkpoint_{lang}_spacy.csv        (after spaCy features)\n    - checkpoint_{lang}_sbert.npy        (after SBERT embeddings)\n\nOutput Columns (26 total):\n    - Index columns: Index, ID, Name, Sentence, Label, LengthWords, LengthChars\n    - New features: words_chars_ratio, cos_simi\n    - Basic features: avg_word_len, long_word_ratio, ttr, punct_density,\n                      comma_density, digit_ratio, upper_ratio, has_parens\n    - spaCy features: n_tokens, max_depth, avg_depth, avg_dependency_distance,\n                      func_word_ratio, n_clauses, clause_ratio, noun_ratio, verb_ratio\n\nKaggle Setup:\n    1. Enable GPU: Settings > Accelerator > GPU T4 x2\n    2. Enable Internet: Settings > Internet > On\n    3. Upload En-Dataset.csv and Fr-Dataset.csv to /kaggle/input/\n\nRun in Kaggle Notebook:\n    # Cell 1: Install dependencies\n    !pip install sentence-transformers spacy joblib -q\n    !python -m spacy download en_core_web_sm -q\n    !python -m spacy download fr_core_news_sm -q\n    \n    # Cell 2: Run pipeline\n    %run feature_extraction_kaggle_multi.py\n\"\"\"","metadata":{"lines_to_next_cell":2,"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.457522Z","iopub.execute_input":"2025-11-30T21:25:13.458047Z","iopub.status.idle":"2025-11-30T21:25:13.464987Z","shell.execute_reply.started":"2025-11-30T21:25:13.458025Z","shell.execute_reply":"2025-11-30T21:25:13.464071Z"}},"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"'\\niDEM Feature Engineering - Kaggle Notebook Version (T4 GPU + 4 CPU)\\n====================================================================\\nComplete feature extraction pipeline for text complexity classification.\\nOptimized for Kaggle Notebook with T4 GPU and 4 CPU cores.\\n\\n** WITH CHECKPOINT SUPPORT **\\nIf disconnected, re-run and it will resume from last checkpoint.\\n\\nHardware Utilization:\\n    - 4 CPU cores: Basic feature extraction (via joblib)\\n    - T4 GPU: Sentence-BERT embeddings (single GPU, large batch size)\\n    - Single-thread: spaCy parsing (Notebook compatibility mode)\\n\\nNote: Multi-process encoding for spaCy and SBERT is disabled due to \\nKaggle Notebook (Jupyter/IPython) multiprocessing limitations.\\n\\nPipeline Steps:\\n    1. Load raw En-Dataset.csv / Fr-Dataset.csv\\n    2. Assign unique Index: viki-000001, wiki-000001...\\n    3. Basic cleaning: remove NaN, blank rows\\n    4. Duplicate detection and removal:\\n       - 4a: Vikidia internal duplicates\\n       - 4b: Wikipedia internal duplicates\\n       - 4c: Cross-dataset duplicates (Leakage) - remove from Vikidia\\n    5. Save En-Dataset_cleaned.csv / Fr-Dataset_cleaned.csv\\n    6. Extract ALL features for ENTIRE dataset\\n    7. Compute Sentence-BERT embeddings + Cosine Similarity (GPU)\\n    8. Output en_cleaned_features.csv / fr_cleaned_features.csv\\n\\nCheckpoints saved:\\n    - checkpoint_{lang}_cleaned.csv      (after Step 4)\\n    - checkpoint_{lang}_basic.csv        (after basic features)\\n    - checkpoint_{lang}_spacy.csv        (after spaCy features)\\n    - checkpoint_{lang}_sbert.npy        (after SBERT embeddings)\\n\\nOutput Columns (26 total):\\n    - Index columns: Index, ID, Name, Sentence, Label, LengthWords, LengthChars\\n    - New features: words_chars_ratio, cos_simi\\n    - Basic features: avg_word_len, long_word_ratio, ttr, punct_density,\\n                      comma_density, digit_ratio, upper_ratio, has_parens\\n    - spaCy features: n_tokens, max_depth, avg_depth, avg_dependency_distance,\\n                      func_word_ratio, n_clauses, clause_ratio, noun_ratio, verb_ratio\\n\\nKaggle Setup:\\n    1. Enable GPU: Settings > Accelerator > GPU T4 x2\\n    2. Enable Internet: Settings > Internet > On\\n    3. Upload En-Dataset.csv and Fr-Dataset.csv to /kaggle/input/\\n\\nRun in Kaggle Notebook:\\n    # Cell 1: Install dependencies\\n    !pip install sentence-transformers spacy joblib -q\\n    !python -m spacy download en_core_web_sm -q\\n    !python -m spacy download fr_core_news_sm -q\\n    \\n    # Cell 2: Run pipeline\\n    %run feature_extraction_kaggle_multi.py\\n'"},"metadata":{}}],"execution_count":101},{"id":"5f2f8091","cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.466401Z","iopub.execute_input":"2025-11-30T21:25:13.466650Z","iopub.status.idle":"2025-11-30T21:25:13.483832Z","shell.execute_reply.started":"2025-11-30T21:25:13.466634Z","shell.execute_reply":"2025-11-30T21:25:13.483134Z"}},"outputs":[],"execution_count":102},{"id":"28a6becc","cell_type":"code","source":"import gc\nimport warnings\nimport multiprocessing\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.484445Z","iopub.execute_input":"2025-11-30T21:25:13.484702Z","iopub.status.idle":"2025-11-30T21:25:13.498046Z","shell.execute_reply.started":"2025-11-30T21:25:13.484680Z","shell.execute_reply":"2025-11-30T21:25:13.497497Z"}},"outputs":[],"execution_count":103},{"id":"1c723f36","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nfrom joblib import Parallel, delayed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.499384Z","iopub.execute_input":"2025-11-30T21:25:13.499655Z","iopub.status.idle":"2025-11-30T21:25:13.514554Z","shell.execute_reply.started":"2025-11-30T21:25:13.499632Z","shell.execute_reply":"2025-11-30T21:25:13.513807Z"}},"outputs":[],"execution_count":104},{"id":"98cf9ea1","cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.515331Z","iopub.execute_input":"2025-11-30T21:25:13.515555Z","iopub.status.idle":"2025-11-30T21:25:13.529320Z","shell.execute_reply.started":"2025-11-30T21:25:13.515530Z","shell.execute_reply":"2025-11-30T21:25:13.528637Z"}},"outputs":[],"execution_count":105},{"id":"66ba4e40","cell_type":"code","source":"# Hardware Configuration\ndef get_hardware_config() -> Dict: \n    config = {\n        'n_cpus': multiprocessing.cpu_count(),\n        'n_gpus': 0,\n        'gpu_names': [],\n        'gpu_memory': [],\n        'device': 'cpu',\n        'use_multi_gpu': False  # Disabled for Kaggle Notebook compatibility\n    }\n    \n    if torch.cuda.is_available():\n        config['n_gpus'] = torch.cuda.device_count()\n        config['device'] = 'cuda'\n        \n        for i in range(config['n_gpus']):\n            props = torch.cuda.get_device_properties(i)\n            config['gpu_names'].append(props.name)\n            config['gpu_memory'].append(props.total_memory / 1e9)\n        \n        # NOTE: Multi-GPU is disabled due to Kaggle Notebook limitations\n        # config['use_multi_gpu'] = False (keep as False)\n    \n    return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.530085Z","iopub.execute_input":"2025-11-30T21:25:13.530344Z","iopub.status.idle":"2025-11-30T21:25:13.544624Z","shell.execute_reply.started":"2025-11-30T21:25:13.530302Z","shell.execute_reply":"2025-11-30T21:25:13.544065Z"}},"outputs":[],"execution_count":106},{"id":"b17fda74","cell_type":"code","source":"def print_hardware_info(config: Dict) -> None:\n    print(\"=\" * 70)\n    print(\"HARDWARE CONFIGURATION\")\n    print(\"=\" * 70)\n    \n    print(f\"\\nCPU:\")\n    print(f\"  Available cores: {config['n_cpus']}\")\n    print(f\"  Using for basic features: {min(config['n_cpus'], 4)} (joblib)\")\n    print(f\"  Using for spaCy: 1 (single-thread for Notebook compatibility)\")\n    \n    print(f\"\\nGPU:\")\n    if config['n_gpus'] > 0:\n        print(f\"  Available GPUs: {config['n_gpus']}\")\n        for i in range(config['n_gpus']):\n            print(f\"    GPU {i}: {config['gpu_names'][i]} ({config['gpu_memory'][i]:.1f} GB)\")\n        print(f\"  Using: Single GPU (multi-GPU disabled for Notebook compatibility)\")\n        print(f\"  Primary device: {config['device']}\")\n    else:\n        print(\"  No GPU available (using CPU)\")\n    \n    print(\"=\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.545433Z","iopub.execute_input":"2025-11-30T21:25:13.545612Z","iopub.status.idle":"2025-11-30T21:25:13.564258Z","shell.execute_reply.started":"2025-11-30T21:25:13.545597Z","shell.execute_reply":"2025-11-30T21:25:13.563456Z"}},"outputs":[],"execution_count":107},{"id":"5cfd5f42","cell_type":"code","source":"def check_environment():\n    print(\"=\" * 70)\n    print(\"ENVIRONMENT CHECK\")\n    print(\"=\" * 70)\n    \n    # Get hardware config\n    hw_config = get_hardware_config()\n    \n    # Print basic info\n    print(f\"\\nPyTorch version: {torch.__version__}\")\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"CUDA version: {torch.version.cuda}\")\n    \n    is_kaggle = os.path.exists('/kaggle')\n    print(f\"Kaggle Environment: {is_kaggle}\")\n    print(f\"TOKENIZERS_PARALLELISM: {os.environ.get('TOKENIZERS_PARALLELISM', 'not set')}\")\n    \n    print(\"=\" * 70)\n    \n    return hw_config, is_kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.565062Z","iopub.execute_input":"2025-11-30T21:25:13.565310Z","iopub.status.idle":"2025-11-30T21:25:13.581396Z","shell.execute_reply.started":"2025-11-30T21:25:13.565271Z","shell.execute_reply":"2025-11-30T21:25:13.580903Z"}},"outputs":[],"execution_count":108},{"id":"d9054310","cell_type":"code","source":"# Configuration\ndef setup_paths(is_kaggle: bool) -> Tuple[Path, Path, Path]:\n    if is_kaggle:\n        INPUT_DIR = Path('/kaggle/input')\n        WORKING_DIR = Path('/kaggle/working')\n        \n        data_dirs = list(INPUT_DIR.glob('*'))\n        if data_dirs:\n            DATA_DIR = data_dirs[0]\n        else:\n            DATA_DIR = INPUT_DIR\n        \n        OUTPUT_DIR = WORKING_DIR / 'output'\n        CHECKPOINT_DIR = WORKING_DIR / 'checkpoints'\n    else:\n        BASE_DIR = Path('.')\n        DATA_DIR = BASE_DIR / 'data'\n        OUTPUT_DIR = BASE_DIR / 'output'\n        CHECKPOINT_DIR = BASE_DIR / 'checkpoints'\n    \n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n    \n    print(f\"Data directory: {DATA_DIR}\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n    \n    return DATA_DIR, OUTPUT_DIR, CHECKPOINT_DIR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.582229Z","iopub.execute_input":"2025-11-30T21:25:13.582472Z","iopub.status.idle":"2025-11-30T21:25:13.598902Z","shell.execute_reply.started":"2025-11-30T21:25:13.582457Z","shell.execute_reply":"2025-11-30T21:25:13.598324Z"}},"outputs":[],"execution_count":109},{"id":"fcf2aa4e","cell_type":"code","source":"# Batch processing config\nBATCH_SIZE = 5000\nSBERT_BATCH_SIZE = 128  # Base batch size (will be doubled for T4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.601195Z","iopub.execute_input":"2025-11-30T21:25:13.601390Z","iopub.status.idle":"2025-11-30T21:25:13.611383Z","shell.execute_reply.started":"2025-11-30T21:25:13.601375Z","shell.execute_reply":"2025-11-30T21:25:13.610736Z"}},"outputs":[],"execution_count":110},{"id":"bde55f50","cell_type":"code","source":"RANDOM_SEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.612038Z","iopub.execute_input":"2025-11-30T21:25:13.612262Z","iopub.status.idle":"2025-11-30T21:25:13.625033Z","shell.execute_reply.started":"2025-11-30T21:25:13.612247Z","shell.execute_reply":"2025-11-30T21:25:13.624425Z"}},"outputs":[],"execution_count":111},{"id":"5c580798","cell_type":"code","source":"# Feature names\nBASIC_FEATURE_NAMES = [\n    'avg_word_len',\n    'long_word_ratio',\n    'ttr',\n    'punct_density',\n    'comma_density',\n    'digit_ratio',\n    'upper_ratio',\n    'has_parens'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.625608Z","iopub.execute_input":"2025-11-30T21:25:13.625796Z","iopub.status.idle":"2025-11-30T21:25:13.638274Z","shell.execute_reply.started":"2025-11-30T21:25:13.625781Z","shell.execute_reply":"2025-11-30T21:25:13.637522Z"}},"outputs":[],"execution_count":112},{"id":"889b55a7","cell_type":"code","source":"SPACY_FEATURE_NAMES = [\n    'n_tokens',\n    'max_depth',\n    'avg_depth',\n    'avg_dependency_distance',\n    'func_word_ratio',\n    'n_clauses',\n    'clause_ratio',\n    'noun_ratio',\n    'verb_ratio'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.639029Z","iopub.execute_input":"2025-11-30T21:25:13.639475Z","iopub.status.idle":"2025-11-30T21:25:13.652522Z","shell.execute_reply.started":"2025-11-30T21:25:13.639448Z","shell.execute_reply":"2025-11-30T21:25:13.651905Z"}},"outputs":[],"execution_count":113},{"id":"12291b02","cell_type":"code","source":"FUNCTION_WORD_POS = {'DET', 'ADP', 'PRON', 'CCONJ', 'SCONJ', 'AUX', 'PART'}\nCLAUSE_DEPS = {'advcl', 'ccomp', 'acl', 'relcl', 'xcomp'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.653258Z","iopub.execute_input":"2025-11-30T21:25:13.653849Z","iopub.status.idle":"2025-11-30T21:25:13.671392Z","shell.execute_reply.started":"2025-11-30T21:25:13.653826Z","shell.execute_reply":"2025-11-30T21:25:13.670714Z"}},"outputs":[],"execution_count":114},{"id":"7245338f","cell_type":"markdown","source":"============================================================\nCheckpoint Functions\n============================================================","metadata":{}},{"id":"6ad80ede","cell_type":"code","source":"def get_checkpoint_path(checkpoint_dir: Path, lang: str, step: str, ext: str = 'csv') -> Path:\n    return checkpoint_dir / f\"checkpoint_{lang}_{step}.{ext}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.671990Z","iopub.execute_input":"2025-11-30T21:25:13.672149Z","iopub.status.idle":"2025-11-30T21:25:13.686481Z","shell.execute_reply.started":"2025-11-30T21:25:13.672136Z","shell.execute_reply":"2025-11-30T21:25:13.685866Z"}},"outputs":[],"execution_count":115},{"id":"2d20f12d","cell_type":"code","source":"def checkpoint_exists(checkpoint_dir: Path, lang: str, step: str, ext: str = 'csv') -> bool:\n    return get_checkpoint_path(checkpoint_dir, lang, step, ext).exists()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.687180Z","iopub.execute_input":"2025-11-30T21:25:13.687427Z","iopub.status.idle":"2025-11-30T21:25:13.702919Z","shell.execute_reply.started":"2025-11-30T21:25:13.687406Z","shell.execute_reply":"2025-11-30T21:25:13.702166Z"}},"outputs":[],"execution_count":116},{"id":"ff854875","cell_type":"code","source":"def save_checkpoint_csv(df: pd.DataFrame, checkpoint_dir: Path, lang: str, step: str) -> None:\n    path = get_checkpoint_path(checkpoint_dir, lang, step, 'csv')\n    df.to_csv(path, index=False)\n    print(f\"  [Checkpoint saved] {path.name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.703615Z","iopub.execute_input":"2025-11-30T21:25:13.703837Z","iopub.status.idle":"2025-11-30T21:25:13.716827Z","shell.execute_reply.started":"2025-11-30T21:25:13.703816Z","shell.execute_reply":"2025-11-30T21:25:13.716174Z"}},"outputs":[],"execution_count":117},{"id":"d2864d90","cell_type":"code","source":"def load_checkpoint_csv(checkpoint_dir: Path, lang: str, step: str) -> pd.DataFrame:\n    path = get_checkpoint_path(checkpoint_dir, lang, step, 'csv')\n    print(f\"  [Checkpoint loaded] {path.name}\")\n    return pd.read_csv(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.717469Z","iopub.execute_input":"2025-11-30T21:25:13.717656Z","iopub.status.idle":"2025-11-30T21:25:13.730459Z","shell.execute_reply.started":"2025-11-30T21:25:13.717641Z","shell.execute_reply":"2025-11-30T21:25:13.729730Z"}},"outputs":[],"execution_count":118},{"id":"71db39f7","cell_type":"code","source":"def save_checkpoint_npy(arr: np.ndarray, checkpoint_dir: Path, lang: str, step: str) -> None:\n    path = get_checkpoint_path(checkpoint_dir, lang, step, 'npy')\n    np.save(path, arr)\n    print(f\"  [Checkpoint saved] {path.name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.731228Z","iopub.execute_input":"2025-11-30T21:25:13.731470Z","iopub.status.idle":"2025-11-30T21:25:13.745841Z","shell.execute_reply.started":"2025-11-30T21:25:13.731454Z","shell.execute_reply":"2025-11-30T21:25:13.745053Z"}},"outputs":[],"execution_count":119},{"id":"d8ec1d0a","cell_type":"code","source":"def load_checkpoint_npy(checkpoint_dir: Path, lang: str, step: str) -> np.ndarray:\n    path = get_checkpoint_path(checkpoint_dir, lang, step, 'npy')\n    print(f\"  [Checkpoint loaded] {path.name}\")\n    return np.load(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.746538Z","iopub.execute_input":"2025-11-30T21:25:13.746806Z","iopub.status.idle":"2025-11-30T21:25:13.760165Z","shell.execute_reply.started":"2025-11-30T21:25:13.746783Z","shell.execute_reply":"2025-11-30T21:25:13.759458Z"}},"outputs":[],"execution_count":120},{"id":"c36d17d2","cell_type":"code","source":"def clear_checkpoints(checkpoint_dir: Path, lang: str) -> None:\n    for f in checkpoint_dir.glob(f\"checkpoint_{lang}_*\"):\n        f.unlink()\n        print(f\"  [Checkpoint deleted] {f.name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.760847Z","iopub.execute_input":"2025-11-30T21:25:13.761012Z","iopub.status.idle":"2025-11-30T21:25:13.774622Z","shell.execute_reply.started":"2025-11-30T21:25:13.760993Z","shell.execute_reply":"2025-11-30T21:25:13.774083Z"}},"outputs":[],"execution_count":121},{"id":"5e2499d5","cell_type":"code","source":"# Step 1: Load Data\ndef load_data(lang: str, data_dir: Path) -> pd.DataFrame:\n    filename = \"En-Dataset.csv\" if lang == 'en' else \"Fr-Dataset.csv\"\n    \n    print(f\"\\nStep 1: Loading data\")\n    \n    # Try multiple possible paths\n    possible_paths = [\n        data_dir / \"data\" / filename,\n        data_dir / filename,\n        data_dir / filename.lower(),\n        data_dir / \"data\" / filename.lower(),\n    ]\n    \n    input_file = None\n    for path in possible_paths:\n        if path.exists():\n            input_file = path\n            break\n    \n    if input_file is None:\n        print(f\"  Searching for {filename}...\")\n        found_files = list(data_dir.rglob(f\"*{filename}\")) + list(data_dir.rglob(f\"*{filename.lower()}\"))\n        if found_files:\n            input_file = found_files[0]\n        else:\n            raise FileNotFoundError(\n                f\"Dataset not found. Searched in:\\n\"\n                f\"  {data_dir}\\n\"\n                f\"  Tried: {[str(p) for p in possible_paths]}\"\n            )\n    \n    print(f\"  File: {input_file}\")\n    \n    df = pd.read_csv(input_file)\n    print(f\"  Loaded {len(df):,} rows\")\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.775360Z","iopub.execute_input":"2025-11-30T21:25:13.775578Z","iopub.status.idle":"2025-11-30T21:25:13.788977Z","shell.execute_reply.started":"2025-11-30T21:25:13.775558Z","shell.execute_reply":"2025-11-30T21:25:13.788280Z"}},"outputs":[],"execution_count":122},{"id":"e81a21f2","cell_type":"code","source":"# Step 2: Assign Unique Index\ndef assign_unique_index(df: pd.DataFrame) -> pd.DataFrame:\n    print(f\"\\nStep 2: Assigning unique Index\")\n    \n    df['source'] = df['ID'].apply(\n        lambda x: 'viki' if str(x).lower().startswith('viki') else 'wiki'\n    )\n    \n    viki_mask = df['source'] == 'viki'\n    wiki_mask = df['source'] == 'wiki'\n    \n    viki_indices = [f\"viki-{i:06d}\" for i in range(1, viki_mask.sum() + 1)]\n    wiki_indices = [f\"wiki-{i:06d}\" for i in range(1, wiki_mask.sum() + 1)]\n    \n    df.loc[viki_mask, 'Index'] = viki_indices\n    df.loc[wiki_mask, 'Index'] = wiki_indices\n    \n    print(f\"  Vikidia sentences: {viki_mask.sum():,}\")\n    print(f\"  Wikipedia sentences: {wiki_mask.sum():,}\")\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.789691Z","iopub.execute_input":"2025-11-30T21:25:13.789873Z","iopub.status.idle":"2025-11-30T21:25:13.808488Z","shell.execute_reply.started":"2025-11-30T21:25:13.789859Z","shell.execute_reply":"2025-11-30T21:25:13.807816Z"}},"outputs":[],"execution_count":123},{"id":"4d49ae5c","cell_type":"code","source":"# Step 3: Basic Cleaning\ndef basic_cleaning(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n    print(f\"\\nStep 3: Basic cleaning\")\n    \n    stats = {'original_rows': len(df)}\n    \n    critical_cols = ['Index', 'Sentence', 'Label']\n    nan_before = len(df)\n    df = df.dropna(subset=critical_cols)\n    stats['nan_removed'] = nan_before - len(df)\n    print(f\"  NaN removed: {stats['nan_removed']:,}\")\n    \n    blank_before = len(df)\n    df = df[df['Sentence'].str.strip().str.len() > 0]\n    stats['blank_removed'] = blank_before - len(df)\n    print(f\"  Blank removed: {stats['blank_removed']:,}\")\n    \n    return df, stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.809237Z","iopub.execute_input":"2025-11-30T21:25:13.809633Z","iopub.status.idle":"2025-11-30T21:25:13.822821Z","shell.execute_reply.started":"2025-11-30T21:25:13.809608Z","shell.execute_reply":"2025-11-30T21:25:13.822334Z"}},"outputs":[],"execution_count":124},{"id":"720316d4","cell_type":"code","source":"# Step 4: Duplicate Detection and Removal\ndef remove_duplicates(df: pd.DataFrame, stats: Dict) -> Tuple[pd.DataFrame, Dict]:\n    print(f\"\\nStep 4: Duplicate detection and removal\")\n    \n    viki_df = df[df['source'] == 'viki'].copy()\n    wiki_df = df[df['source'] == 'wiki'].copy()\n    \n    print(f\"  Before cleaning:\")\n    print(f\"    Vikidia: {len(viki_df):,}\")\n    print(f\"    Wikipedia: {len(wiki_df):,}\")\n    \n    viki_before = len(viki_df)\n    viki_df = viki_df.drop_duplicates(subset=['Sentence'], keep='first')\n    stats['viki_internal_dup'] = viki_before - len(viki_df)\n    print(f\"  4a. Vikidia internal duplicates: {stats['viki_internal_dup']:,}\")\n    \n    wiki_before = len(wiki_df)\n    wiki_df = wiki_df.drop_duplicates(subset=['Sentence'], keep='first')\n    stats['wiki_internal_dup'] = wiki_before - len(wiki_df)\n    print(f\"  4b. Wikipedia internal duplicates: {stats['wiki_internal_dup']:,}\")\n    \n    wiki_sentences = set(wiki_df['Sentence'].values)\n    leakage_mask = viki_df['Sentence'].isin(wiki_sentences)\n    stats['leakage_removed'] = leakage_mask.sum()\n    viki_df = viki_df[~leakage_mask]\n    print(f\"  4c. Leakage (Viki ∩ Wiki): {stats['leakage_removed']:,}\")\n    \n    df = pd.concat([viki_df, wiki_df], ignore_index=True)\n    df = df.sort_values('Index').reset_index(drop=True)\n    \n    print(f\"  After cleaning: {len(df):,}\")\n    \n    stats['after_dedup'] = len(df)\n    stats['viki_final'] = len(viki_df)\n    stats['wiki_final'] = len(wiki_df)\n    \n    return df, stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.823465Z","iopub.execute_input":"2025-11-30T21:25:13.823629Z","iopub.status.idle":"2025-11-30T21:25:13.842091Z","shell.execute_reply.started":"2025-11-30T21:25:13.823615Z","shell.execute_reply":"2025-11-30T21:25:13.841442Z"}},"outputs":[],"execution_count":125},{"id":"65d85d7f","cell_type":"code","source":"# Step 5: Save Cleaned Dataset\ndef save_cleaned_dataset(df: pd.DataFrame, lang: str, output_dir: Path) -> Path:\n    print(f\"\\nStep 5: Saving cleaned dataset\")\n    \n    cols_to_save = ['Index', 'ID', 'Name', 'Sentence', 'Label', 'LengthWords', 'LengthChars', 'source']\n    df_to_save = df[cols_to_save]\n    \n    output_file = output_dir / f\"{lang.capitalize()}-Dataset_cleaned.csv\"\n    \n    df_to_save.to_csv(output_file, index=False)\n    print(f\"  Saved: {output_file}\")\n    print(f\"  Rows: {len(df_to_save):,}\")\n    \n    return output_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.842802Z","iopub.execute_input":"2025-11-30T21:25:13.843096Z","iopub.status.idle":"2025-11-30T21:25:13.859312Z","shell.execute_reply.started":"2025-11-30T21:25:13.843073Z","shell.execute_reply":"2025-11-30T21:25:13.858664Z"}},"outputs":[],"execution_count":126},{"id":"40126368","cell_type":"code","source":"# Step 6: Feature Extraction\ndef extract_basic_features_single(sent: str) -> List[float]:\n    words = sent.split()\n    chars = list(sent)\n    \n    if len(words) == 0:\n        return [0.0] * 8\n    \n    avg_word_len = np.mean([len(w) for w in words])\n    long_word_ratio = sum(1 for w in words if len(w) > 6) / len(words)\n    \n    words_norm = words[:50] if len(words) > 50 else words\n    ttr = len(set(w.lower() for w in words_norm)) / len(words_norm)\n    \n    punct_count = sum(1 for c in chars if c in '.,;:!?()[]{}\"-')\n    punct_density = punct_count / len(words)\n    \n    comma_density = sent.count(',') / len(words)\n    digit_ratio = sum(1 for c in chars if c.isdigit()) / len(chars) if len(chars) > 0 else 0\n    upper_ratio = sum(1 for c in chars if c.isupper()) / len(chars) if len(chars) > 0 else 0\n    has_parens = 1.0 if '(' in sent else 0.0\n    \n    return [\n        avg_word_len, long_word_ratio, ttr, punct_density,\n        comma_density, digit_ratio, upper_ratio, has_parens\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.860039Z","iopub.execute_input":"2025-11-30T21:25:13.860332Z","iopub.status.idle":"2025-11-30T21:25:13.877567Z","shell.execute_reply.started":"2025-11-30T21:25:13.860310Z","shell.execute_reply":"2025-11-30T21:25:13.876942Z"}},"outputs":[],"execution_count":127},{"id":"c305895f","cell_type":"code","source":"def extract_basic_features(sentences: List[str], n_jobs: int = 4) -> pd.DataFrame:\n    print(f\"    Using {n_jobs} CPU cores for basic features (joblib)...\")\n    \n    # Use joblib for parallel processing\n    features = Parallel(n_jobs=n_jobs, backend='loky')(\n        delayed(extract_basic_features_single)(sent) \n        for sent in tqdm(sentences, desc=\"Basic features\", unit=\"sent\")\n    )\n    \n    return pd.DataFrame(features, columns=BASIC_FEATURE_NAMES, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.880592Z","iopub.execute_input":"2025-11-30T21:25:13.880814Z","iopub.status.idle":"2025-11-30T21:25:13.891512Z","shell.execute_reply.started":"2025-11-30T21:25:13.880800Z","shell.execute_reply":"2025-11-30T21:25:13.890816Z"}},"outputs":[],"execution_count":128},{"id":"47c89e34","cell_type":"code","source":"def get_dependency_depth(token) -> int:\n    depth = 0\n    current = token\n    while current.head != current:\n        depth += 1\n        current = current.head\n    return depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.892356Z","iopub.execute_input":"2025-11-30T21:25:13.892700Z","iopub.status.idle":"2025-11-30T21:25:13.906439Z","shell.execute_reply.started":"2025-11-30T21:25:13.892672Z","shell.execute_reply":"2025-11-30T21:25:13.905706Z"}},"outputs":[],"execution_count":129},{"id":"bfc32489","cell_type":"code","source":"def extract_spacy_features_single(doc) -> List[float]:\n    tokens = [token for token in doc if not token.is_space]\n    n_tokens = len(tokens)\n    \n    if n_tokens == 0:\n        return [0.0] * 9\n    \n    depths = [get_dependency_depth(token) for token in tokens]\n    max_depth = max(depths) if depths else 0\n    avg_depth = np.mean(depths) if depths else 0.0\n    \n    dep_distances = [abs(token.i - token.head.i) for token in tokens]\n    avg_dependency_distance = np.mean(dep_distances) if dep_distances else 0.0\n    \n    func_word_count = sum(1 for token in tokens if token.pos_ in FUNCTION_WORD_POS)\n    func_word_ratio = func_word_count / n_tokens\n    \n    n_clauses = sum(1 for token in tokens if token.dep_ in CLAUSE_DEPS)\n    clause_ratio = n_clauses / n_tokens\n    \n    noun_count = sum(1 for token in tokens if token.pos_ in {'NOUN', 'PROPN'})\n    verb_count = sum(1 for token in tokens if token.pos_ == 'VERB')\n    noun_ratio = noun_count / n_tokens\n    verb_ratio = verb_count / n_tokens\n    \n    return [\n        float(n_tokens), float(max_depth), float(avg_depth),\n        float(avg_dependency_distance), float(func_word_ratio),\n        float(n_clauses), float(clause_ratio),\n        float(noun_ratio), float(verb_ratio)\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.907352Z","iopub.execute_input":"2025-11-30T21:25:13.907565Z","iopub.status.idle":"2025-11-30T21:25:13.925230Z","shell.execute_reply.started":"2025-11-30T21:25:13.907543Z","shell.execute_reply":"2025-11-30T21:25:13.924642Z"}},"outputs":[],"execution_count":130},{"id":"014a9e89","cell_type":"code","source":"def load_spacy_model(lang: str):\n    import spacy\n    \n    model_name = 'en_core_web_sm' if lang == 'en' else 'fr_core_news_sm'\n    print(f\"  Loading spaCy model: {model_name}\")\n    \n    try:\n        nlp = spacy.load(model_name, disable=['ner', 'textcat'])\n    except OSError:\n        print(f\"  Downloading {model_name}...\")\n        os.system(f\"python -m spacy download {model_name}\")\n        nlp = spacy.load(model_name, disable=['ner', 'textcat'])\n    \n    nlp.max_length = 100000\n    \n    return nlp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.925958Z","iopub.execute_input":"2025-11-30T21:25:13.926187Z","iopub.status.idle":"2025-11-30T21:25:13.940520Z","shell.execute_reply.started":"2025-11-30T21:25:13.926172Z","shell.execute_reply":"2025-11-30T21:25:13.939674Z"}},"outputs":[],"execution_count":131},{"id":"eb058235","cell_type":"code","source":"def extract_spacy_features(sentences: List[str], nlp) -> pd.DataFrame:\n    print(f\"    Processing with single-thread (Notebook compatibility mode)...\")\n    \n    all_features = []\n    \n    for doc in tqdm(\n        nlp.pipe(sentences, batch_size=1000), \n        total=len(sentences), \n        desc=\"spaCy features\", \n        unit=\"sent\"\n    ):\n        features = extract_spacy_features_single(doc)\n        all_features.append(features)\n        del doc\n    \n    return pd.DataFrame(all_features, columns=SPACY_FEATURE_NAMES, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.941364Z","iopub.execute_input":"2025-11-30T21:25:13.941603Z","iopub.status.idle":"2025-11-30T21:25:13.956481Z","shell.execute_reply.started":"2025-11-30T21:25:13.941581Z","shell.execute_reply":"2025-11-30T21:25:13.955837Z"}},"outputs":[],"execution_count":132},{"id":"79e6f0e0","cell_type":"code","source":"# Step 7: Sentence-BERT Embeddings + Cosine Similarity (GPU)\ndef compute_sbert_embeddings(\n    sentences: List[str],\n    lang: str,\n    hw_config: Dict,\n    batch_size: int = SBERT_BATCH_SIZE\n) -> np.ndarray:\n    from sentence_transformers import SentenceTransformer\n    \n    print(\"\\n  Computing Sentence-BERT embeddings...\")\n    print(f\"    Device: {hw_config['device']}\")\n    print(f\"    GPUs available: {hw_config['n_gpus']} (using single GPU)\")\n    \n    if lang == 'en':\n        model_name = 'all-MiniLM-L6-v2'\n    else:\n        model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n    \n    print(f\"    Model: {model_name}\")\n    \n    # Load model on primary GPU\n    model = SentenceTransformer(model_name, device=hw_config['device'])\n    \n    if hw_config['n_gpus'] > 0 and hw_config['device'] == 'cuda':\n        # Use larger batch size for faster processing on T4\n        effective_batch_size = batch_size * 2  # 256 for T4\n        print(f\"    Using enlarged batch size for T4: {effective_batch_size}\")\n    else:\n        effective_batch_size = batch_size\n    \n    print(f\"    Total sentences: {len(sentences):,}\")\n    print(f\"    Batch size: {effective_batch_size}\")\n    \n    start_time = time.time()\n    embeddings = model.encode(\n        sentences,\n        batch_size=effective_batch_size,\n        show_progress_bar=True,\n        convert_to_numpy=True\n    )\n    \n    encode_time = time.time() - start_time\n    print(f\"    Encoding time: {encode_time:.1f}s ({len(sentences)/encode_time:.0f} sent/s)\")\n    \n    del model\n    if hw_config['device'] == 'cuda':\n        torch.cuda.empty_cache()\n    gc.collect()\n    \n    return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.957081Z","iopub.execute_input":"2025-11-30T21:25:13.957280Z","iopub.status.idle":"2025-11-30T21:25:13.974550Z","shell.execute_reply.started":"2025-11-30T21:25:13.957265Z","shell.execute_reply":"2025-11-30T21:25:13.973872Z"}},"outputs":[],"execution_count":133},{"id":"e96e5145","cell_type":"code","source":"def compute_cosine_similarity_from_embeddings(\n    embeddings: np.ndarray,\n    df: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Compute cosine similarity from pre-computed embeddings.\"\"\"\n    from sklearn.metrics.pairwise import cosine_similarity\n    \n    print(\"\\n  Computing cosine similarity from embeddings...\")\n    \n    viki_mask = df['source'] == 'viki'\n    viki_indices = df[viki_mask].index.tolist()\n    viki_embeddings = embeddings[viki_indices]\n    \n    print(f\"    Vikidia (simple prototype): {len(viki_indices):,}\")\n    \n    simple_centroid = viki_embeddings.mean(axis=0, keepdims=True)\n    \n    similarities = cosine_similarity(embeddings, simple_centroid).flatten()\n    \n    print(f\"    Similarity range: [{similarities.min():.4f}, {similarities.max():.4f}]\")\n    print(f\"    Similarity mean: {similarities.mean():.4f}\")\n    \n    return similarities.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.975318Z","iopub.execute_input":"2025-11-30T21:25:13.975512Z","iopub.status.idle":"2025-11-30T21:25:13.993657Z","shell.execute_reply.started":"2025-11-30T21:25:13.975497Z","shell.execute_reply":"2025-11-30T21:25:13.992952Z"}},"outputs":[],"execution_count":134},{"id":"7b232321","cell_type":"code","source":"# Main Feature Extraction with Checkpoints\ndef extract_all_features_with_checkpoints(\n    df: pd.DataFrame,\n    lang: str,\n    hw_config: Dict,\n    checkpoint_dir: Path\n) -> pd.DataFrame:\n    \"\"\"Extract all features with checkpoint support.\"\"\"\n    print(f\"\\nStep 6-7: Extracting features for ENTIRE dataset\")\n    \n    sentences = df['Sentence'].tolist()\n    n_sentences = len(sentences)\n    print(f\"  Total sentences: {n_sentences:,}\")\n    \n    # Determine number of CPU cores to use (for basic features only)\n    n_jobs = min(hw_config['n_cpus'], 4)\n    print(f\"  CPU cores for basic features: {n_jobs}\")\n    print(f\"  spaCy mode: single-thread (Notebook compatibility)\")\n    \n    # Basic Features (CPU Parallel via joblib)\n    if checkpoint_exists(checkpoint_dir, lang, 'basic'):\n        print(\"\\n  [CHECKPOINT] Loading basic features...\")\n        basic_df = load_checkpoint_csv(checkpoint_dir, lang, 'basic')\n    else:\n        print(\"\\n  Extracting basic features (CPU parallel)...\")\n        basic_df = extract_basic_features(sentences, n_jobs=n_jobs)\n        save_checkpoint_csv(basic_df, checkpoint_dir, lang, 'basic')\n    \n    # spaCy Features (Single-thread for Notebook compatibility)\n    if checkpoint_exists(checkpoint_dir, lang, 'spacy'):\n        print(\"\\n  [CHECKPOINT] Loading spaCy features...\")\n        spacy_df = load_checkpoint_csv(checkpoint_dir, lang, 'spacy')\n    else:\n        print(\"\\n  Extracting spaCy features (single-thread)...\")\n        nlp = load_spacy_model(lang)\n        spacy_df = extract_spacy_features(sentences, nlp)\n        del nlp\n        gc.collect()\n        save_checkpoint_csv(spacy_df, checkpoint_dir, lang, 'spacy')\n    \n    # Sentence-BERT Embeddings (Single GPU)\n    if checkpoint_exists(checkpoint_dir, lang, 'sbert', 'npy'):\n        print(\"\\n  [CHECKPOINT] Loading SBERT embeddings...\")\n        embeddings = load_checkpoint_npy(checkpoint_dir, lang, 'sbert')\n    else:\n        print(\"\\n  Computing SBERT embeddings (GPU accelerated)...\")\n        embeddings = compute_sbert_embeddings(sentences, lang, hw_config)\n        save_checkpoint_npy(embeddings, checkpoint_dir, lang, 'sbert')\n    \n    # Cosine Similarity\n    cos_simi = compute_cosine_similarity_from_embeddings(embeddings, df)\n    \n    del embeddings\n    gc.collect()\n    if hw_config['device'] == 'cuda':\n        torch.cuda.empty_cache()\n    \n    # Additional Features\n    print(\"\\n  Computing additional features...\")\n    words_chars_ratio = (df['LengthWords'] / df['LengthChars'].replace(0, 1)).astype(np.float32)\n    \n    # Build Final DataFrame\n    print(\"\\n  Building final feature DataFrame...\")\n    \n    result_df = pd.DataFrame({\n        'Index': df['Index'].values,\n        'ID': df['ID'].values,\n        'Name': df['Name'].values,\n        'Sentence': df['Sentence'].values,\n        'Label': df['Label'].values,\n        'LengthWords': df['LengthWords'].values,\n        'LengthChars': df['LengthChars'].values,\n        'words_chars_ratio': words_chars_ratio.values,\n        'cos_simi': cos_simi\n    })\n    \n    for col in BASIC_FEATURE_NAMES:\n        result_df[col] = basic_df[col].values\n    \n    for col in SPACY_FEATURE_NAMES:\n        result_df[col] = spacy_df[col].values\n    \n    print(f\"  Final shape: {result_df.shape}\")\n    \n    del basic_df, spacy_df\n    gc.collect()\n    \n    return result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:13.994385Z","iopub.execute_input":"2025-11-30T21:25:13.994633Z","iopub.status.idle":"2025-11-30T21:25:14.014769Z","shell.execute_reply.started":"2025-11-30T21:25:13.994609Z","shell.execute_reply":"2025-11-30T21:25:14.014029Z"}},"outputs":[],"execution_count":135},{"id":"cadba8ce","cell_type":"code","source":"# Step 8: Save Final Features\ndef save_features(df: pd.DataFrame, lang: str, output_dir: Path) -> Path:\n    print(f\"\\nStep 8: Saving final features\")\n    \n    output_file = output_dir / f\"{lang}_full_features.csv\"\n    \n    df.to_csv(output_file, index=False)\n    print(f\"  Saved: {output_file}\")\n    print(f\"  Rows: {len(df):,}\")\n    print(f\"  Columns: {len(df.columns)}\")\n    \n    return output_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:14.015522Z","iopub.execute_input":"2025-11-30T21:25:14.015744Z","iopub.status.idle":"2025-11-30T21:25:14.032161Z","shell.execute_reply.started":"2025-11-30T21:25:14.015711Z","shell.execute_reply":"2025-11-30T21:25:14.031623Z"}},"outputs":[],"execution_count":136},{"id":"3038fee2","cell_type":"code","source":"# Print Reports\ndef print_cleaning_report(lang: str, stats: Dict) -> None:\n    print(\"\\n\")\n    print(\"=\" * 70)\n    print(f\"CLEANING REPORT - {lang.upper()}\")\n    print(\"=\" * 70)\n    \n    print(f\"\\nOriginal rows: {stats['original_rows']:,}\")\n    \n    print(f\"\\nBasic cleaning:\")\n    print(f\"  NaN removed: {stats['nan_removed']:,}\")\n    print(f\"  Blank removed: {stats['blank_removed']:,}\")\n    \n    print(f\"\\nDuplicates removed:\")\n    print(f\"  Vikidia internal: {stats['viki_internal_dup']:,}\")\n    print(f\"  Wikipedia internal: {stats['wiki_internal_dup']:,}\")\n    print(f\"  Leakage (Viki ∩ Wiki): {stats['leakage_removed']:,}\")\n    \n    print(f\"\\nAfter cleaning:\")\n    print(f\"  Vikidia: {stats['viki_final']:,}\")\n    print(f\"  Wikipedia: {stats['wiki_final']:,}\")\n    print(f\"  Total: {stats['after_dedup']:,}\")\n    \n    print(\"=\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:14.032856Z","iopub.execute_input":"2025-11-30T21:25:14.033045Z","iopub.status.idle":"2025-11-30T21:25:14.046624Z","shell.execute_reply.started":"2025-11-30T21:25:14.033023Z","shell.execute_reply":"2025-11-30T21:25:14.046042Z"}},"outputs":[],"execution_count":137},{"id":"61864435","cell_type":"code","source":"def print_feature_summary(df: pd.DataFrame, lang: str) -> None:\n    print(\"\\n\")\n    print(\"=\" * 70)\n    print(f\"FEATURE SUMMARY - {lang.upper()}\")\n    print(\"=\" * 70)\n    \n    print(f\"\\nDataset shape: {df.shape}\")\n    print(f\"Columns: {list(df.columns)}\")\n    \n    numeric_cols = ['LengthWords', 'LengthChars', 'words_chars_ratio', 'cos_simi'] + \\\n                   BASIC_FEATURE_NAMES + SPACY_FEATURE_NAMES\n    \n    print(f\"\\nFeature statistics:\")\n    print(df[numeric_cols].describe().round(3).to_string())\n    \n    print(\"=\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:14.047232Z","iopub.execute_input":"2025-11-30T21:25:14.047877Z","iopub.status.idle":"2025-11-30T21:25:14.061568Z","shell.execute_reply.started":"2025-11-30T21:25:14.047851Z","shell.execute_reply":"2025-11-30T21:25:14.060808Z"}},"outputs":[],"execution_count":138},{"id":"e4290ae9","cell_type":"code","source":"# Main Pipeline\ndef process_language(\n    lang: str, \n    data_dir: Path, \n    output_dir: Path, \n    checkpoint_dir: Path,\n    hw_config: Dict\n) -> None:\n    print(\"\\n\" + \"=\" * 70)\n    print(f\"PROCESSING {lang.upper()} DATASET\")\n    print(\"=\" * 70)\n    \n    start_time = time.time()\n    \n    # Check if we have cleaned data checkpoint\n    if checkpoint_exists(checkpoint_dir, lang, 'cleaned'):\n        print(\"\\n[CHECKPOINT] Loading cleaned dataset...\")\n        df = load_checkpoint_csv(checkpoint_dir, lang, 'cleaned')\n        # Reconstruct stats (approximate)\n        stats = {\n            'original_rows': len(df),\n            'nan_removed': 0,\n            'blank_removed': 0,\n            'viki_internal_dup': 0,\n            'wiki_internal_dup': 0,\n            'leakage_removed': 0,\n            'after_dedup': len(df),\n            'viki_final': len(df[df['source'] == 'viki']),\n            'wiki_final': len(df[df['source'] == 'wiki'])\n        }\n        print(f\"  Loaded {len(df):,} rows from checkpoint\")\n    else:\n        # Step 1: Load data\n        df = load_data(lang, data_dir)\n        \n        # Step 2: Assign unique Index\n        df = assign_unique_index(df)\n        \n        # Step 3: Basic cleaning\n        df, stats = basic_cleaning(df)\n        \n        # Step 4: Remove duplicates\n        df, stats = remove_duplicates(df, stats)\n        \n        # Save cleaned checkpoint\n        save_checkpoint_csv(df, checkpoint_dir, lang, 'cleaned')\n        \n        # Step 5: Save cleaned dataset\n        save_cleaned_dataset(df, lang, output_dir)\n    \n    # Step 6-7: Extract all features\n    result_df = extract_all_features_with_checkpoints(df, lang, hw_config, checkpoint_dir)\n    \n    # Step 8: Save final features\n    save_features(result_df, lang, output_dir)\n    \n    # Print reports\n    print_cleaning_report(lang, stats)\n    print_feature_summary(result_df, lang)\n    \n    total_time = time.time() - start_time\n    print(f\"\\nTotal processing time: {total_time/60:.1f} minutes\")\n    \n    # Clear checkpoints after successful completion\n    print(\"\\nClearing checkpoints...\")\n    clear_checkpoints(checkpoint_dir, lang)\n    \n    # Cleanup\n    del df, result_df\n    gc.collect()\n    if hw_config['device'] == 'cuda':\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:14.062178Z","iopub.execute_input":"2025-11-30T21:25:14.062373Z","iopub.status.idle":"2025-11-30T21:25:14.077454Z","shell.execute_reply.started":"2025-11-30T21:25:14.062359Z","shell.execute_reply":"2025-11-30T21:25:14.076762Z"}},"outputs":[],"execution_count":139},{"id":"f382c0f0","cell_type":"code","source":"def main():\n    \"\"\"Main entry point.\"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"iDEM FEATURE EXTRACTION PIPELINE\")\n    print(\"Kaggle Notebook Version (T4 GPU + Single-thread spaCy)\")\n    print(\"** WITH CHECKPOINT SUPPORT **\")\n    print(\"=\" * 70)\n    \n    # Check environment and get hardware config\n    hw_config, is_kaggle = check_environment()\n    \n    # Print detailed hardware info\n    print_hardware_info(hw_config)\n    \n    # Setup paths\n    data_dir, output_dir, checkpoint_dir = setup_paths(is_kaggle)\n    \n    # Check for existing checkpoints\n    existing_checkpoints = list(checkpoint_dir.glob(\"checkpoint_*\"))\n    if existing_checkpoints:\n        print(f\"\\nFound {len(existing_checkpoints)} existing checkpoints:\")\n        for cp in existing_checkpoints:\n            print(f\"  {cp.name}\")\n        print(\"Will resume from checkpoints where possible.\\n\")\n    \n    # Process both languages\n    for lang in ['en', 'fr']:\n        try:\n            process_language(lang, data_dir, output_dir, checkpoint_dir, hw_config)\n        except FileNotFoundError as e:\n            print(f\"\\nSkipping {lang}: {e}\")\n        \n        gc.collect()\n        if hw_config['device'] == 'cuda':\n            torch.cuda.empty_cache()\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ALL PROCESSING COMPLETE!\")\n    print(\"=\" * 70)\n    \n    print(\"\\nOutput files:\")\n    for f in sorted(output_dir.glob(\"*.csv\")):\n        size_mb = f.stat().st_size / 1e6\n        print(f\"  {f.name} ({size_mb:.1f} MB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:14.078220Z","iopub.execute_input":"2025-11-30T21:25:14.078479Z","iopub.status.idle":"2025-11-30T21:25:14.094195Z","shell.execute_reply.started":"2025-11-30T21:25:14.078459Z","shell.execute_reply":"2025-11-30T21:25:14.093615Z"}},"outputs":[],"execution_count":140},{"id":"b3b1c417","cell_type":"code","source":"# Kaggle Notebook Cell Execution\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:25:14.094772Z","iopub.execute_input":"2025-11-30T21:25:14.095029Z","iopub.status.idle":"2025-11-30T23:20:20.668866Z","shell.execute_reply.started":"2025-11-30T21:25:14.095008Z","shell.execute_reply":"2025-11-30T23:20:20.668094Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\niDEM FEATURE EXTRACTION PIPELINE\nKaggle Notebook Version (T4 GPU + Single-thread spaCy)\n** WITH CHECKPOINT SUPPORT **\n======================================================================\n======================================================================\nENVIRONMENT CHECK\n======================================================================\n\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nKaggle Environment: True\nTOKENIZERS_PARALLELISM: false\n======================================================================\n======================================================================\nHARDWARE CONFIGURATION\n======================================================================\n\nCPU:\n  Available cores: 4\n  Using for basic features: 4 (joblib)\n  Using for spaCy: 1 (single-thread for Notebook compatibility)\n\nGPU:\n  Available GPUs: 2\n    GPU 0: Tesla T4 (15.8 GB)\n    GPU 1: Tesla T4 (15.8 GB)\n  Using: Single GPU (multi-GPU disabled for Notebook compatibility)\n  Primary device: cuda\n======================================================================\nData directory: /kaggle/input/dataset\nOutput directory: /kaggle/working/output\nCheckpoint directory: /kaggle/working/checkpoints\n\n======================================================================\nPROCESSING EN DATASET\n======================================================================\n\nStep 1: Loading data\n  File: /kaggle/input/dataset/En-Dataset.csv\n  Loaded 290,708 rows\n\nStep 2: Assigning unique Index\n  Vikidia sentences: 17,970\n  Wikipedia sentences: 272,738\n\nStep 3: Basic cleaning\n  NaN removed: 0\n  Blank removed: 0\n\nStep 4: Duplicate detection and removal\n  Before cleaning:\n    Vikidia: 17,970\n    Wikipedia: 272,738\n  4a. Vikidia internal duplicates: 553\n  4b. Wikipedia internal duplicates: 262\n  4c. Leakage (Viki ∩ Wiki): 112\n  After cleaning: 289,781\n  [Checkpoint saved] checkpoint_en_cleaned.csv\n\nStep 5: Saving cleaned dataset\n  Saved: /kaggle/working/output/En-Dataset_cleaned.csv\n  Rows: 289,781\n\nStep 6-7: Extracting features for ENTIRE dataset\n  Total sentences: 289,781\n  CPU cores for basic features: 4\n  spaCy mode: single-thread (Notebook compatibility)\n\n  Extracting basic features (CPU parallel)...\n    Using 4 CPU cores for basic features (joblib)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Basic features:   0%|          | 0/289781 [00:00<?, ?sent/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e7b481ff5e4b5c94ddf00339fea63c"}},"metadata":{}},{"name":"stdout","text":"  [Checkpoint saved] checkpoint_en_basic.csv\n\n  Extracting spaCy features (single-thread)...\n  Loading spaCy model: en_core_web_sm\n    Processing with single-thread (Notebook compatibility mode)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spaCy features:   0%|          | 0/289781 [00:00<?, ?sent/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943a0110ceb949dfa38dbf69bf3c9eb0"}},"metadata":{}},{"name":"stdout","text":"  [Checkpoint saved] checkpoint_en_spacy.csv\n\n  Computing SBERT embeddings (GPU accelerated)...\n","output_type":"stream"},{"name":"stderr","text":"2025-11-30 21:37:06.179563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764538626.398433      92 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764538626.459321      92 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"\n  Computing Sentence-BERT embeddings...\n    Device: cuda\n    GPUs available: 2 (using single GPU)\n    Model: all-MiniLM-L6-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b0469a354b4166b0870b58d609316e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41cd2f1106b34e83b919f858811316da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef924c35d8a48ff9374c8465e7a5dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81493743bc5a4d98a3c9aa1d8bef75a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbf45a8fb1c940b8b8e1fbdb4ff991a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ceb99089414d8a9257ce3e1dec631c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046986d6aaa240678fd50c78a762989d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa17c66bb1834169a18e9b3d6c71afaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f33dd82923a4ae6803308f588395928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4747e451279494382e1e4ac193d1ce9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9c116027be47fb8eb0279370bb2f8a"}},"metadata":{}},{"name":"stdout","text":"    Using enlarged batch size for T4: 256\n    Total sentences: 289,781\n    Batch size: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1132 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87021d8d301649b6b5883c44af7ca8e6"}},"metadata":{}},{"name":"stdout","text":"    Encoding time: 154.0s (1882 sent/s)\n  [Checkpoint saved] checkpoint_en_sbert.npy\n\n  Computing cosine similarity from embeddings...\n    Vikidia (simple prototype): 17,305\n    Similarity range: [-0.1852, 0.6206]\n    Similarity mean: 0.1631\n\n  Computing additional features...\n\n  Building final feature DataFrame...\n  Final shape: (289781, 26)\n\nStep 8: Saving final features\n  Saved: /kaggle/working/output/en_full_features.csv\n  Rows: 289,781\n  Columns: 26\n\n\n======================================================================\nCLEANING REPORT - EN\n======================================================================\n\nOriginal rows: 290,708\n\nBasic cleaning:\n  NaN removed: 0\n  Blank removed: 0\n\nDuplicates removed:\n  Vikidia internal: 553\n  Wikipedia internal: 262\n  Leakage (Viki ∩ Wiki): 112\n\nAfter cleaning:\n  Vikidia: 17,305\n  Wikipedia: 272,476\n  Total: 289,781\n======================================================================\n\n\n======================================================================\nFEATURE SUMMARY - EN\n======================================================================\n\nDataset shape: (289781, 26)\nColumns: ['Index', 'ID', 'Name', 'Sentence', 'Label', 'LengthWords', 'LengthChars', 'words_chars_ratio', 'cos_simi', 'avg_word_len', 'long_word_ratio', 'ttr', 'punct_density', 'comma_density', 'digit_ratio', 'upper_ratio', 'has_parens', 'n_tokens', 'max_depth', 'avg_depth', 'avg_dependency_distance', 'func_word_ratio', 'n_clauses', 'clause_ratio', 'noun_ratio', 'verb_ratio']\n\nFeature statistics:\n       LengthWords  LengthChars  words_chars_ratio    cos_simi  avg_word_len  long_word_ratio         ttr  punct_density  comma_density  digit_ratio  upper_ratio  has_parens    n_tokens   max_depth   avg_depth  avg_dependency_distance  func_word_ratio   n_clauses  clause_ratio  noun_ratio  verb_ratio\ncount   289781.000   289781.000         289781.000  289781.000    289781.000       289781.000  289781.000     289781.000     289781.000   289781.000   289781.000  289781.000  289781.000  289781.000  289781.000               289781.000       289781.000  289781.000    289781.000  289781.000  289781.000\nmean        23.752      148.501              0.162       0.163         5.273            0.314       0.899          0.162          0.064        0.015        0.035       0.153      27.821       6.280       2.930                    3.104            0.345       1.146         0.039       0.309       0.087\nstd         13.027       87.279              0.019       0.097         0.736            0.116       0.082          0.110          0.062        0.028        0.026       0.360      15.249       2.369       1.016                    0.942            0.095       1.263         0.038       0.091       0.050\nmin          8.000       33.000              0.052      -0.185         1.000            0.000       0.040          0.000          0.000        0.000        0.000       0.000       9.000       1.000       0.667                    1.100            0.000       0.000         0.000       0.000       0.000\n25%         16.000       96.000              0.150       0.093         4.789            0.235       0.846          0.091          0.000        0.000        0.014       0.000      18.000       5.000       2.241                    2.533            0.286       0.000         0.000       0.250       0.054\n50%         21.000      131.000              0.162       0.159         5.211            0.308       0.909          0.136          0.056        0.000        0.029       0.000      25.000       6.000       2.741                    2.957            0.350       1.000         0.037       0.304       0.083\n75%         29.000      180.000              0.174       0.229         5.688            0.387       0.957          0.200          0.091        0.022        0.048       0.000      34.000       7.000       3.389                    3.500            0.409       2.000         0.062       0.364       0.118\nmax       2500.000    17355.000              0.400       0.621        18.000            1.000       1.000          2.818          1.150        0.706        0.807       1.000    2500.000     181.000      77.017                  166.478            1.000      35.000         0.333       0.964       0.417\n======================================================================\n\nTotal processing time: 15.0 minutes\n\nClearing checkpoints...\n  [Checkpoint deleted] checkpoint_en_sbert.npy\n  [Checkpoint deleted] checkpoint_en_spacy.csv\n  [Checkpoint deleted] checkpoint_en_cleaned.csv\n  [Checkpoint deleted] checkpoint_en_basic.csv\n\n======================================================================\nPROCESSING FR DATASET\n======================================================================\n\nStep 1: Loading data\n  File: /kaggle/input/dataset/Fr-Dataset.csv\n  Loaded 1,699,063 rows\n\nStep 2: Assigning unique Index\n  Vikidia sentences: 228,359\n  Wikipedia sentences: 1,470,704\n\nStep 3: Basic cleaning\n  NaN removed: 0\n  Blank removed: 0\n\nStep 4: Duplicate detection and removal\n  Before cleaning:\n    Vikidia: 228,359\n    Wikipedia: 1,470,704\n  4a. Vikidia internal duplicates: 38,195\n  4b. Wikipedia internal duplicates: 6,385\n  4c. Leakage (Viki ∩ Wiki): 1,308\n  After cleaning: 1,653,175\n  [Checkpoint saved] checkpoint_fr_cleaned.csv\n\nStep 5: Saving cleaned dataset\n  Saved: /kaggle/working/output/Fr-Dataset_cleaned.csv\n  Rows: 1,653,175\n\nStep 6-7: Extracting features for ENTIRE dataset\n  Total sentences: 1,653,175\n  CPU cores for basic features: 4\n  spaCy mode: single-thread (Notebook compatibility)\n\n  Extracting basic features (CPU parallel)...\n    Using 4 CPU cores for basic features (joblib)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Basic features:   0%|          | 0/1653175 [00:00<?, ?sent/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73fbcbe2dcb41a4b6dd3b81140925f4"}},"metadata":{}},{"name":"stdout","text":"  [Checkpoint saved] checkpoint_fr_basic.csv\n\n  Extracting spaCy features (single-thread)...\n  Loading spaCy model: fr_core_news_sm\n  Downloading fr_core_news_sm...\nCollecting fr-core-news-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/16.3 MB 77.1 MB/s eta 0:00:00\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n    Processing with single-thread (Notebook compatibility mode)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spaCy features:   0%|          | 0/1653175 [00:00<?, ?sent/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef7f4f2974b45138ffab7209d476031"}},"metadata":{}},{"name":"stdout","text":"  [Checkpoint saved] checkpoint_fr_spacy.csv\n\n  Computing SBERT embeddings (GPU accelerated)...\n\n  Computing Sentence-BERT embeddings...\n    Device: cuda\n    GPUs available: 2 (using single GPU)\n    Model: paraphrase-multilingual-MiniLM-L12-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0344cf5d0a3467f827daadac0dd3d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0253d938336475682084e944e2be359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa78f565698b45a8aff26189c351fe25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b638efba4f4ec0824ca3fdbce2ec1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dad86dc61914a23b3f2458a360f464f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e80c2689e248e5b6f4971c3367ec6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44bb150d22264e7d9cf428e1f032630b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3cd9ca5ff364a8cb7d2c183559faa6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29681a26ad474371a351c69ae26fa127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a43baeea22ca46fba5a3851fb3044683"}},"metadata":{}},{"name":"stdout","text":"    Using enlarged batch size for T4: 256\n    Total sentences: 1,653,175\n    Batch size: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/6458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1decd28e8f504e2286bb40e609bdd58d"}},"metadata":{}},{"name":"stdout","text":"    Encoding time: 1897.4s (871 sent/s)\n  [Checkpoint saved] checkpoint_fr_sbert.npy\n\n  Computing cosine similarity from embeddings...\n    Vikidia (simple prototype): 188,856\n    Similarity range: [-0.3231, 0.7707]\n    Similarity mean: 0.2303\n\n  Computing additional features...\n\n  Building final feature DataFrame...\n  Final shape: (1653175, 26)\n\nStep 8: Saving final features\n  Saved: /kaggle/working/output/fr_full_features.csv\n  Rows: 1,653,175\n  Columns: 26\n\n\n======================================================================\nCLEANING REPORT - FR\n======================================================================\n\nOriginal rows: 1,699,063\n\nBasic cleaning:\n  NaN removed: 0\n  Blank removed: 0\n\nDuplicates removed:\n  Vikidia internal: 38,195\n  Wikipedia internal: 6,385\n  Leakage (Viki ∩ Wiki): 1,308\n\nAfter cleaning:\n  Vikidia: 188,856\n  Wikipedia: 1,464,319\n  Total: 1,653,175\n======================================================================\n\n\n======================================================================\nFEATURE SUMMARY - FR\n======================================================================\n\nDataset shape: (1653175, 26)\nColumns: ['Index', 'ID', 'Name', 'Sentence', 'Label', 'LengthWords', 'LengthChars', 'words_chars_ratio', 'cos_simi', 'avg_word_len', 'long_word_ratio', 'ttr', 'punct_density', 'comma_density', 'digit_ratio', 'upper_ratio', 'has_parens', 'n_tokens', 'max_depth', 'avg_depth', 'avg_dependency_distance', 'func_word_ratio', 'n_clauses', 'clause_ratio', 'noun_ratio', 'verb_ratio']\n\nFeature statistics:\n       LengthWords  LengthChars  words_chars_ratio     cos_simi  avg_word_len  long_word_ratio          ttr  punct_density  comma_density  digit_ratio  upper_ratio   has_parens     n_tokens    max_depth    avg_depth  avg_dependency_distance  func_word_ratio    n_clauses  clause_ratio   noun_ratio   verb_ratio\ncount  1653175.000  1653175.000        1653175.000  1653175.000   1653175.000      1653175.000  1653175.000    1653175.000    1653175.000  1653175.000  1653175.000  1653175.000  1653175.000  1653175.000  1653175.000              1653175.000      1653175.000  1653175.000   1653175.000  1653175.000  1653175.000\nmean        24.253      151.648              0.163        0.230         5.264            0.330        0.908          0.161          0.063        0.014        0.028        0.210       28.767        4.859        2.380                    3.157            0.384        1.070         0.036        0.289        0.077\nstd         14.098       90.807              0.021        0.129         0.843            0.110        0.080          0.119          0.062        0.028        0.024        0.409       17.139        1.759        0.799                    1.067            0.099        1.277         0.039        0.084        0.051\nmin         10.000       21.000              0.015       -0.323         1.000            0.000        0.020          0.000          0.000        0.000        0.000        0.000       10.000        1.000        0.019                    0.028            0.000        0.000         0.000        0.000        0.000\n25%         15.000       93.000              0.150        0.144         4.737            0.258        0.857          0.087          0.000        0.000        0.011        0.000       18.000        4.000        1.833                    2.500            0.333        0.000         0.000        0.235        0.044\n50%         21.000      131.000              0.162        0.233         5.190            0.333        0.917          0.133          0.056        0.000        0.021        0.000       25.000        5.000        2.250                    3.000            0.400        1.000         0.030        0.283        0.071\n75%         30.000      186.000              0.175        0.321         5.692            0.400        1.000          0.200          0.091        0.019        0.039        0.000       35.000        6.000        2.774                    3.600            0.450        2.000         0.059        0.333        0.107\nmax       5266.000    27309.000              0.524        0.771        66.200            1.000        1.000         29.100          1.312        0.706        0.814        1.000     7169.000       53.000       24.585                   90.695            1.000       55.000         0.593        0.938        0.778\n======================================================================\n\nTotal processing time: 100.0 minutes\n\nClearing checkpoints...\n  [Checkpoint deleted] checkpoint_fr_spacy.csv\n  [Checkpoint deleted] checkpoint_fr_sbert.npy\n  [Checkpoint deleted] checkpoint_fr_cleaned.csv\n  [Checkpoint deleted] checkpoint_fr_basic.csv\n\n======================================================================\nALL PROCESSING COMPLETE!\n======================================================================\n\nOutput files:\n  En-Dataset_cleaned.csv (57.7 MB)\n  Fr-Dataset_cleaned.csv (348.2 MB)\n  en_full_features.csv (101.0 MB)\n  fr_full_features.csv (593.1 MB)\n","output_type":"stream"}],"execution_count":141}]}